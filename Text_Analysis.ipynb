{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment 8.ipynb",
      "version": "0.3.2",
      "provenance": []
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "bXqLMoDhqYaR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Set Up"
      ]
    },
    {
      "metadata": {
        "id": "dwl9pucl2w1h",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Authenticate into Google Drive and find file ids in order to read in data files\n",
        "!pip install -U -q PyDrive\n",
        "\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# 1. Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "X8X9XXMY2_t7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd \n",
        "import numpy as np \n",
        "\n",
        "import statsmodels.api as sm  \n",
        "import matplotlib.pyplot as plt \n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.cross_validation import cross_val_score, cross_val_predict\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "from math import sqrt \n",
        "\n",
        "import tensorflow as tf\n",
        "import json\n",
        "import os\n",
        "from collections import defaultdict\n",
        "\n",
        "import time\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "\n",
        "get_ipython().magic(u'matplotlib inline')\n",
        "#these are some options to control output on your screen \n",
        "pd.set_option('display.notebook_repr_html', False) \n",
        "pd.set_option('display.max_columns', 40) \n",
        "pd.set_option('display.max_rows', 50) \n",
        "pd.set_option('display.width', 120)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "b11Rr_fX6m8L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "133bf809-6b37-49aa-b0e2-3a975f870938"
      },
      "cell_type": "code",
      "source": [
        "!pip install chakin"
      ],
      "execution_count": 189,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: chakin in /usr/local/lib/python2.7/dist-packages (0.0.6)\r\n",
            "Requirement already satisfied: progressbar2>=3.20.0 in /usr/local/lib/python2.7/dist-packages (from chakin) (3.38.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python2.7/dist-packages (from chakin) (1.11.0)\n",
            "Requirement already satisfied: pandas>=0.20.1 in /usr/local/lib/python2.7/dist-packages (from chakin) (0.22.0)\n",
            "Requirement already satisfied: python-utils>=2.3.0 in /usr/local/lib/python2.7/dist-packages (from progressbar2>=3.20.0->chakin) (2.3.0)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python2.7/dist-packages (from pandas>=0.20.1->chakin) (2018.5)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python2.7/dist-packages (from pandas>=0.20.1->chakin) (2.5.3)\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python2.7/dist-packages (from pandas>=0.20.1->chakin) (1.14.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kgK23abB612i",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import chakin\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "import json\n",
        "import os\n",
        "from collections import defaultdict\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oCzZXxuMCjQM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "outputId": "24ad6595-688d-4b57-804e-3a956a6b2144"
      },
      "cell_type": "code",
      "source": [
        "chakin.search(lang='English')"
      ],
      "execution_count": 191,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                   Name  Dimension                     Corpus VocabularySize    Method Language    Author\n",
            "2          fastText(en)        300                  Wikipedia           2.5M  fastText  English  Facebook\n",
            "11         GloVe.6B.50d         50  Wikipedia+Gigaword 5 (6B)           400K     GloVe  English  Stanford\n",
            "12        GloVe.6B.100d        100  Wikipedia+Gigaword 5 (6B)           400K     GloVe  English  Stanford\n",
            "13        GloVe.6B.200d        200  Wikipedia+Gigaword 5 (6B)           400K     GloVe  English  Stanford\n",
            "14        GloVe.6B.300d        300  Wikipedia+Gigaword 5 (6B)           400K     GloVe  English  Stanford\n",
            "15       GloVe.42B.300d        300          Common Crawl(42B)           1.9M     GloVe  English  Stanford\n",
            "16      GloVe.840B.300d        300         Common Crawl(840B)           2.2M     GloVe  English  Stanford\n",
            "17    GloVe.Twitter.25d         25               Twitter(27B)           1.2M     GloVe  English  Stanford\n",
            "18    GloVe.Twitter.50d         50               Twitter(27B)           1.2M     GloVe  English  Stanford\n",
            "19   GloVe.Twitter.100d        100               Twitter(27B)           1.2M     GloVe  English  Stanford\n",
            "20   GloVe.Twitter.200d        200               Twitter(27B)           1.2M     GloVe  English  Stanford\n",
            "21  word2vec.GoogleNews        300          Google News(100B)           3.0M  word2vec  English    Google\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qyhmOOIE63Gw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#download directory\n",
        "#chakin.download(number=11, save_dir='./') # GloVe.6B.50d"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CAUdVwpm63R8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#https://post2web.github.io/posts/embeddings-with-tensorflow/\n",
        "#https://github.com/guillaume-chevalier/GloVe-as-a-TensorFlow-Embedding-Layer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SDwLT37g63lj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "42d3ce84-440b-4475-8018-deece1f322af"
      },
      "cell_type": "code",
      "source": [
        "CHAKIN_INDEX = 11\n",
        "NUMBER_OF_DIMENSIONS = 50\n",
        "SUBFOLDER_NAME = \"glove.6B\"\n",
        "\n",
        "DATA_FOLDER = \"embeddings\"\n",
        "ZIP_FILE = os.path.join(DATA_FOLDER, \"{}.zip\".format(SUBFOLDER_NAME))\n",
        "ZIP_FILE_ALT = \"glove\" + ZIP_FILE[5:]  # sometimes it's lowercase only...\n",
        "UNZIP_FOLDER = os.path.join(DATA_FOLDER, SUBFOLDER_NAME)\n",
        "if SUBFOLDER_NAME[-1] == \"d\":\n",
        "    GLOVE_FILENAME = os.path.join(\n",
        "        UNZIP_FOLDER, \"{}.txt\".format(SUBFOLDER_NAME))\n",
        "else:\n",
        "    GLOVE_FILENAME = os.path.join(UNZIP_FOLDER, \"{}.{}d.txt\".format(\n",
        "        SUBFOLDER_NAME, NUMBER_OF_DIMENSIONS))\n",
        "\n",
        "\n",
        "if not os.path.exists(ZIP_FILE) and not os.path.exists(UNZIP_FOLDER):\n",
        "    # GloVe by Stanford is licensed Apache 2.0:\n",
        "    #     https://github.com/stanfordnlp/GloVe/blob/master/LICENSE\n",
        "    #     http://nlp.stanford.edu/data/glove.twitter.27B.zip\n",
        "    #     Copyright 2014 The Board of Trustees of The Leland Stanford Junior University\n",
        "    print(\"Downloading embeddings to '{}'\".format(ZIP_FILE))\n",
        "    chakin.download(number=CHAKIN_INDEX, save_dir='./{}'.format(DATA_FOLDER))\n",
        "else:\n",
        "    print(\"Embeddings already downloaded.\")\n",
        "\n",
        "if not os.path.exists(UNZIP_FOLDER):\n",
        "    import zipfile\n",
        "    if not os.path.exists(ZIP_FILE) and os.path.exists(ZIP_FILE_ALT):\n",
        "        ZIP_FILE = ZIP_FILE_ALT\n",
        "    with zipfile.ZipFile(ZIP_FILE, \"r\") as zip_ref:\n",
        "        print(\"Extracting embeddings to '{}'\".format(UNZIP_FOLDER))\n",
        "        zip_ref.extractall(UNZIP_FOLDER)\n",
        "else:\n",
        "    print(\"Embeddings already extracted.\")\n",
        "\n",
        "print('\\nRun complete')"
      ],
      "execution_count": 194,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Embeddings already downloaded.\n",
            "Embeddings already extracted.\n",
            "\n",
            "Run complete\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wq-LWwiaGcB-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Second Set of Code"
      ]
    },
    {
      "metadata": {
        "id": "NPDIu6Qx63bu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# coding: utf-8\n",
        "\n",
        "# Program by Thomas W. Miller, August 16, 2018\n",
        "\n",
        "# Previous work involved gathering embeddings via chakin\n",
        "# Following methods described in\n",
        "#    https://github.com/chakki-works/chakin\n",
        "# The previous program, run-chakin-to-get-embeddings-v001.py\n",
        "# downloaded pre-trained GloVe embeddings, saved them in a zip archive,\n",
        "# and unzipped that archive to create the four word-to-embeddings\n",
        "# text files for use in language models. \n",
        "\n",
        "# This program sets uses word embeddings to set up defaultdict \n",
        "# dictionary data structures, that can them be employed in language\n",
        "# models. This is demonstrated with a simple RNN model for predicting\n",
        "# sentiment (thumbs-down versus thumbs-up) for movie reviews."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qmbhSrbP3B2q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import os  # operating system functions\n",
        "import os.path  # for manipulation of file path names\n",
        "\n",
        "import re  # regular expressions\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eNAnNwbcF8VB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "RANDOM_SEED = 9999"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nSVcF1QxFblI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# To make output stable across runs\n",
        "def reset_graph(seed= RANDOM_SEED):\n",
        "    tf.reset_default_graph()\n",
        "    tf.set_random_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "REMOVE_STOPWORDS = False  # no stopword removal \n",
        "\n",
        "EVOCABSIZE = 10000  # specify desired size of pre-defined embedding vocabulary "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "50vI44u2FmVk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------------- \n",
        "# Select the pre-defined embeddings source        \n",
        "# Define vocabulary size for the language model    \n",
        "# Create a word_to_embedding_dict for GloVe.6B.50d\n",
        "embeddings_directory = 'embeddings/glove.6B'\n",
        "filename = 'glove.6B.50d.txt'\n",
        "embeddings_filename = os.path.join(embeddings_directory, filename)\n",
        "# ------------------------------------------------------------- "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tb8_xfYoFpIM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Utility function for loading embeddings follows methods described in\n",
        "# https://github.com/guillaume-chevalier/GloVe-as-a-TensorFlow-Embedding-Layer\n",
        "# Creates the Python defaultdict dictionary word_to_embedding_dict\n",
        "# for the requested pre-trained word embeddings\n",
        "# \n",
        "# Note the use of defaultdict data structure from the Python Standard Library\n",
        "# collections_defaultdict.py lets the caller specify a default value up front\n",
        "# The default value will be retuned if the key is not a known dictionary key\n",
        "# That is, unknown words are represented by a vector of zeros\n",
        "# For word embeddings, this default value is a vector of zeros\n",
        "# Documentation for the Python standard library:\n",
        "#   Hellmann, D. 2017. The Python 3 Standard Library by Example. Boston: \n",
        "#     Addison-Wesley. [ISBN-13: 978-0-13-429105-5]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AabdjqhmFtns",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "adf17165-c88a-4ded-fb1c-b7e7824429d1"
      },
      "cell_type": "code",
      "source": [
        "# Utility function for loading embeddings follows methods described in\n",
        "# https://github.com/guillaume-chevalier/GloVe-as-a-TensorFlow-Embedding-Layer\n",
        "# Creates the Python defaultdict dictionary word_to_embedding_dict\n",
        "# for the requested pre-trained word embeddings\n",
        "# \n",
        "# Note the use of defaultdict data structure from the Python Standard Library\n",
        "# collections_defaultdict.py lets the caller specify a default value up front\n",
        "# The default value will be retuned if the key is not a known dictionary key\n",
        "# That is, unknown words are represented by a vector of zeros\n",
        "# For word embeddings, this default value is a vector of zeros\n",
        "# Documentation for the Python standard library:\n",
        "#   Hellmann, D. 2017. The Python 3 Standard Library by Example. Boston: \n",
        "#     Addison-Wesley. [ISBN-13: 978-0-13-429105-5]\n",
        "def load_embedding_from_disks(embeddings_filename, with_indexes=True):\n",
        "    \"\"\"\n",
        "    Read a embeddings txt file. If `with_indexes=True`, \n",
        "    we return a tuple of two dictionnaries\n",
        "    `(word_to_index_dict, index_to_embedding_array)`, \n",
        "    otherwise we return only a direct \n",
        "    `word_to_embedding_dict` dictionnary mapping \n",
        "    from a string to a numpy array.\n",
        "    \"\"\"\n",
        "    if with_indexes:\n",
        "        word_to_index_dict = dict()\n",
        "        index_to_embedding_array = []\n",
        "  \n",
        "    else:\n",
        "        word_to_embedding_dict = dict()\n",
        "\n",
        "    with open(embeddings_filename, 'r') as embeddings_file:\n",
        "        for (i, line) in enumerate(embeddings_file):\n",
        "\n",
        "            split = line.split(' ')\n",
        "\n",
        "            word = split[0]\n",
        "\n",
        "            representation = split[1:]\n",
        "            representation = np.array(\n",
        "                [float(val) for val in representation]\n",
        "            )\n",
        "\n",
        "            if with_indexes:\n",
        "                word_to_index_dict[word] = i\n",
        "                index_to_embedding_array.append(representation)\n",
        "            else:\n",
        "                word_to_embedding_dict[word] = representation\n",
        "\n",
        "    # Empty representation for unknown words.\n",
        "    _WORD_NOT_FOUND = [0.0] * len(representation)\n",
        "    if with_indexes:\n",
        "        _LAST_INDEX = i + 1\n",
        "        word_to_index_dict = defaultdict(\n",
        "            lambda: _LAST_INDEX, word_to_index_dict)\n",
        "        index_to_embedding_array = np.array(\n",
        "            index_to_embedding_array + [_WORD_NOT_FOUND])\n",
        "        return word_to_index_dict, index_to_embedding_array\n",
        "    else:\n",
        "        word_to_embedding_dict = defaultdict(lambda: _WORD_NOT_FOUND)\n",
        "        return word_to_embedding_dict\n",
        "\n",
        "print('\\nLoading embeddings from', embeddings_filename)\n",
        "word_to_index, index_to_embedding = \\\n",
        "    load_embedding_from_disks(embeddings_filename, with_indexes=True)\n",
        "print(\"Embedding loaded from disks.\")"
      ],
      "execution_count": 295,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Loading embeddings from embeddings/glove.6B/glove.6B.50d.txt\n",
            "Embedding loaded from disks.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ry3U-jOoJibH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "outputId": "126e469e-b3f8-4254-c1ae-bf5ec777d01b"
      },
      "cell_type": "code",
      "source": [
        "# Note: unknown words have representations with values [0, 0, ..., 0]\n",
        "\n",
        "# Additional background code from\n",
        "# https://github.com/guillaume-chevalier/GloVe-as-a-TensorFlow-Embedding-Layer\n",
        "# shows the general structure of the data structures for word embeddings\n",
        "# This code is modified for our purposes in language modeling \n",
        "vocab_size, embedding_dim = index_to_embedding.shape\n",
        "print(\"Embedding is of shape: {}\".format(index_to_embedding.shape))\n",
        "print(\"This means (number of words, number of dimensions per word)\\n\")\n",
        "print(\"The first words are words that tend occur more often.\")\n",
        "\n",
        "print(\"Note: for unknown words, the representation is an empty vector,\\n\"\n",
        "      \"and the index is the last one. The dictionnary has a limit:\")\n",
        "print(\"    {} --> {} --> {}\".format(\"A word\", \"Index in embedding\", \n",
        "      \"Representation\"))\n",
        "word = \"worsdfkljsdf\"  # a word obviously not in the vocabulary\n",
        "idx = word_to_index[word] # index for word obviously not in the vocabulary\n",
        "complete_vocabulary_size = idx \n",
        "embd = list(np.array(index_to_embedding[idx], dtype=int)) # \"int\" compact print\n",
        "print(\"    {} --> {} --> {}\".format(word, idx, embd))\n",
        "word = \"the\"\n",
        "idx = word_to_index[word]\n",
        "embd = list(index_to_embedding[idx])  # \"int\" for compact print only.\n",
        "print(\"    {} --> {} --> {}\".format(word, idx, embd))"
      ],
      "execution_count": 296,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Embedding is of shape: (400001, 50)\n",
            "This means (number of words, number of dimensions per word)\n",
            "\n",
            "The first words are words that tend occur more often.\n",
            "Note: for unknown words, the representation is an empty vector,\n",
            "and the index is the last one. The dictionnary has a limit:\n",
            "    A word --> Index in embedding --> Representation\n",
            "    worsdfkljsdf --> 400000 --> [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "    the --> 0 --> [0.418, 0.24968, -0.41242, 0.1217, 0.34527, -0.044457, -0.49688, -0.17862, -0.00066023, -0.6566, 0.27843, -0.14767, -0.55677, 0.14658, -0.0095095, 0.011658, 0.10204, -0.12792, -0.8443, -0.12181, -0.016801, -0.33279, -0.1552, -0.23131, -0.19181, -1.8823, -0.76746, 0.099051, -0.42125, -0.19526, 4.0071, -0.18594, -0.52287, -0.31681, 0.00059213, 0.0074449, 0.17778, -0.15897, 0.012041, -0.054223, -0.29871, -0.15749, -0.34758, -0.045637, -0.44251, 0.18785, 0.0027849, -0.18411, -0.11514, -0.78581]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UouprojaJ6gA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1448
        },
        "outputId": "3e8f2ec8-1c0a-42d1-cb9a-b8021fb0a9fa"
      },
      "cell_type": "code",
      "source": [
        "# Show how to use embeddings dictionaries with a test sentence\n",
        "# This is a famous typing exercise with all letters of the alphabet\n",
        "# https://en.wikipedia.org/wiki/The_quick_brown_fox_jumps_over_the_lazy_dog\n",
        "a_typing_test_sentence = 'The quick brown fox jumps over the lazy dog'\n",
        "print('\\nTest sentence: ', a_typing_test_sentence, '\\n')\n",
        "words_in_test_sentence = a_typing_test_sentence.split()\n",
        "\n",
        "print('Test sentence embeddings from complete vocabulary of', \n",
        "      complete_vocabulary_size, 'words:\\n')\n",
        "for word in words_in_test_sentence:\n",
        "    word_ = word.lower()\n",
        "    embedding = index_to_embedding[word_to_index[word_]]\n",
        "    print(word_ + \": \", embedding)"
      ],
      "execution_count": 297,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test sentence:  The quick brown fox jumps over the lazy dog \n",
            "\n",
            "Test sentence embeddings from complete vocabulary of 400000 words:\n",
            "\n",
            "the:  [ 4.1800e-01  2.4968e-01 -4.1242e-01  1.2170e-01  3.4527e-01 -4.4457e-02\n",
            " -4.9688e-01 -1.7862e-01 -6.6023e-04 -6.5660e-01  2.7843e-01 -1.4767e-01\n",
            " -5.5677e-01  1.4658e-01 -9.5095e-03  1.1658e-02  1.0204e-01 -1.2792e-01\n",
            " -8.4430e-01 -1.2181e-01 -1.6801e-02 -3.3279e-01 -1.5520e-01 -2.3131e-01\n",
            " -1.9181e-01 -1.8823e+00 -7.6746e-01  9.9051e-02 -4.2125e-01 -1.9526e-01\n",
            "  4.0071e+00 -1.8594e-01 -5.2287e-01 -3.1681e-01  5.9213e-04  7.4449e-03\n",
            "  1.7778e-01 -1.5897e-01  1.2041e-02 -5.4223e-02 -2.9871e-01 -1.5749e-01\n",
            " -3.4758e-01 -4.5637e-02 -4.4251e-01  1.8785e-01  2.7849e-03 -1.8411e-01\n",
            " -1.1514e-01 -7.8581e-01]\n",
            "quick:  [ 0.13967   -0.53798   -0.18047   -0.25142    0.16203   -0.13868\n",
            " -0.24637    0.75111    0.27264    0.61035   -0.82548    0.038647\n",
            " -0.32361    0.30373   -0.14598   -0.23551    0.39267   -1.1287\n",
            " -0.23636   -1.0629     0.046277   0.29143   -0.25819   -0.094902\n",
            "  0.79478   -1.2095    -0.01039   -0.092086   0.84322   -0.11061\n",
            "  3.0096     0.51652   -0.76986    0.51074    0.37508    0.12156\n",
            "  0.082794   0.43605   -0.1584    -0.61048    0.35006    0.52465\n",
            " -0.51747    0.0034705  0.73625    0.16252    0.85279    0.85268\n",
            "  0.57892    0.64483  ]\n",
            "brown:  [-0.88497   0.71685  -0.40379  -0.10698   0.81457   1.0258   -1.2698\n",
            " -0.49382  -0.27839  -0.92251  -0.49409   0.78942  -0.20066  -0.057371\n",
            "  0.060682  0.30746   0.13441  -0.49376  -0.54788  -0.81912  -0.45394\n",
            "  0.52098   1.0325   -0.8584   -0.65848  -1.2736    0.23616   1.0486\n",
            "  0.18442  -0.3901    2.1385   -0.45301  -0.16911  -0.46737   0.15938\n",
            " -0.095071 -0.26512  -0.056479  0.63849  -1.0494    0.037507  0.76434\n",
            " -0.6412   -0.59594   0.46589   0.31494  -0.34072  -0.59167  -0.31057\n",
            "  0.73274 ]\n",
            "fox:  [ 0.44206   0.059552  0.15861   0.92777   0.1876    0.24256  -1.593\n",
            " -0.79847  -0.34099  -0.24021  -0.32756   0.43639  -0.11057   0.50472\n",
            "  0.43853   0.19738  -0.1498   -0.046979 -0.83286   0.39878   0.062174\n",
            "  0.28803   0.79134   0.31798  -0.21933  -1.1015   -0.080309  0.39122\n",
            "  0.19503  -0.5936    1.7921    0.3826   -0.30509  -0.58686  -0.76935\n",
            " -0.61914  -0.61771  -0.68484  -0.67919  -0.74626  -0.036646  0.78251\n",
            " -1.0072   -0.59057  -0.7849   -0.39113  -0.49727  -0.4283   -0.15204\n",
            "  1.5064  ]\n",
            "jumps:  [-0.46105   -0.34219    0.71473   -0.29778    0.28839    0.6248\n",
            "  0.36807   -0.072746   0.60476    0.31463   -0.052247  -0.62302\n",
            " -0.56332    0.7855     0.18116   -0.31698    0.38298   -0.081953\n",
            " -1.3658    -0.78263    0.39804   -0.17001   -0.11926   -0.40146\n",
            "  1.1057    -0.51142   -0.36614    0.22177    0.34626   -0.30648\n",
            "  1.3869     0.77328    0.5946     1.2577     0.23472   -0.46087\n",
            " -0.009223   0.44534    0.012732  -0.24749   -0.7142     0.02422\n",
            "  0.083527   0.25088   -0.24259   -1.354      1.5481    -0.31728\n",
            "  0.55305   -0.0028062]\n",
            "over:  [ 0.12972    0.088073   0.24375    0.078102  -0.12783    0.27831\n",
            " -0.48693    0.19649   -0.39558   -0.28362   -0.47425   -0.59317\n",
            " -0.58804   -0.31702    0.49593    0.0087594  0.039613  -0.42495\n",
            " -0.97641   -0.46534    0.020675   0.086042   0.39317   -0.51255\n",
            " -0.17913   -1.8333     0.5622     0.41626    0.075127   0.02189\n",
            "  3.784      0.71067   -0.073943   0.15373   -0.3853    -0.070163\n",
            " -0.35374    0.074501  -0.084228  -0.45548   -0.081068   0.39157\n",
            "  0.173      0.2254    -0.12836    0.40951   -0.26079    0.090912\n",
            " -0.60515   -0.9827   ]\n",
            "the:  [ 4.1800e-01  2.4968e-01 -4.1242e-01  1.2170e-01  3.4527e-01 -4.4457e-02\n",
            " -4.9688e-01 -1.7862e-01 -6.6023e-04 -6.5660e-01  2.7843e-01 -1.4767e-01\n",
            " -5.5677e-01  1.4658e-01 -9.5095e-03  1.1658e-02  1.0204e-01 -1.2792e-01\n",
            " -8.4430e-01 -1.2181e-01 -1.6801e-02 -3.3279e-01 -1.5520e-01 -2.3131e-01\n",
            " -1.9181e-01 -1.8823e+00 -7.6746e-01  9.9051e-02 -4.2125e-01 -1.9526e-01\n",
            "  4.0071e+00 -1.8594e-01 -5.2287e-01 -3.1681e-01  5.9213e-04  7.4449e-03\n",
            "  1.7778e-01 -1.5897e-01  1.2041e-02 -5.4223e-02 -2.9871e-01 -1.5749e-01\n",
            " -3.4758e-01 -4.5637e-02 -4.4251e-01  1.8785e-01  2.7849e-03 -1.8411e-01\n",
            " -1.1514e-01 -7.8581e-01]\n",
            "lazy:  [-0.27611  -0.59712  -0.49227  -1.0372   -0.35878  -0.097425 -0.21014\n",
            " -0.092836 -0.054118  0.4542   -0.53296   0.37602   0.77087   0.79669\n",
            " -0.076608 -0.42515   0.42576   0.32791  -0.21996  -0.20261  -0.85139\n",
            "  0.80547   0.97621   0.9792    1.1118   -0.36062  -0.2588    0.8596\n",
            "  0.73631  -0.18601   1.2376   -0.038938  0.19246   0.52473  -0.04842\n",
            " -0.044149  0.064432  0.087822  0.42232  -0.55991  -0.44096   0.097736\n",
            " -0.17589   1.1799    0.13152  -1.0795    0.45685  -0.63312   1.2752\n",
            "  1.1672  ]\n",
            "dog:  [ 0.11008   -0.38781   -0.57615   -0.27714    0.70521    0.53994\n",
            " -1.0786    -0.40146    1.1504    -0.5678     0.0038977  0.52878\n",
            "  0.64561    0.47262    0.48549   -0.18407    0.1801     0.91397\n",
            " -1.1979    -0.5778    -0.37985    0.33606    0.772      0.75555\n",
            "  0.45506   -1.7671    -1.0503     0.42566    0.41893   -0.68327\n",
            "  1.5673     0.27685   -0.61708    0.64638   -0.076996   0.37118\n",
            "  0.1308    -0.45137    0.25398   -0.74392   -0.086199   0.24068\n",
            " -0.64819    0.83549    1.2502    -0.51379    0.04224   -0.88118\n",
            "  0.7158     0.38519  ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "21E0FzZ7K3sH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------------- \n",
        "# Define vocabulary size for the language model    \n",
        "# To reduce the size of the vocabulary to the n most frequently used words\n",
        "\n",
        "def default_factory():\n",
        "    return EVOCABSIZE  # last/unknown-word row in limited_index_to_embedding\n",
        "# dictionary has the items() function, returns list of (key, value) tuples\n",
        "limited_word_to_index = defaultdict(default_factory, \\\n",
        "    {k: v for k, v in word_to_index.items() if v < EVOCABSIZE})\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "At96iZDyLIoJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Select the first EVOCABSIZE rows to the index_to_embedding\n",
        "limited_index_to_embedding = index_to_embedding[0:EVOCABSIZE,:]\n",
        "# Set the unknown-word row to be all zeros as previously\n",
        "limited_index_to_embedding = np.append(limited_index_to_embedding, \n",
        "    index_to_embedding[index_to_embedding.shape[0] - 1, :].\\\n",
        "        reshape(1,embedding_dim), \n",
        "    axis = 0)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5fqosyoGLJKi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Delete large numpy array to clear some CPU RAM\n",
        "del index_to_embedding"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-utS6fiOLM5Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1227
        },
        "outputId": "439b1cce-29e1-4f4e-d5e7-2380a381fed4"
      },
      "cell_type": "code",
      "source": [
        "# Verify the new vocabulary: should get same embeddings for test sentence\n",
        "# Note that a small EVOCABSIZE may yield some zero vectors for embeddings\n",
        "print('\\nTest sentence embeddings from vocabulary of', EVOCABSIZE, 'words:\\n')\n",
        "for word in words_in_test_sentence:\n",
        "    word_ = word.lower()\n",
        "    embedding = limited_index_to_embedding[limited_word_to_index[word_]]\n",
        "    print(word_ + \": \", embedding)\n",
        "\n"
      ],
      "execution_count": 301,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test sentence embeddings from vocabulary of 10000 words:\n",
            "\n",
            "the:  [ 4.1800e-01  2.4968e-01 -4.1242e-01  1.2170e-01  3.4527e-01 -4.4457e-02\n",
            " -4.9688e-01 -1.7862e-01 -6.6023e-04 -6.5660e-01  2.7843e-01 -1.4767e-01\n",
            " -5.5677e-01  1.4658e-01 -9.5095e-03  1.1658e-02  1.0204e-01 -1.2792e-01\n",
            " -8.4430e-01 -1.2181e-01 -1.6801e-02 -3.3279e-01 -1.5520e-01 -2.3131e-01\n",
            " -1.9181e-01 -1.8823e+00 -7.6746e-01  9.9051e-02 -4.2125e-01 -1.9526e-01\n",
            "  4.0071e+00 -1.8594e-01 -5.2287e-01 -3.1681e-01  5.9213e-04  7.4449e-03\n",
            "  1.7778e-01 -1.5897e-01  1.2041e-02 -5.4223e-02 -2.9871e-01 -1.5749e-01\n",
            " -3.4758e-01 -4.5637e-02 -4.4251e-01  1.8785e-01  2.7849e-03 -1.8411e-01\n",
            " -1.1514e-01 -7.8581e-01]\n",
            "quick:  [ 0.13967   -0.53798   -0.18047   -0.25142    0.16203   -0.13868\n",
            " -0.24637    0.75111    0.27264    0.61035   -0.82548    0.038647\n",
            " -0.32361    0.30373   -0.14598   -0.23551    0.39267   -1.1287\n",
            " -0.23636   -1.0629     0.046277   0.29143   -0.25819   -0.094902\n",
            "  0.79478   -1.2095    -0.01039   -0.092086   0.84322   -0.11061\n",
            "  3.0096     0.51652   -0.76986    0.51074    0.37508    0.12156\n",
            "  0.082794   0.43605   -0.1584    -0.61048    0.35006    0.52465\n",
            " -0.51747    0.0034705  0.73625    0.16252    0.85279    0.85268\n",
            "  0.57892    0.64483  ]\n",
            "brown:  [-0.88497   0.71685  -0.40379  -0.10698   0.81457   1.0258   -1.2698\n",
            " -0.49382  -0.27839  -0.92251  -0.49409   0.78942  -0.20066  -0.057371\n",
            "  0.060682  0.30746   0.13441  -0.49376  -0.54788  -0.81912  -0.45394\n",
            "  0.52098   1.0325   -0.8584   -0.65848  -1.2736    0.23616   1.0486\n",
            "  0.18442  -0.3901    2.1385   -0.45301  -0.16911  -0.46737   0.15938\n",
            " -0.095071 -0.26512  -0.056479  0.63849  -1.0494    0.037507  0.76434\n",
            " -0.6412   -0.59594   0.46589   0.31494  -0.34072  -0.59167  -0.31057\n",
            "  0.73274 ]\n",
            "fox:  [ 0.44206   0.059552  0.15861   0.92777   0.1876    0.24256  -1.593\n",
            " -0.79847  -0.34099  -0.24021  -0.32756   0.43639  -0.11057   0.50472\n",
            "  0.43853   0.19738  -0.1498   -0.046979 -0.83286   0.39878   0.062174\n",
            "  0.28803   0.79134   0.31798  -0.21933  -1.1015   -0.080309  0.39122\n",
            "  0.19503  -0.5936    1.7921    0.3826   -0.30509  -0.58686  -0.76935\n",
            " -0.61914  -0.61771  -0.68484  -0.67919  -0.74626  -0.036646  0.78251\n",
            " -1.0072   -0.59057  -0.7849   -0.39113  -0.49727  -0.4283   -0.15204\n",
            "  1.5064  ]\n",
            "jumps:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0.]\n",
            "over:  [ 0.12972    0.088073   0.24375    0.078102  -0.12783    0.27831\n",
            " -0.48693    0.19649   -0.39558   -0.28362   -0.47425   -0.59317\n",
            " -0.58804   -0.31702    0.49593    0.0087594  0.039613  -0.42495\n",
            " -0.97641   -0.46534    0.020675   0.086042   0.39317   -0.51255\n",
            " -0.17913   -1.8333     0.5622     0.41626    0.075127   0.02189\n",
            "  3.784      0.71067   -0.073943   0.15373   -0.3853    -0.070163\n",
            " -0.35374    0.074501  -0.084228  -0.45548   -0.081068   0.39157\n",
            "  0.173      0.2254    -0.12836    0.40951   -0.26079    0.090912\n",
            " -0.60515   -0.9827   ]\n",
            "the:  [ 4.1800e-01  2.4968e-01 -4.1242e-01  1.2170e-01  3.4527e-01 -4.4457e-02\n",
            " -4.9688e-01 -1.7862e-01 -6.6023e-04 -6.5660e-01  2.7843e-01 -1.4767e-01\n",
            " -5.5677e-01  1.4658e-01 -9.5095e-03  1.1658e-02  1.0204e-01 -1.2792e-01\n",
            " -8.4430e-01 -1.2181e-01 -1.6801e-02 -3.3279e-01 -1.5520e-01 -2.3131e-01\n",
            " -1.9181e-01 -1.8823e+00 -7.6746e-01  9.9051e-02 -4.2125e-01 -1.9526e-01\n",
            "  4.0071e+00 -1.8594e-01 -5.2287e-01 -3.1681e-01  5.9213e-04  7.4449e-03\n",
            "  1.7778e-01 -1.5897e-01  1.2041e-02 -5.4223e-02 -2.9871e-01 -1.5749e-01\n",
            " -3.4758e-01 -4.5637e-02 -4.4251e-01  1.8785e-01  2.7849e-03 -1.8411e-01\n",
            " -1.1514e-01 -7.8581e-01]\n",
            "lazy:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0.]\n",
            "dog:  [ 0.11008   -0.38781   -0.57615   -0.27714    0.70521    0.53994\n",
            " -1.0786    -0.40146    1.1504    -0.5678     0.0038977  0.52878\n",
            "  0.64561    0.47262    0.48549   -0.18407    0.1801     0.91397\n",
            " -1.1979    -0.5778    -0.37985    0.33606    0.772      0.75555\n",
            "  0.45506   -1.7671    -1.0503     0.42566    0.41893   -0.68327\n",
            "  1.5673     0.27685   -0.61708    0.64638   -0.076996   0.37118\n",
            "  0.1308    -0.45137    0.25398   -0.74392   -0.086199   0.24068\n",
            " -0.64819    0.83549    1.2502    -0.51379    0.04224   -0.88118\n",
            "  0.7158     0.38519  ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_rBwdNBjLZK8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------------\n",
        "# code for working with movie reviews data \n",
        "# Source: Miller, T. W. (2016). Web and Network Data Science.\n",
        "#    Upper Saddle River, N.J.: Pearson Education.\n",
        "#    ISBN-13: 978-0-13-388644-3\n",
        "# This original study used a simple bag-of-words approach\n",
        "# to sentiment analysis, along with pre-defined lists of\n",
        "# negative and positive words.        \n",
        "# Code available at:  https://github.com/mtpa/wnds       \n",
        "# ------------------------------------------------------------"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "erTeLXVWLbyp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Utility function to get file names within a directory\n",
        "def listdir_no_hidden(path):\n",
        "    start_list = os.listdir(path)\n",
        "    end_list = []\n",
        "    for file in start_list:\n",
        "        if (not file.startswith('.')):\n",
        "            end_list.append(file)\n",
        "    return(end_list)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oikX6YwcLhkd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# define list of codes to be dropped from document\n",
        "# carriage-returns, line-feeds, tabs\n",
        "codelist = ['\\r', '\\n', '\\t']   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KQq6GHRGLjJV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# We will not remove stopwords in this exercise because they are\n",
        "# important to keeping sentences intact\n",
        "if REMOVE_STOPWORDS:\n",
        "    print(nltk.corpus.stopwords.words('english'))\n",
        "\n",
        "# previous analysis of a list of top terms showed a number of words, along \n",
        "# with contractions and other word strings to drop from further analysis, add\n",
        "# these to the usual English stopwords to be dropped from a document collection\n",
        "    more_stop_words = ['cant','didnt','doesnt','dont','goes','isnt','hes',\\\n",
        "        'shes','thats','theres','theyre','wont','youll','youre','youve', 'br'\\\n",
        "        've', 're', 'vs'] \n",
        "\n",
        "    some_proper_nouns_to_remove = ['dick','ginger','hollywood','jack',\\\n",
        "        'jill','john','karloff','kudrow','orson','peter','tcm','tom',\\\n",
        "        'toni','welles','william','wolheim','nikita']\n",
        "\n",
        "    # start with the initial list and add to it for movie text work \n",
        "    stoplist = nltk.corpus.stopwords.words('english') + more_stop_words +\\\n",
        "        some_proper_nouns_to_remove"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1zyPF6cxLlIg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# text parsing function for creating text documents \n",
        "# there is more we could do for data preparation \n",
        "# stemming... looking for contractions... possessives... \n",
        "# but we will work with what we have in this parsing function\n",
        "# if we want to do stemming at a later time, we can use\n",
        "#     porter = nltk.PorterStemmer()  \n",
        "# in a construction like this\n",
        "#     words_stemmed =  [porter.stem(word) for word in initial_words]  \n",
        "def text_parse(string):\n",
        "    # replace non-alphanumeric with space \n",
        "    temp_string = re.sub('[^a-zA-Z]', '  ', string)    \n",
        "    # replace codes with space\n",
        "    for i in range(len(codelist)):\n",
        "        stopstring = ' ' + codelist[i] + '  '\n",
        "        temp_string = re.sub(stopstring, '  ', temp_string)      \n",
        "    # replace single-character words with space\n",
        "    temp_string = re.sub('\\s.\\s', ' ', temp_string)   \n",
        "    # convert uppercase to lowercase\n",
        "    temp_string = temp_string.lower()    \n",
        "    if REMOVE_STOPWORDS:\n",
        "        # replace selected character strings/stop-words with space\n",
        "        for i in range(len(stoplist)):\n",
        "            stopstring = ' ' + str(stoplist[i]) + ' '\n",
        "            temp_string = re.sub(stopstring, ' ', temp_string)        \n",
        "    # replace multiple blank characters with one blank character\n",
        "    temp_string = re.sub('\\s+', ' ', temp_string)    \n",
        "    return(temp_string)    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eR7D2Ab0MVRL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Manually import these"
      ]
    },
    {
      "metadata": {
        "id": "dug6Q67RY5fz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "6848ef3b-aa7e-4a3c-fda1-85277ec1da02"
      },
      "cell_type": "code",
      "source": [
        "#https://drive.google.com/open?id=1Fhu9STk2t1kFlrcQpx5jMo7RYP7B6MCV\n",
        "# 2. Load files by ID and print its contents.\n",
        "\n",
        "file_id = '1Fhu9STk2t1kFlrcQpx5jMo7RYP7B6MCV' #Link Share the file and copy the id in the url after \"id=\"\n",
        "file = drive.CreateFile({'id': file_id}) #This creates a file in your virtual machine\n",
        "file.GetContentFile('movie-reviews-negative.zip') #This names the file you linked in drive \".zip file\"\n",
        "!ls -lha #This lists all the folder in your local folder, you should see \"negative\""
      ],
      "execution_count": 307,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 896K\r\n",
            "drwxr-xr-x 1 root root 4.0K Aug 26 14:38 .\r\n",
            "drwxr-xr-x 1 root root 4.0K Aug 26 14:32 ..\r\n",
            "-rw-r--r-- 1 root root 2.5K Aug 26 14:34 adc.json\r\n",
            "drwxr-xr-x 1 root root 4.0K Aug 26 14:34 .config\r\n",
            "drwxr-xr-x 3 root root 4.0K Aug 26 14:36 embeddings\r\n",
            "drwxr-xr-x 2 root root  20K Aug 23 11:09 movie-reviews-negative\r\n",
            "-rw-r--r-- 1 root root 407K Aug 26 17:36 movie-reviews-negative.zip\r\n",
            "drwxr-xr-x 2 root root  20K Aug 23 11:11 movie-reviews-positive\r\n",
            "-rw-r--r-- 1 root root 414K Aug 26 17:35 movie-reviews-positive.zip\r\n",
            "drwxr-xr-x 2 root root 4.0K Aug 22 16:42 sample_data\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "yQduqC89Zzt3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "outputId": "465e5ea4-e5e6-4f7e-9844-3697ef39341c"
      },
      "cell_type": "code",
      "source": [
        "!unzip movie-reviews-negative.zip -d ./"
      ],
      "execution_count": 284,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  movie-reviews-negative.zip\n",
            "replace ./movie-reviews-negative/0_3.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: ^C\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "g2-pe4qbL4hk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "5543fa48-9ac9-4675-fc63-33cda34ffa5b"
      },
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------\n",
        "# gather data for 500 negative movie reviews\n",
        "# -----------------------------------------------\n",
        "dir_name = 'movie-reviews-negative'\n",
        "    \n",
        "filenames = listdir_no_hidden(path=dir_name)\n",
        "num_files = len(filenames)\n",
        "\n",
        "for i in range(len(filenames)):\n",
        "    file_exists = os.path.isfile(os.path.join(dir_name, filenames[i]))\n",
        "    assert file_exists\n",
        "print('\\nDirectory:',dir_name)    \n",
        "print('%d files found' % len(filenames))"
      ],
      "execution_count": 308,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Directory: movie-reviews-negative\n",
            "500 files found\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Ow4Z7d3pL4ma",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "1dad28c5-885d-41c0-be15-7d2cad165ed6"
      },
      "cell_type": "code",
      "source": [
        "# Read data for negative movie reviews\n",
        "# Data will be stored in a list of lists where the each list represents \n",
        "# a document and document is a list of words.\n",
        "# We then break the text into words.\n",
        "\n",
        "def read_data(filename):\n",
        "\n",
        "  #with open(filename, encoding='utf-8') as f:\n",
        "  with open(filename) as f:\n",
        "    data = tf.compat.as_str(f.read())\n",
        "    data = data.lower()\n",
        "    data = text_parse(data)\n",
        "    data = TreebankWordTokenizer().tokenize(data)  # The Penn Treebank\n",
        "\n",
        "  return data\n",
        "\n",
        "negative_documents = []\n",
        "\n",
        "print('\\nProcessing document files under', dir_name)\n",
        "for i in range(num_files):\n",
        "    ## print(' ', filenames[i])\n",
        "\n",
        "    words = read_data(os.path.join(dir_name, filenames[i]))\n",
        "\n",
        "    negative_documents.append(words)\n",
        "    # print('Data size (Characters) (Document %d) %d' %(i,len(words)))\n",
        "    # print('Sample string (Document %d) %s'%(i,words[:50]))"
      ],
      "execution_count": 309,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Processing document files under movie-reviews-negative\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ogJmtHd8a_-B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "87f2187f-c559-4013-91ea-16d374edd232"
      },
      "cell_type": "code",
      "source": [
        "#https://drive.google.com/open?id=1AZq6F1fHykY2P8xwxQQRIwFJWLU5WWQQ\n",
        "# 2. Load files by ID and print its contents.\n",
        "\n",
        "file_id = '1AZq6F1fHykY2P8xwxQQRIwFJWLU5WWQQ' #Link Share the file and copy the id in the url after \"id=\"\n",
        "file = drive.CreateFile({'id': file_id}) #This creates a file in your virtual machine\n",
        "file.GetContentFile('movie-reviews-positive.zip') #This names the file you linked in drive \".zip file\"\n",
        "!ls -lha #This lists all the folder in your local folder, you should see \"negative\""
      ],
      "execution_count": 310,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 896K\r\n",
            "drwxr-xr-x 1 root root 4.0K Aug 26 14:38 .\r\n",
            "drwxr-xr-x 1 root root 4.0K Aug 26 14:32 ..\r\n",
            "-rw-r--r-- 1 root root 2.5K Aug 26 14:34 adc.json\r\n",
            "drwxr-xr-x 1 root root 4.0K Aug 26 14:34 .config\r\n",
            "drwxr-xr-x 3 root root 4.0K Aug 26 14:36 embeddings\r\n",
            "drwxr-xr-x 2 root root  20K Aug 23 11:09 movie-reviews-negative\r\n",
            "-rw-r--r-- 1 root root 407K Aug 26 17:36 movie-reviews-negative.zip\r\n",
            "drwxr-xr-x 2 root root  20K Aug 23 11:11 movie-reviews-positive\r\n",
            "-rw-r--r-- 1 root root 414K Aug 26 17:36 movie-reviews-positive.zip\r\n",
            "drwxr-xr-x 2 root root 4.0K Aug 22 16:42 sample_data\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hGJoIN8obFIa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "outputId": "d2ba769b-8335-4783-c2f5-c16a924a7798"
      },
      "cell_type": "code",
      "source": [
        "!unzip movie-reviews-positive.zip -d ./"
      ],
      "execution_count": 218,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  movie-reviews-positive.zip\n",
            "replace ./movie-reviews-positive/0_9.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: ^C\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2vvDF0QjMAJF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "34b0446a-4816-4211-be66-b828332a93e8"
      },
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------\n",
        "# gather data for 500 positive movie reviews\n",
        "# -----------------------------------------------\n",
        "dir_name = 'movie-reviews-positive'  \n",
        "filenames = listdir_no_hidden(path=dir_name)\n",
        "num_files = len(filenames)\n",
        "\n",
        "for i in range(len(filenames)):\n",
        "    file_exists = os.path.isfile(os.path.join(dir_name, filenames[i]))\n",
        "    assert file_exists\n",
        "print('\\nDirectory:',dir_name)    \n",
        "print('%d files found' % len(filenames))"
      ],
      "execution_count": 311,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Directory: movie-reviews-positive\n",
            "500 files found\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3c5tL6MQMFzK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "3a5b0fca-ff08-4e7c-cdcf-8ed274a1d6e9"
      },
      "cell_type": "code",
      "source": [
        "# Read data for positive movie reviews\n",
        "# Data will be stored in a list of lists where the each list \n",
        "# represents a document and document is a list of words.\n",
        "# We then break the text into words.\n",
        "\n",
        "def read_data(filename):\n",
        "\n",
        "  with open(filename) as f:\n",
        "    data = tf.compat.as_str(f.read())\n",
        "    data = data.lower()\n",
        "    data = text_parse(data)\n",
        "    data = TreebankWordTokenizer().tokenize(data)  # The Penn Treebank\n",
        "\n",
        "  return data\n",
        "\n",
        "positive_documents = []\n",
        "\n",
        "print('\\nProcessing document files under', dir_name)\n",
        "for i in range(num_files):\n",
        "    ## print(' ', filenames[i])\n",
        "\n",
        "    words = read_data(os.path.join(dir_name, filenames[i]))\n",
        "\n",
        "    positive_documents.append(words)\n",
        "    # print('Data size (Characters) (Document %d) %d' %(i,len(words)))\n",
        "    # print('Sample string (Document %d) %s'%(i,words[:50]))\n",
        "\n"
      ],
      "execution_count": 312,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Processing document files under movie-reviews-positive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vwmIx2FgMiMC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "e0e5d4ce-6e47-4978-f6e8-eea8b0b2550f"
      },
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------------\n",
        "# convert positive/negative documents into numpy array\n",
        "# note that reviews vary from 22 to 1052 words   \n",
        "# so we use the first 20 and last 20 words of each review \n",
        "# as our word sequences for analysis\n",
        "# -----------------------------------------------------\n",
        "max_review_length = 0  # initialize\n",
        "for doc in negative_documents:\n",
        "    max_review_length = max(max_review_length, len(doc))    \n",
        "    \n",
        "for doc in positive_documents:\n",
        "    max_review_length = max(max_review_length, len(doc)) \n",
        "print('max_review_length:', max_review_length) \n",
        "\n",
        "min_review_length = max_review_length  # initialize\n",
        "for doc in negative_documents:\n",
        "    min_review_length = min(min_review_length, len(doc))    \n",
        "for doc in positive_documents:\n",
        "    min_review_length = min(min_review_length, len(doc)) \n",
        "print('min_review_length:', min_review_length) \n",
        "\n"
      ],
      "execution_count": 313,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "max_review_length: 1052\n",
            "min_review_length: 22\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Ne0mEev_Mlq9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# construct list of 1000 lists with 40 words in each list\n",
        "from itertools import chain\n",
        "documents = []\n",
        "for doc in negative_documents:\n",
        "    doc_begin = doc[0:20]\n",
        "    doc_end = doc[len(doc) - 20: len(doc)]\n",
        "    documents.append(list(chain(*[doc_begin, doc_end])))    \n",
        "for doc in positive_documents:\n",
        "    doc_begin = doc[0:20]\n",
        "    doc_end = doc[len(doc) - 20: len(doc)]\n",
        "    documents.append(list(chain(*[doc_begin, doc_end])))    \n",
        "\n",
        "# create list of lists of lists for embeddings\n",
        "embeddings = []    \n",
        "for doc in documents:\n",
        "    embedding = []\n",
        "    for word in doc:\n",
        "       embedding.append(limited_index_to_embedding[limited_word_to_index[word]]) \n",
        "    embeddings.append(embedding)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zcQHPX_U0fEl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "af57a687-a2a0-40fe-fec5-0bb404eaaac4"
      },
      "cell_type": "code",
      "source": [
        "embeddings=np.array(embeddings)\n",
        "embeddings.shape"
      ],
      "execution_count": 315,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1000, 40, 50)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 315
        }
      ]
    },
    {
      "metadata": {
        "id": "XV9OVduBMpi2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------------    \n",
        "# Check on the embeddings list of list of lists \n",
        "# -----------------------------------------------------\n",
        "# Show the first word in the first document\n",
        "test_word = documents[0][0]    \n",
        "print('First word in first document:', test_word)    \n",
        "print('Embedding for this word:\\n', \n",
        "      limited_index_to_embedding[limited_word_to_index[test_word]])\n",
        "print('Corresponding embedding from embeddings list of list of lists\\n',\n",
        "      embeddings[0][0][:])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NS5Pg9VXMr9P",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Show the seventh word in the tenth document\n",
        "test_word = documents[6][9]    \n",
        "print('First word in first document:', test_word)    \n",
        "print('Embedding for this word:\\n', \n",
        "      limited_index_to_embedding[limited_word_to_index[test_word]])\n",
        "print('Corresponding embedding from embeddings list of list of lists\\n',\n",
        "      embeddings[6][9][:])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "X_16Z8H5MtrW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Show the last word in the last document\n",
        "test_word = documents[999][39]    \n",
        "print('First word in first document:', test_word)    \n",
        "print('Embedding for this word:\\n', \n",
        "      limited_index_to_embedding[limited_word_to_index[test_word]])\n",
        "print('Corresponding embedding from embeddings list of list of lists\\n',\n",
        "      embeddings[999][39][:])        \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-ss7AmmGMvRJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "32a1a6b8-3b2d-4125-a535-d19bf1da18bc"
      },
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------------    \n",
        "# Make embeddings a numpy array for use in an RNN \n",
        "# Create training and test sets with Scikit Learn\n",
        "# -----------------------------------------------------\n",
        "embeddings_array = np.array(embeddings)\n",
        "\n",
        "\n",
        "embeddings_array.shape"
      ],
      "execution_count": 227,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1000, 40, 50)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 227
        }
      ]
    },
    {
      "metadata": {
        "id": "T2vYJ9nU1cIT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Define the labels to be used 500 negative (0) and 500 positive (1)\n",
        "thumbs_down_up = np.concatenate((np.zeros((500), dtype = np.int32), \n",
        "                      np.ones((500), dtype = np.int32)), axis = 0)\n",
        "\n",
        "# Scikit Learn for random splitting of the data  \n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Random splitting of the data in to training (80%) and test (20%)  \n",
        "X_train, X_test, y_train, y_test = \\\n",
        "    train_test_split(embeddings_array, thumbs_down_up, test_size=0.20, \n",
        "                     random_state = RANDOM_SEED)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l9h5ESLIzGB6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "26684d5b-2130-4092-8647-b63a30f394d3"
      },
      "cell_type": "code",
      "source": [
        "embeddings_array.shape"
      ],
      "execution_count": 229,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1000, 40, 50)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 229
        }
      ]
    },
    {
      "metadata": {
        "id": "0j8LCtUbM2SG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# --------------------------------------------------------------------------      \n",
        "# We use a very simple Recurrent Neural Network for this assignment\n",
        "# Gron, A. 2017. Hands-On Machine Learning with Scikit-Learn & TensorFlow: \n",
        "#    Concepts, Tools, and Techniques to Build Intelligent Systems. \n",
        "#    Sebastopol, Calif.: O'Reilly. [ISBN-13 978-1-491-96229-9] \n",
        "#    Chapter 14 Recurrent Neural Networks, pages 390-391\n",
        "#    Source code available at https://github.com/ageron/handson-ml\n",
        "#    Jupyter notebook file 14_recurrent_neural_networks.ipynb\n",
        "#    See section on Training an sequence Classifier, # In [34]:\n",
        "#    which uses the MNIST case data...  we revise to accommodate\n",
        "#    the movie review data in this assignment    \n",
        "# --------------------------------------------------------------------------  \n",
        "reset_graph()\n",
        "\n",
        "n_steps = embeddings_array.shape[1]  # number of words per document \n",
        "n_inputs = embeddings_array.shape[2]  # dimension of  pre-trained embeddings\n",
        "n_neurons = 20  # analyst specified number of neurons\n",
        "n_outputs = 2  # thumbs-down or thumbs-up\n",
        "\n",
        "learning_rate = 0.001"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "r-Kp2HZnFeX-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
        "y = tf.placeholder(tf.int32, [None])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "guPZeq8tM-v8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "basic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)\n",
        "outputs, states = tf.nn.dynamic_rnn(basic_cell, X, dtype=tf.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NUc3ZtY8NAuR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "logits = tf.layers.dense(states, n_outputs)\n",
        "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,\n",
        "                                                          logits=logits)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "H4pLo-MNNDQO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "loss = tf.reduce_mean(xentropy)\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "training_op = optimizer.minimize(loss)\n",
        "correct = tf.nn.in_top_k(logits, y, 1)\n",
        "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Jc9_XCYoNFNe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "init = tf.global_variables_initializer()\n",
        "\n",
        "n_epochs = 50\n",
        "batch_size = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CcINcAVxNHxF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with tf.Session() as sess:\n",
        "    init.run()\n",
        "    for epoch in range(n_epochs):\n",
        "        print('\\n  ---- Epoch ', epoch, ' ----\\n')\n",
        "        for iteration in range(y_train.shape[0] // batch_size):          \n",
        "            X_batch = X_train[iteration*batch_size:(iteration + 1)*batch_size,:]\n",
        "            y_batch = y_train[iteration*batch_size:(iteration + 1)*batch_size]\n",
        "            print('  Batch ', iteration, ' training observations from ',  \n",
        "                  iteration*batch_size, ' to ', (iteration + 1)*batch_size-1,)\n",
        "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
        "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
        "        acc_test = accuracy.eval(feed_dict={X: X_test, y: y_test})\n",
        "        print('\\n  Train accuracy:', acc_train, 'Test accuracy:', acc_test)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XH-o-Ps6M1vQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Second Experiment"
      ]
    },
    {
      "metadata": {
        "id": "TAUFXxDaM5jf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "K_eD3tj-M7t7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# To make output stable across runs\n",
        "def reset_graph(seed= RANDOM_SEED):\n",
        "    tf.reset_default_graph()\n",
        "    tf.set_random_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "REMOVE_STOPWORDS = False  # no stopword removal \n",
        "\n",
        "EVOCABSIZE = 100000  # specify desired size of pre-defined embedding vocabulary "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NFgQ-DmJM7uI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------------- \n",
        "# Select the pre-defined embeddings source        \n",
        "# Define vocabulary size for the language model    \n",
        "# Create a word_to_embedding_dict for GloVe.6B.50d\n",
        "embeddings_directory = 'embeddings/glove.6B'\n",
        "filename = 'glove.6B.50d.txt'\n",
        "embeddings_filename = os.path.join(embeddings_directory, filename)\n",
        "# ------------------------------------------------------------- "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4RiGdqbCM7uN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Utility function for loading embeddings follows methods described in\n",
        "# https://github.com/guillaume-chevalier/GloVe-as-a-TensorFlow-Embedding-Layer\n",
        "# Creates the Python defaultdict dictionary word_to_embedding_dict\n",
        "# for the requested pre-trained word embeddings\n",
        "# \n",
        "# Note the use of defaultdict data structure from the Python Standard Library\n",
        "# collections_defaultdict.py lets the caller specify a default value up front\n",
        "# The default value will be retuned if the key is not a known dictionary key\n",
        "# That is, unknown words are represented by a vector of zeros\n",
        "# For word embeddings, this default value is a vector of zeros\n",
        "# Documentation for the Python standard library:\n",
        "#   Hellmann, D. 2017. The Python 3 Standard Library by Example. Boston: \n",
        "#     Addison-Wesley. [ISBN-13: 978-0-13-429105-5]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zZtRtkTwM7uY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "4ee00a7a-0c4d-4030-b1f0-a1120421c078"
      },
      "cell_type": "code",
      "source": [
        "# Utility function for loading embeddings follows methods described in\n",
        "# https://github.com/guillaume-chevalier/GloVe-as-a-TensorFlow-Embedding-Layer\n",
        "# Creates the Python defaultdict dictionary word_to_embedding_dict\n",
        "# for the requested pre-trained word embeddings\n",
        "# \n",
        "# Note the use of defaultdict data structure from the Python Standard Library\n",
        "# collections_defaultdict.py lets the caller specify a default value up front\n",
        "# The default value will be retuned if the key is not a known dictionary key\n",
        "# That is, unknown words are represented by a vector of zeros\n",
        "# For word embeddings, this default value is a vector of zeros\n",
        "# Documentation for the Python standard library:\n",
        "#   Hellmann, D. 2017. The Python 3 Standard Library by Example. Boston: \n",
        "#     Addison-Wesley. [ISBN-13: 978-0-13-429105-5]\n",
        "def load_embedding_from_disks(embeddings_filename, with_indexes=True):\n",
        "    \"\"\"\n",
        "    Read a embeddings txt file. If `with_indexes=True`, \n",
        "    we return a tuple of two dictionnaries\n",
        "    `(word_to_index_dict, index_to_embedding_array)`, \n",
        "    otherwise we return only a direct \n",
        "    `word_to_embedding_dict` dictionnary mapping \n",
        "    from a string to a numpy array.\n",
        "    \"\"\"\n",
        "    if with_indexes:\n",
        "        word_to_index_dict = dict()\n",
        "        index_to_embedding_array = []\n",
        "  \n",
        "    else:\n",
        "        word_to_embedding_dict = dict()\n",
        "\n",
        "    with open(embeddings_filename, 'r') as embeddings_file:\n",
        "        for (i, line) in enumerate(embeddings_file):\n",
        "\n",
        "            split = line.split(' ')\n",
        "\n",
        "            word = split[0]\n",
        "\n",
        "            representation = split[1:]\n",
        "            representation = np.array(\n",
        "                [float(val) for val in representation]\n",
        "            )\n",
        "\n",
        "            if with_indexes:\n",
        "                word_to_index_dict[word] = i\n",
        "                index_to_embedding_array.append(representation)\n",
        "            else:\n",
        "                word_to_embedding_dict[word] = representation\n",
        "\n",
        "    # Empty representation for unknown words.\n",
        "    _WORD_NOT_FOUND = [0.0] * len(representation)\n",
        "    if with_indexes:\n",
        "        _LAST_INDEX = i + 1\n",
        "        word_to_index_dict = defaultdict(\n",
        "            lambda: _LAST_INDEX, word_to_index_dict)\n",
        "        index_to_embedding_array = np.array(\n",
        "            index_to_embedding_array + [_WORD_NOT_FOUND])\n",
        "        return word_to_index_dict, index_to_embedding_array\n",
        "    else:\n",
        "        word_to_embedding_dict = defaultdict(lambda: _WORD_NOT_FOUND)\n",
        "        return word_to_embedding_dict\n",
        "\n",
        "print('\\nLoading embeddings from', embeddings_filename)\n",
        "word_to_index, index_to_embedding = \\\n",
        "    load_embedding_from_disks(embeddings_filename, with_indexes=True)\n",
        "print(\"Embedding loaded from disks.\")"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Loading embeddings from embeddings/glove.6B/glove.6B.50d.txt\n",
            "Embedding loaded from disks.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-lgUW7FiM7uc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "outputId": "950ae34a-97c4-47a3-bb7b-58822e88475b"
      },
      "cell_type": "code",
      "source": [
        "# Note: unknown words have representations with values [0, 0, ..., 0]\n",
        "\n",
        "# Additional background code from\n",
        "# https://github.com/guillaume-chevalier/GloVe-as-a-TensorFlow-Embedding-Layer\n",
        "# shows the general structure of the data structures for word embeddings\n",
        "# This code is modified for our purposes in language modeling \n",
        "vocab_size, embedding_dim = index_to_embedding.shape\n",
        "print(\"Embedding is of shape: {}\".format(index_to_embedding.shape))\n",
        "print(\"This means (number of words, number of dimensions per word)\\n\")\n",
        "print(\"The first words are words that tend occur more often.\")\n",
        "\n",
        "print(\"Note: for unknown words, the representation is an empty vector,\\n\"\n",
        "      \"and the index is the last one. The dictionnary has a limit:\")\n",
        "print(\"    {} --> {} --> {}\".format(\"A word\", \"Index in embedding\", \n",
        "      \"Representation\"))\n",
        "word = \"worsdfkljsdf\"  # a word obviously not in the vocabulary\n",
        "idx = word_to_index[word] # index for word obviously not in the vocabulary\n",
        "complete_vocabulary_size = idx \n",
        "embd = list(np.array(index_to_embedding[idx], dtype=int)) # \"int\" compact print\n",
        "print(\"    {} --> {} --> {}\".format(word, idx, embd))\n",
        "word = \"the\"\n",
        "idx = word_to_index[word]\n",
        "embd = list(index_to_embedding[idx])  # \"int\" for compact print only.\n",
        "print(\"    {} --> {} --> {}\".format(word, idx, embd))"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Embedding is of shape: (400001, 50)\n",
            "This means (number of words, number of dimensions per word)\n",
            "\n",
            "The first words are words that tend occur more often.\n",
            "Note: for unknown words, the representation is an empty vector,\n",
            "and the index is the last one. The dictionnary has a limit:\n",
            "    A word --> Index in embedding --> Representation\n",
            "    worsdfkljsdf --> 400000 --> [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "    the --> 0 --> [0.418, 0.24968, -0.41242, 0.1217, 0.34527, -0.044457, -0.49688, -0.17862, -0.00066023, -0.6566, 0.27843, -0.14767, -0.55677, 0.14658, -0.0095095, 0.011658, 0.10204, -0.12792, -0.8443, -0.12181, -0.016801, -0.33279, -0.1552, -0.23131, -0.19181, -1.8823, -0.76746, 0.099051, -0.42125, -0.19526, 4.0071, -0.18594, -0.52287, -0.31681, 0.00059213, 0.0074449, 0.17778, -0.15897, 0.012041, -0.054223, -0.29871, -0.15749, -0.34758, -0.045637, -0.44251, 0.18785, 0.0027849, -0.18411, -0.11514, -0.78581]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "MoknCINGM7ug",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1428
        },
        "outputId": "3bc75acc-52ea-4b47-c30f-846ab4c90b52"
      },
      "cell_type": "code",
      "source": [
        "# Show how to use embeddings dictionaries with a test sentence\n",
        "# This is a famous typing exercise with all letters of the alphabet\n",
        "# https://en.wikipedia.org/wiki/The_quick_brown_fox_jumps_over_the_lazy_dog\n",
        "a_typing_test_sentence = 'The quick brown fox jumps over the lazy dog'\n",
        "print('\\nTest sentence: ', a_typing_test_sentence, '\\n')\n",
        "words_in_test_sentence = a_typing_test_sentence.split()\n",
        "\n",
        "print('Test sentence embeddings from complete vocabulary of', \n",
        "      complete_vocabulary_size, 'words:\\n')\n",
        "for word in words_in_test_sentence:\n",
        "    word_ = word.lower()\n",
        "    embedding = index_to_embedding[word_to_index[word_]]\n",
        "    print(word_ + \": \", embedding)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test sentence:  The quick brown fox jumps over the lazy dog \n",
            "\n",
            "Test sentence embeddings from complete vocabulary of 400000 words:\n",
            "\n",
            "the:  [ 4.1800e-01  2.4968e-01 -4.1242e-01  1.2170e-01  3.4527e-01 -4.4457e-02\n",
            " -4.9688e-01 -1.7862e-01 -6.6023e-04 -6.5660e-01  2.7843e-01 -1.4767e-01\n",
            " -5.5677e-01  1.4658e-01 -9.5095e-03  1.1658e-02  1.0204e-01 -1.2792e-01\n",
            " -8.4430e-01 -1.2181e-01 -1.6801e-02 -3.3279e-01 -1.5520e-01 -2.3131e-01\n",
            " -1.9181e-01 -1.8823e+00 -7.6746e-01  9.9051e-02 -4.2125e-01 -1.9526e-01\n",
            "  4.0071e+00 -1.8594e-01 -5.2287e-01 -3.1681e-01  5.9213e-04  7.4449e-03\n",
            "  1.7778e-01 -1.5897e-01  1.2041e-02 -5.4223e-02 -2.9871e-01 -1.5749e-01\n",
            " -3.4758e-01 -4.5637e-02 -4.4251e-01  1.8785e-01  2.7849e-03 -1.8411e-01\n",
            " -1.1514e-01 -7.8581e-01]\n",
            "quick:  [ 0.13967   -0.53798   -0.18047   -0.25142    0.16203   -0.13868\n",
            " -0.24637    0.75111    0.27264    0.61035   -0.82548    0.038647\n",
            " -0.32361    0.30373   -0.14598   -0.23551    0.39267   -1.1287\n",
            " -0.23636   -1.0629     0.046277   0.29143   -0.25819   -0.094902\n",
            "  0.79478   -1.2095    -0.01039   -0.092086   0.84322   -0.11061\n",
            "  3.0096     0.51652   -0.76986    0.51074    0.37508    0.12156\n",
            "  0.082794   0.43605   -0.1584    -0.61048    0.35006    0.52465\n",
            " -0.51747    0.0034705  0.73625    0.16252    0.85279    0.85268\n",
            "  0.57892    0.64483  ]\n",
            "brown:  [-0.88497   0.71685  -0.40379  -0.10698   0.81457   1.0258   -1.2698\n",
            " -0.49382  -0.27839  -0.92251  -0.49409   0.78942  -0.20066  -0.057371\n",
            "  0.060682  0.30746   0.13441  -0.49376  -0.54788  -0.81912  -0.45394\n",
            "  0.52098   1.0325   -0.8584   -0.65848  -1.2736    0.23616   1.0486\n",
            "  0.18442  -0.3901    2.1385   -0.45301  -0.16911  -0.46737   0.15938\n",
            " -0.095071 -0.26512  -0.056479  0.63849  -1.0494    0.037507  0.76434\n",
            " -0.6412   -0.59594   0.46589   0.31494  -0.34072  -0.59167  -0.31057\n",
            "  0.73274 ]\n",
            "fox:  [ 0.44206   0.059552  0.15861   0.92777   0.1876    0.24256  -1.593\n",
            " -0.79847  -0.34099  -0.24021  -0.32756   0.43639  -0.11057   0.50472\n",
            "  0.43853   0.19738  -0.1498   -0.046979 -0.83286   0.39878   0.062174\n",
            "  0.28803   0.79134   0.31798  -0.21933  -1.1015   -0.080309  0.39122\n",
            "  0.19503  -0.5936    1.7921    0.3826   -0.30509  -0.58686  -0.76935\n",
            " -0.61914  -0.61771  -0.68484  -0.67919  -0.74626  -0.036646  0.78251\n",
            " -1.0072   -0.59057  -0.7849   -0.39113  -0.49727  -0.4283   -0.15204\n",
            "  1.5064  ]\n",
            "jumps:  [-0.46105   -0.34219    0.71473   -0.29778    0.28839    0.6248\n",
            "  0.36807   -0.072746   0.60476    0.31463   -0.052247  -0.62302\n",
            " -0.56332    0.7855     0.18116   -0.31698    0.38298   -0.081953\n",
            " -1.3658    -0.78263    0.39804   -0.17001   -0.11926   -0.40146\n",
            "  1.1057    -0.51142   -0.36614    0.22177    0.34626   -0.30648\n",
            "  1.3869     0.77328    0.5946     1.2577     0.23472   -0.46087\n",
            " -0.009223   0.44534    0.012732  -0.24749   -0.7142     0.02422\n",
            "  0.083527   0.25088   -0.24259   -1.354      1.5481    -0.31728\n",
            "  0.55305   -0.0028062]\n",
            "over:  [ 0.12972    0.088073   0.24375    0.078102  -0.12783    0.27831\n",
            " -0.48693    0.19649   -0.39558   -0.28362   -0.47425   -0.59317\n",
            " -0.58804   -0.31702    0.49593    0.0087594  0.039613  -0.42495\n",
            " -0.97641   -0.46534    0.020675   0.086042   0.39317   -0.51255\n",
            " -0.17913   -1.8333     0.5622     0.41626    0.075127   0.02189\n",
            "  3.784      0.71067   -0.073943   0.15373   -0.3853    -0.070163\n",
            " -0.35374    0.074501  -0.084228  -0.45548   -0.081068   0.39157\n",
            "  0.173      0.2254    -0.12836    0.40951   -0.26079    0.090912\n",
            " -0.60515   -0.9827   ]\n",
            "the:  [ 4.1800e-01  2.4968e-01 -4.1242e-01  1.2170e-01  3.4527e-01 -4.4457e-02\n",
            " -4.9688e-01 -1.7862e-01 -6.6023e-04 -6.5660e-01  2.7843e-01 -1.4767e-01\n",
            " -5.5677e-01  1.4658e-01 -9.5095e-03  1.1658e-02  1.0204e-01 -1.2792e-01\n",
            " -8.4430e-01 -1.2181e-01 -1.6801e-02 -3.3279e-01 -1.5520e-01 -2.3131e-01\n",
            " -1.9181e-01 -1.8823e+00 -7.6746e-01  9.9051e-02 -4.2125e-01 -1.9526e-01\n",
            "  4.0071e+00 -1.8594e-01 -5.2287e-01 -3.1681e-01  5.9213e-04  7.4449e-03\n",
            "  1.7778e-01 -1.5897e-01  1.2041e-02 -5.4223e-02 -2.9871e-01 -1.5749e-01\n",
            " -3.4758e-01 -4.5637e-02 -4.4251e-01  1.8785e-01  2.7849e-03 -1.8411e-01\n",
            " -1.1514e-01 -7.8581e-01]\n",
            "lazy:  [-0.27611  -0.59712  -0.49227  -1.0372   -0.35878  -0.097425 -0.21014\n",
            " -0.092836 -0.054118  0.4542   -0.53296   0.37602   0.77087   0.79669\n",
            " -0.076608 -0.42515   0.42576   0.32791  -0.21996  -0.20261  -0.85139\n",
            "  0.80547   0.97621   0.9792    1.1118   -0.36062  -0.2588    0.8596\n",
            "  0.73631  -0.18601   1.2376   -0.038938  0.19246   0.52473  -0.04842\n",
            " -0.044149  0.064432  0.087822  0.42232  -0.55991  -0.44096   0.097736\n",
            " -0.17589   1.1799    0.13152  -1.0795    0.45685  -0.63312   1.2752\n",
            "  1.1672  ]\n",
            "dog:  [ 0.11008   -0.38781   -0.57615   -0.27714    0.70521    0.53994\n",
            " -1.0786    -0.40146    1.1504    -0.5678     0.0038977  0.52878\n",
            "  0.64561    0.47262    0.48549   -0.18407    0.1801     0.91397\n",
            " -1.1979    -0.5778    -0.37985    0.33606    0.772      0.75555\n",
            "  0.45506   -1.7671    -1.0503     0.42566    0.41893   -0.68327\n",
            "  1.5673     0.27685   -0.61708    0.64638   -0.076996   0.37118\n",
            "  0.1308    -0.45137    0.25398   -0.74392   -0.086199   0.24068\n",
            " -0.64819    0.83549    1.2502    -0.51379    0.04224   -0.88118\n",
            "  0.7158     0.38519  ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4W4myhLSM7ul",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------------- \n",
        "# Define vocabulary size for the language model    \n",
        "# To reduce the size of the vocabulary to the n most frequently used words\n",
        "\n",
        "def default_factory():\n",
        "    return EVOCABSIZE  # last/unknown-word row in limited_index_to_embedding\n",
        "# dictionary has the items() function, returns list of (key, value) tuples\n",
        "limited_word_to_index = defaultdict(default_factory, \\\n",
        "    {k: v for k, v in word_to_index.items() if v < EVOCABSIZE})\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aUs93OdfM7us",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Select the first EVOCABSIZE rows to the index_to_embedding\n",
        "limited_index_to_embedding = index_to_embedding[0:EVOCABSIZE,:]\n",
        "# Set the unknown-word row to be all zeros as previously\n",
        "limited_index_to_embedding = np.append(limited_index_to_embedding, \n",
        "    index_to_embedding[index_to_embedding.shape[0] - 1, :].\\\n",
        "        reshape(1,embedding_dim), \n",
        "    axis = 0)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lzzA_HC6M7uy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Delete large numpy array to clear some CPU RAM\n",
        "del index_to_embedding"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LuHYAzmzM7u4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1394
        },
        "outputId": "99f97519-81c4-49a8-ff30-e836cf59cb99"
      },
      "cell_type": "code",
      "source": [
        "# Verify the new vocabulary: should get same embeddings for test sentence\n",
        "# Note that a small EVOCABSIZE may yield some zero vectors for embeddings\n",
        "print('\\nTest sentence embeddings from vocabulary of', EVOCABSIZE, 'words:\\n')\n",
        "for word in words_in_test_sentence:\n",
        "    word_ = word.lower()\n",
        "    embedding = limited_index_to_embedding[limited_word_to_index[word_]]\n",
        "    print(word_ + \": \", embedding)\n",
        "\n"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test sentence embeddings from vocabulary of 100000 words:\n",
            "\n",
            "the:  [ 4.1800e-01  2.4968e-01 -4.1242e-01  1.2170e-01  3.4527e-01 -4.4457e-02\n",
            " -4.9688e-01 -1.7862e-01 -6.6023e-04 -6.5660e-01  2.7843e-01 -1.4767e-01\n",
            " -5.5677e-01  1.4658e-01 -9.5095e-03  1.1658e-02  1.0204e-01 -1.2792e-01\n",
            " -8.4430e-01 -1.2181e-01 -1.6801e-02 -3.3279e-01 -1.5520e-01 -2.3131e-01\n",
            " -1.9181e-01 -1.8823e+00 -7.6746e-01  9.9051e-02 -4.2125e-01 -1.9526e-01\n",
            "  4.0071e+00 -1.8594e-01 -5.2287e-01 -3.1681e-01  5.9213e-04  7.4449e-03\n",
            "  1.7778e-01 -1.5897e-01  1.2041e-02 -5.4223e-02 -2.9871e-01 -1.5749e-01\n",
            " -3.4758e-01 -4.5637e-02 -4.4251e-01  1.8785e-01  2.7849e-03 -1.8411e-01\n",
            " -1.1514e-01 -7.8581e-01]\n",
            "quick:  [ 0.13967   -0.53798   -0.18047   -0.25142    0.16203   -0.13868\n",
            " -0.24637    0.75111    0.27264    0.61035   -0.82548    0.038647\n",
            " -0.32361    0.30373   -0.14598   -0.23551    0.39267   -1.1287\n",
            " -0.23636   -1.0629     0.046277   0.29143   -0.25819   -0.094902\n",
            "  0.79478   -1.2095    -0.01039   -0.092086   0.84322   -0.11061\n",
            "  3.0096     0.51652   -0.76986    0.51074    0.37508    0.12156\n",
            "  0.082794   0.43605   -0.1584    -0.61048    0.35006    0.52465\n",
            " -0.51747    0.0034705  0.73625    0.16252    0.85279    0.85268\n",
            "  0.57892    0.64483  ]\n",
            "brown:  [-0.88497   0.71685  -0.40379  -0.10698   0.81457   1.0258   -1.2698\n",
            " -0.49382  -0.27839  -0.92251  -0.49409   0.78942  -0.20066  -0.057371\n",
            "  0.060682  0.30746   0.13441  -0.49376  -0.54788  -0.81912  -0.45394\n",
            "  0.52098   1.0325   -0.8584   -0.65848  -1.2736    0.23616   1.0486\n",
            "  0.18442  -0.3901    2.1385   -0.45301  -0.16911  -0.46737   0.15938\n",
            " -0.095071 -0.26512  -0.056479  0.63849  -1.0494    0.037507  0.76434\n",
            " -0.6412   -0.59594   0.46589   0.31494  -0.34072  -0.59167  -0.31057\n",
            "  0.73274 ]\n",
            "fox:  [ 0.44206   0.059552  0.15861   0.92777   0.1876    0.24256  -1.593\n",
            " -0.79847  -0.34099  -0.24021  -0.32756   0.43639  -0.11057   0.50472\n",
            "  0.43853   0.19738  -0.1498   -0.046979 -0.83286   0.39878   0.062174\n",
            "  0.28803   0.79134   0.31798  -0.21933  -1.1015   -0.080309  0.39122\n",
            "  0.19503  -0.5936    1.7921    0.3826   -0.30509  -0.58686  -0.76935\n",
            " -0.61914  -0.61771  -0.68484  -0.67919  -0.74626  -0.036646  0.78251\n",
            " -1.0072   -0.59057  -0.7849   -0.39113  -0.49727  -0.4283   -0.15204\n",
            "  1.5064  ]\n",
            "jumps:  [-0.46105   -0.34219    0.71473   -0.29778    0.28839    0.6248\n",
            "  0.36807   -0.072746   0.60476    0.31463   -0.052247  -0.62302\n",
            " -0.56332    0.7855     0.18116   -0.31698    0.38298   -0.081953\n",
            " -1.3658    -0.78263    0.39804   -0.17001   -0.11926   -0.40146\n",
            "  1.1057    -0.51142   -0.36614    0.22177    0.34626   -0.30648\n",
            "  1.3869     0.77328    0.5946     1.2577     0.23472   -0.46087\n",
            " -0.009223   0.44534    0.012732  -0.24749   -0.7142     0.02422\n",
            "  0.083527   0.25088   -0.24259   -1.354      1.5481    -0.31728\n",
            "  0.55305   -0.0028062]\n",
            "over:  [ 0.12972    0.088073   0.24375    0.078102  -0.12783    0.27831\n",
            " -0.48693    0.19649   -0.39558   -0.28362   -0.47425   -0.59317\n",
            " -0.58804   -0.31702    0.49593    0.0087594  0.039613  -0.42495\n",
            " -0.97641   -0.46534    0.020675   0.086042   0.39317   -0.51255\n",
            " -0.17913   -1.8333     0.5622     0.41626    0.075127   0.02189\n",
            "  3.784      0.71067   -0.073943   0.15373   -0.3853    -0.070163\n",
            " -0.35374    0.074501  -0.084228  -0.45548   -0.081068   0.39157\n",
            "  0.173      0.2254    -0.12836    0.40951   -0.26079    0.090912\n",
            " -0.60515   -0.9827   ]\n",
            "the:  [ 4.1800e-01  2.4968e-01 -4.1242e-01  1.2170e-01  3.4527e-01 -4.4457e-02\n",
            " -4.9688e-01 -1.7862e-01 -6.6023e-04 -6.5660e-01  2.7843e-01 -1.4767e-01\n",
            " -5.5677e-01  1.4658e-01 -9.5095e-03  1.1658e-02  1.0204e-01 -1.2792e-01\n",
            " -8.4430e-01 -1.2181e-01 -1.6801e-02 -3.3279e-01 -1.5520e-01 -2.3131e-01\n",
            " -1.9181e-01 -1.8823e+00 -7.6746e-01  9.9051e-02 -4.2125e-01 -1.9526e-01\n",
            "  4.0071e+00 -1.8594e-01 -5.2287e-01 -3.1681e-01  5.9213e-04  7.4449e-03\n",
            "  1.7778e-01 -1.5897e-01  1.2041e-02 -5.4223e-02 -2.9871e-01 -1.5749e-01\n",
            " -3.4758e-01 -4.5637e-02 -4.4251e-01  1.8785e-01  2.7849e-03 -1.8411e-01\n",
            " -1.1514e-01 -7.8581e-01]\n",
            "lazy:  [-0.27611  -0.59712  -0.49227  -1.0372   -0.35878  -0.097425 -0.21014\n",
            " -0.092836 -0.054118  0.4542   -0.53296   0.37602   0.77087   0.79669\n",
            " -0.076608 -0.42515   0.42576   0.32791  -0.21996  -0.20261  -0.85139\n",
            "  0.80547   0.97621   0.9792    1.1118   -0.36062  -0.2588    0.8596\n",
            "  0.73631  -0.18601   1.2376   -0.038938  0.19246   0.52473  -0.04842\n",
            " -0.044149  0.064432  0.087822  0.42232  -0.55991  -0.44096   0.097736\n",
            " -0.17589   1.1799    0.13152  -1.0795    0.45685  -0.63312   1.2752\n",
            "  1.1672  ]\n",
            "dog:  [ 0.11008   -0.38781   -0.57615   -0.27714    0.70521    0.53994\n",
            " -1.0786    -0.40146    1.1504    -0.5678     0.0038977  0.52878\n",
            "  0.64561    0.47262    0.48549   -0.18407    0.1801     0.91397\n",
            " -1.1979    -0.5778    -0.37985    0.33606    0.772      0.75555\n",
            "  0.45506   -1.7671    -1.0503     0.42566    0.41893   -0.68327\n",
            "  1.5673     0.27685   -0.61708    0.64638   -0.076996   0.37118\n",
            "  0.1308    -0.45137    0.25398   -0.74392   -0.086199   0.24068\n",
            " -0.64819    0.83549    1.2502    -0.51379    0.04224   -0.88118\n",
            "  0.7158     0.38519  ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JiYbp3gCM7vC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------------\n",
        "# code for working with movie reviews data \n",
        "# Source: Miller, T. W. (2016). Web and Network Data Science.\n",
        "#    Upper Saddle River, N.J.: Pearson Education.\n",
        "#    ISBN-13: 978-0-13-388644-3\n",
        "# This original study used a simple bag-of-words approach\n",
        "# to sentiment analysis, along with pre-defined lists of\n",
        "# negative and positive words.        \n",
        "# Code available at:  https://github.com/mtpa/wnds       \n",
        "# ------------------------------------------------------------"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aZzc5b0WM7vL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Utility function to get file names within a directory\n",
        "def listdir_no_hidden(path):\n",
        "    start_list = os.listdir(path)\n",
        "    end_list = []\n",
        "    for file in start_list:\n",
        "        if (not file.startswith('.')):\n",
        "            end_list.append(file)\n",
        "    return(end_list)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9u7Sk16jM7vN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# define list of codes to be dropped from document\n",
        "# carriage-returns, line-feeds, tabs\n",
        "codelist = ['\\r', '\\n', '\\t']   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "h2Kfn28pM7vO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# We will not remove stopwords in this exercise because they are\n",
        "# important to keeping sentences intact\n",
        "if REMOVE_STOPWORDS:\n",
        "    print(nltk.corpus.stopwords.words('english'))\n",
        "\n",
        "# previous analysis of a list of top terms showed a number of words, along \n",
        "# with contractions and other word strings to drop from further analysis, add\n",
        "# these to the usual English stopwords to be dropped from a document collection\n",
        "    more_stop_words = ['cant','didnt','doesnt','dont','goes','isnt','hes',\\\n",
        "        'shes','thats','theres','theyre','wont','youll','youre','youve', 'br'\\\n",
        "        've', 're', 'vs'] \n",
        "\n",
        "    some_proper_nouns_to_remove = ['dick','ginger','hollywood','jack',\\\n",
        "        'jill','john','karloff','kudrow','orson','peter','tcm','tom',\\\n",
        "        'toni','welles','william','wolheim','nikita']\n",
        "\n",
        "    # start with the initial list and add to it for movie text work \n",
        "    stoplist = nltk.corpus.stopwords.words('english') + more_stop_words +\\\n",
        "        some_proper_nouns_to_remove"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GceFzKn2M7vW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# text parsing function for creating text documents \n",
        "# there is more we could do for data preparation \n",
        "# stemming... looking for contractions... possessives... \n",
        "# but we will work with what we have in this parsing function\n",
        "# if we want to do stemming at a later time, we can use\n",
        "#     porter = nltk.PorterStemmer()  \n",
        "# in a construction like this\n",
        "#     words_stemmed =  [porter.stem(word) for word in initial_words]  \n",
        "def text_parse(string):\n",
        "    # replace non-alphanumeric with space \n",
        "    temp_string = re.sub('[^a-zA-Z]', '  ', string)    \n",
        "    # replace codes with space\n",
        "    for i in range(len(codelist)):\n",
        "        stopstring = ' ' + codelist[i] + '  '\n",
        "        temp_string = re.sub(stopstring, '  ', temp_string)      \n",
        "    # replace single-character words with space\n",
        "    temp_string = re.sub('\\s.\\s', ' ', temp_string)   \n",
        "    # convert uppercase to lowercase\n",
        "    temp_string = temp_string.lower()    \n",
        "    if REMOVE_STOPWORDS:\n",
        "        # replace selected character strings/stop-words with space\n",
        "        for i in range(len(stoplist)):\n",
        "            stopstring = ' ' + str(stoplist[i]) + ' '\n",
        "            temp_string = re.sub(stopstring, ' ', temp_string)        \n",
        "    # replace multiple blank characters with one blank character\n",
        "    temp_string = re.sub('\\s+', ' ', temp_string)    \n",
        "    return(temp_string)    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vrDIXJUtM7vg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Manually import these"
      ]
    },
    {
      "metadata": {
        "id": "sL1X-HsCM7vs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "a5fdeba4-74cc-4ff8-b182-68b03c4d737f"
      },
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------\n",
        "# gather data for 500 negative movie reviews\n",
        "# -----------------------------------------------\n",
        "dir_name = 'movie-reviews-negative'\n",
        "    \n",
        "filenames = listdir_no_hidden(path=dir_name)\n",
        "num_files = len(filenames)\n",
        "\n",
        "for i in range(len(filenames)):\n",
        "    file_exists = os.path.isfile(os.path.join(dir_name, filenames[i]))\n",
        "    assert file_exists\n",
        "print('\\nDirectory:',dir_name)    \n",
        "print('%d files found' % len(filenames))"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Directory: movie-reviews-negative\n",
            "500 files found\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cUofvp-IM7vx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "b7751725-645f-46eb-b431-b43c722d5e10"
      },
      "cell_type": "code",
      "source": [
        "# Read data for negative movie reviews\n",
        "# Data will be stored in a list of lists where the each list represents \n",
        "# a document and document is a list of words.\n",
        "# We then break the text into words.\n",
        "\n",
        "def read_data(filename):\n",
        "\n",
        "  #with open(filename, encoding='utf-8') as f:\n",
        "  with open(filename) as f:\n",
        "    data = tf.compat.as_str(f.read())\n",
        "    data = data.lower()\n",
        "    data = text_parse(data)\n",
        "    data = TreebankWordTokenizer().tokenize(data)  # The Penn Treebank\n",
        "\n",
        "  return data\n",
        "\n",
        "negative_documents = []\n",
        "\n",
        "print('\\nProcessing document files under', dir_name)\n",
        "for i in range(num_files):\n",
        "    ## print(' ', filenames[i])\n",
        "\n",
        "    words = read_data(os.path.join(dir_name, filenames[i]))\n",
        "\n",
        "    negative_documents.append(words)\n",
        "    # print('Data size (Characters) (Document %d) %d' %(i,len(words)))\n",
        "    # print('Sample string (Document %d) %s'%(i,words[:50]))"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Processing document files under movie-reviews-negative\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "SbxvjYgwM7v6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "aab62cb0-ac04-4b5a-c5c8-bddf9e25b798"
      },
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------\n",
        "# gather data for 500 positive movie reviews\n",
        "# -----------------------------------------------\n",
        "dir_name = 'movie-reviews-positive'  \n",
        "filenames = listdir_no_hidden(path=dir_name)\n",
        "num_files = len(filenames)\n",
        "\n",
        "for i in range(len(filenames)):\n",
        "    file_exists = os.path.isfile(os.path.join(dir_name, filenames[i]))\n",
        "    assert file_exists\n",
        "print('\\nDirectory:',dir_name)    \n",
        "print('%d files found' % len(filenames))"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Directory: movie-reviews-positive\n",
            "500 files found\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "klzPDYoPM7v-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "5f15e819-c4fe-4934-8124-2fb015f91a99"
      },
      "cell_type": "code",
      "source": [
        "# Read data for positive movie reviews\n",
        "# Data will be stored in a list of lists where the each list \n",
        "# represents a document and document is a list of words.\n",
        "# We then break the text into words.\n",
        "\n",
        "def read_data(filename):\n",
        "\n",
        "  with open(filename) as f:\n",
        "    data = tf.compat.as_str(f.read())\n",
        "    data = data.lower()\n",
        "    data = text_parse(data)\n",
        "    data = TreebankWordTokenizer().tokenize(data)  # The Penn Treebank\n",
        "\n",
        "  return data\n",
        "\n",
        "positive_documents = []\n",
        "\n",
        "print('\\nProcessing document files under', dir_name)\n",
        "for i in range(num_files):\n",
        "    ## print(' ', filenames[i])\n",
        "\n",
        "    words = read_data(os.path.join(dir_name, filenames[i]))\n",
        "\n",
        "    positive_documents.append(words)\n",
        "    # print('Data size (Characters) (Document %d) %d' %(i,len(words)))\n",
        "    # print('Sample string (Document %d) %s'%(i,words[:50]))\n",
        "\n"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Processing document files under movie-reviews-positive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "o-BTLLaWM7wC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "1b23b49b-8674-431a-faf5-af2ea09e9704"
      },
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------------\n",
        "# convert positive/negative documents into numpy array\n",
        "# note that reviews vary from 22 to 1052 words   \n",
        "# so we use the first 20 and last 20 words of each review \n",
        "# as our word sequences for analysis\n",
        "# -----------------------------------------------------\n",
        "max_review_length = 0  # initialize\n",
        "for doc in negative_documents:\n",
        "    max_review_length = max(max_review_length, len(doc))    \n",
        "for doc in positive_documents:\n",
        "    max_review_length = max(max_review_length, len(doc)) \n",
        "print('max_review_length:', max_review_length) \n",
        "\n",
        "min_review_length = max_review_length  # initialize\n",
        "for doc in negative_documents:\n",
        "    min_review_length = min(min_review_length, len(doc))    \n",
        "for doc in positive_documents:\n",
        "    min_review_length = min(min_review_length, len(doc)) \n",
        "print('min_review_length:', min_review_length) \n",
        "\n"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "max_review_length: 1052\n",
            "min_review_length: 22\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "A0KIlQa8M7wK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# construct list of 1000 lists with 40 words in each list\n",
        "from itertools import chain\n",
        "documents = []\n",
        "for doc in negative_documents:\n",
        "    doc_begin = doc[0:20]\n",
        "    doc_end = doc[len(doc) - 20: len(doc)]\n",
        "    documents.append(list(chain(*[doc_begin, doc_end])))    \n",
        "for doc in positive_documents:\n",
        "    doc_begin = doc[0:20]\n",
        "    doc_end = doc[len(doc) - 20: len(doc)]\n",
        "    documents.append(list(chain(*[doc_begin, doc_end])))    \n",
        "\n",
        "# create list of lists of lists for embeddings\n",
        "embeddings = []    \n",
        "for doc in documents:\n",
        "    embedding = []\n",
        "    for word in doc:\n",
        "       embedding.append(limited_index_to_embedding[limited_word_to_index[word]]) \n",
        "    embeddings.append(embedding)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ab4Mtp42M7wN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "6d112bf1-9d03-4f23-c2a0-25481e72010e"
      },
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------------    \n",
        "# Check on the embeddings list of list of lists \n",
        "# -----------------------------------------------------\n",
        "# Show the first word in the first document\n",
        "test_word = documents[0][0]    \n",
        "print('First word in first document:', test_word)    \n",
        "print('Embedding for this word:\\n', \n",
        "      limited_index_to_embedding[limited_word_to_index[test_word]])\n",
        "print('Corresponding embedding from embeddings list of list of lists\\n',\n",
        "      embeddings[0][0][:])\n",
        "\n"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First word in first document: strained\n",
            "Embedding for this word:\n",
            " [ 1.8565e-01  4.5892e-02 -7.9469e-01  4.5509e-02 -3.4420e-01  1.3177e+00\n",
            " -1.8251e-02  9.0133e-01 -2.4033e-01 -2.2524e-01  6.2932e-01 -9.8314e-04\n",
            " -1.0314e+00  3.3354e-01 -3.5601e-02  7.4361e-01 -1.3273e+00 -6.8377e-01\n",
            "  1.0453e+00  9.3062e-01 -9.2818e-01  5.0075e-01  5.6506e-02 -4.6347e-01\n",
            " -1.8777e-01 -1.1040e+00  1.6604e+00  1.1720e+00  8.9319e-01  3.9953e-01\n",
            "  2.0753e+00  7.9695e-01  1.2480e+00 -1.4327e-02 -5.8202e-01  5.8175e-01\n",
            " -5.3769e-01  4.2895e-01 -3.3331e-01 -1.1000e-01 -6.3102e-01 -2.1928e-02\n",
            " -1.9526e-01 -1.3278e-01  4.0494e-01  3.5085e-01 -1.3195e-01  3.3285e-01\n",
            " -1.0968e+00 -3.1978e-01]\n",
            "Corresponding embedding from embeddings list of list of lists\n",
            " [ 1.8565e-01  4.5892e-02 -7.9469e-01  4.5509e-02 -3.4420e-01  1.3177e+00\n",
            " -1.8251e-02  9.0133e-01 -2.4033e-01 -2.2524e-01  6.2932e-01 -9.8314e-04\n",
            " -1.0314e+00  3.3354e-01 -3.5601e-02  7.4361e-01 -1.3273e+00 -6.8377e-01\n",
            "  1.0453e+00  9.3062e-01 -9.2818e-01  5.0075e-01  5.6506e-02 -4.6347e-01\n",
            " -1.8777e-01 -1.1040e+00  1.6604e+00  1.1720e+00  8.9319e-01  3.9953e-01\n",
            "  2.0753e+00  7.9695e-01  1.2480e+00 -1.4327e-02 -5.8202e-01  5.8175e-01\n",
            " -5.3769e-01  4.2895e-01 -3.3331e-01 -1.1000e-01 -6.3102e-01 -2.1928e-02\n",
            " -1.9526e-01 -1.3278e-01  4.0494e-01  3.5085e-01 -1.3195e-01  3.3285e-01\n",
            " -1.0968e+00 -3.1978e-01]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "a1tRTmzgM7wP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "0fa91025-059e-4817-83e2-63bd7e34b8c2"
      },
      "cell_type": "code",
      "source": [
        "# Show the seventh word in the tenth document\n",
        "test_word = documents[6][9]    \n",
        "print('First word in first document:', test_word)    \n",
        "print('Embedding for this word:\\n', \n",
        "      limited_index_to_embedding[limited_word_to_index[test_word]])\n",
        "print('Corresponding embedding from embeddings list of list of lists\\n',\n",
        "      embeddings[6][9][:])\n",
        "\n"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First word in first document: ever\n",
            "Embedding for this word:\n",
            " [-9.1840e-02  4.5829e-02 -1.4578e-02 -1.5912e-01  3.6966e-01 -5.5325e-02\n",
            " -6.4645e-01  3.7028e-01  4.4005e-02  3.1908e-01 -2.2221e-03  5.4468e-02\n",
            " -5.5971e-01  3.2805e-01  1.2631e+00 -2.4974e-02  4.1510e-01  4.1472e-01\n",
            " -8.0000e-01 -3.6899e-01 -2.6027e-01 -1.6870e-03  1.9953e-01  7.3913e-02\n",
            "  5.8280e-01 -2.0068e+00 -1.0067e+00  3.4075e-02  4.1367e-01 -7.2996e-02\n",
            "  2.9454e+00  2.7576e-01  1.0770e-01 -4.0572e-01  1.1923e-03  6.1022e-02\n",
            "  6.9987e-02  5.2403e-01 -7.6733e-01 -9.3638e-01 -8.1252e-01 -1.1230e-01\n",
            "  1.0376e-01  2.9453e-01 -2.7063e-01 -1.2739e-02 -3.1710e-01 -3.3740e-02\n",
            " -3.9644e-01 -1.9175e-02]\n",
            "Corresponding embedding from embeddings list of list of lists\n",
            " [-9.1840e-02  4.5829e-02 -1.4578e-02 -1.5912e-01  3.6966e-01 -5.5325e-02\n",
            " -6.4645e-01  3.7028e-01  4.4005e-02  3.1908e-01 -2.2221e-03  5.4468e-02\n",
            " -5.5971e-01  3.2805e-01  1.2631e+00 -2.4974e-02  4.1510e-01  4.1472e-01\n",
            " -8.0000e-01 -3.6899e-01 -2.6027e-01 -1.6870e-03  1.9953e-01  7.3913e-02\n",
            "  5.8280e-01 -2.0068e+00 -1.0067e+00  3.4075e-02  4.1367e-01 -7.2996e-02\n",
            "  2.9454e+00  2.7576e-01  1.0770e-01 -4.0572e-01  1.1923e-03  6.1022e-02\n",
            "  6.9987e-02  5.2403e-01 -7.6733e-01 -9.3638e-01 -8.1252e-01 -1.1230e-01\n",
            "  1.0376e-01  2.9453e-01 -2.7063e-01 -1.2739e-02 -3.1710e-01 -3.3740e-02\n",
            " -3.9644e-01 -1.9175e-02]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "X60up2DFM7wS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "700528a8-6060-48b0-d5c7-df93acda0dc6"
      },
      "cell_type": "code",
      "source": [
        "# Show the last word in the last document\n",
        "test_word = documents[999][39]    \n",
        "print('First word in first document:', test_word)    \n",
        "print('Embedding for this word:\\n', \n",
        "      limited_index_to_embedding[limited_word_to_index[test_word]])\n",
        "print('Corresponding embedding from embeddings list of list of lists\\n',\n",
        "      embeddings[999][39][:])        \n",
        "\n"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First word in first document: out\n",
            "Embedding for this word:\n",
            " [ 3.2112e-01 -6.9306e-01  4.7922e-01 -5.4602e-01  2.8352e-01  2.0346e-01\n",
            " -9.8445e-01 -1.4103e-01 -1.3147e-01 -8.5975e-02 -4.9509e-01  2.7600e-03\n",
            " -1.1173e+00  3.3729e-01  6.1312e-01 -6.7110e-02  3.5380e-01 -3.5183e-01\n",
            " -5.8191e-01 -6.9525e-01 -2.5032e-02  6.1675e-01  7.8522e-01 -1.9594e-01\n",
            "  2.6324e-01 -1.8976e+00  1.4645e-01  4.8885e-01  6.1818e-01 -1.0120e+00\n",
            "  3.7285e+00  6.6615e-01 -3.3364e-01  3.1896e-01 -1.5174e-01  3.0980e-01\n",
            "  4.9670e-02  2.7144e-01  3.4595e-01 -8.1850e-02 -3.7469e-01  3.9981e-01\n",
            "  8.4925e-02  3.1237e-01 -1.2677e-01  3.6322e-02 -6.9533e-02 -4.3547e-01\n",
            " -1.1080e-01 -5.8500e-01]\n",
            "Corresponding embedding from embeddings list of list of lists\n",
            " [ 3.2112e-01 -6.9306e-01  4.7922e-01 -5.4602e-01  2.8352e-01  2.0346e-01\n",
            " -9.8445e-01 -1.4103e-01 -1.3147e-01 -8.5975e-02 -4.9509e-01  2.7600e-03\n",
            " -1.1173e+00  3.3729e-01  6.1312e-01 -6.7110e-02  3.5380e-01 -3.5183e-01\n",
            " -5.8191e-01 -6.9525e-01 -2.5032e-02  6.1675e-01  7.8522e-01 -1.9594e-01\n",
            "  2.6324e-01 -1.8976e+00  1.4645e-01  4.8885e-01  6.1818e-01 -1.0120e+00\n",
            "  3.7285e+00  6.6615e-01 -3.3364e-01  3.1896e-01 -1.5174e-01  3.0980e-01\n",
            "  4.9670e-02  2.7144e-01  3.4595e-01 -8.1850e-02 -3.7469e-01  3.9981e-01\n",
            "  8.4925e-02  3.1237e-01 -1.2677e-01  3.6322e-02 -6.9533e-02 -4.3547e-01\n",
            " -1.1080e-01 -5.8500e-01]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bH1Ue_PKM7wf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------------    \n",
        "# Make embeddings a numpy array for use in an RNN \n",
        "# Create training and test sets with Scikit Learn\n",
        "# -----------------------------------------------------\n",
        "embeddings_array = np.array(embeddings)\n",
        "\n",
        "# Define the labels to be used 500 negative (0) and 500 positive (1)\n",
        "thumbs_down_up = np.concatenate((np.zeros((500), dtype = np.int32), \n",
        "                      np.ones((500), dtype = np.int32)), axis = 0)\n",
        "\n",
        "# Scikit Learn for random splitting of the data  \n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Random splitting of the data in to training (80%) and test (20%)  \n",
        "X_train, X_test, y_train, y_test = \\\n",
        "    train_test_split(embeddings_array, thumbs_down_up, test_size=0.20, \n",
        "                     random_state = RANDOM_SEED)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kqRz6xPBM7wh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# --------------------------------------------------------------------------      \n",
        "# We use a very simple Recurrent Neural Network for this assignment\n",
        "# Gron, A. 2017. Hands-On Machine Learning with Scikit-Learn & TensorFlow: \n",
        "#    Concepts, Tools, and Techniques to Build Intelligent Systems. \n",
        "#    Sebastopol, Calif.: O'Reilly. [ISBN-13 978-1-491-96229-9] \n",
        "#    Chapter 14 Recurrent Neural Networks, pages 390-391\n",
        "#    Source code available at https://github.com/ageron/handson-ml\n",
        "#    Jupyter notebook file 14_recurrent_neural_networks.ipynb\n",
        "#    See section on Training an sequence Classifier, # In [34]:\n",
        "#    which uses the MNIST case data...  we revise to accommodate\n",
        "#    the movie review data in this assignment    \n",
        "# --------------------------------------------------------------------------  \n",
        "reset_graph()\n",
        "\n",
        "n_steps = embeddings_array.shape[1]  # number of words per document \n",
        "n_inputs = embeddings_array.shape[2]  # dimension of  pre-trained embeddings\n",
        "n_neurons = 20  # analyst specified number of neurons\n",
        "n_outputs = 2  # thumbs-down or thumbs-up\n",
        "\n",
        "learning_rate = 0.001"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iHDR0lEuM7wm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
        "y = tf.placeholder(tf.int32, [None])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KCrwW8-IM7wp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "basic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)\n",
        "outputs, states = tf.nn.dynamic_rnn(basic_cell, X, dtype=tf.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XcBvUleaM7wq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "logits = tf.layers.dense(states, n_outputs)\n",
        "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,\n",
        "                                                          logits=logits)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eUku0ANvM7ws",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "loss = tf.reduce_mean(xentropy)\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "training_op = optimizer.minimize(loss)\n",
        "correct = tf.nn.in_top_k(logits, y, 1)\n",
        "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ig4ua7HhM7wu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "init = tf.global_variables_initializer()\n",
        "\n",
        "n_epochs = 50\n",
        "batch_size = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VWjpz7TxM7ww",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 11067
        },
        "outputId": "9604e3ec-6e1a-446c-ad93-4d754ddb85a6"
      },
      "cell_type": "code",
      "source": [
        "with tf.Session() as sess:\n",
        "    init.run()\n",
        "    for epoch in range(n_epochs):\n",
        "        print('\\n  ---- Epoch ', epoch, ' ----\\n')\n",
        "        for iteration in range(y_train.shape[0] // batch_size):          \n",
        "            X_batch = X_train[iteration*batch_size:(iteration + 1)*batch_size,:]\n",
        "            y_batch = y_train[iteration*batch_size:(iteration + 1)*batch_size]\n",
        "            print('  Batch ', iteration, ' training observations from ',  \n",
        "                  iteration*batch_size, ' to ', (iteration + 1)*batch_size-1,)\n",
        "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
        "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
        "        acc_test = accuracy.eval(feed_dict={X: X_test, y: y_test})\n",
        "        print('\\n  Train accuracy:', acc_train, 'Test accuracy:', acc_test)\n",
        "\n",
        "\n"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  ---- Epoch  0  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.5 Test accuracy: 0.505\n",
            "\n",
            "  ---- Epoch  1  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.46 Test accuracy: 0.49\n",
            "\n",
            "  ---- Epoch  2  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.5 Test accuracy: 0.51\n",
            "\n",
            "  ---- Epoch  3  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.46 Test accuracy: 0.505\n",
            "\n",
            "  ---- Epoch  4  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.5 Test accuracy: 0.495\n",
            "\n",
            "  ---- Epoch  5  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.55 Test accuracy: 0.47\n",
            "\n",
            "  ---- Epoch  6  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.58 Test accuracy: 0.495\n",
            "\n",
            "  ---- Epoch  7  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.58 Test accuracy: 0.5\n",
            "\n",
            "  ---- Epoch  8  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.59 Test accuracy: 0.48\n",
            "\n",
            "  ---- Epoch  9  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.61 Test accuracy: 0.5\n",
            "\n",
            "  ---- Epoch  10  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.61 Test accuracy: 0.505\n",
            "\n",
            "  ---- Epoch  11  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.62 Test accuracy: 0.505\n",
            "\n",
            "  ---- Epoch  12  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.64 Test accuracy: 0.51\n",
            "\n",
            "  ---- Epoch  13  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.64 Test accuracy: 0.51\n",
            "\n",
            "  ---- Epoch  14  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.63 Test accuracy: 0.53\n",
            "\n",
            "  ---- Epoch  15  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.64 Test accuracy: 0.535\n",
            "\n",
            "  ---- Epoch  16  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.65 Test accuracy: 0.54\n",
            "\n",
            "  ---- Epoch  17  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.67 Test accuracy: 0.53\n",
            "\n",
            "  ---- Epoch  18  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.67 Test accuracy: 0.53\n",
            "\n",
            "  ---- Epoch  19  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.69 Test accuracy: 0.53\n",
            "\n",
            "  ---- Epoch  20  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.7 Test accuracy: 0.52\n",
            "\n",
            "  ---- Epoch  21  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.72 Test accuracy: 0.52\n",
            "\n",
            "  ---- Epoch  22  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.72 Test accuracy: 0.525\n",
            "\n",
            "  ---- Epoch  23  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.72 Test accuracy: 0.515\n",
            "\n",
            "  ---- Epoch  24  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.73 Test accuracy: 0.535\n",
            "\n",
            "  ---- Epoch  25  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.73 Test accuracy: 0.54\n",
            "\n",
            "  ---- Epoch  26  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.72 Test accuracy: 0.545\n",
            "\n",
            "  ---- Epoch  27  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.71 Test accuracy: 0.545\n",
            "\n",
            "  ---- Epoch  28  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.73 Test accuracy: 0.54\n",
            "\n",
            "  ---- Epoch  29  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.75 Test accuracy: 0.535\n",
            "\n",
            "  ---- Epoch  30  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.75 Test accuracy: 0.54\n",
            "\n",
            "  ---- Epoch  31  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.75 Test accuracy: 0.535\n",
            "\n",
            "  ---- Epoch  32  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.76 Test accuracy: 0.545\n",
            "\n",
            "  ---- Epoch  33  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.77 Test accuracy: 0.55\n",
            "\n",
            "  ---- Epoch  34  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.78 Test accuracy: 0.56\n",
            "\n",
            "  ---- Epoch  35  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.78 Test accuracy: 0.565\n",
            "\n",
            "  ---- Epoch  36  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.8 Test accuracy: 0.575\n",
            "\n",
            "  ---- Epoch  37  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.8 Test accuracy: 0.59\n",
            "\n",
            "  ---- Epoch  38  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.81 Test accuracy: 0.585\n",
            "\n",
            "  ---- Epoch  39  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.81 Test accuracy: 0.585\n",
            "\n",
            "  ---- Epoch  40  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.81 Test accuracy: 0.575\n",
            "\n",
            "  ---- Epoch  41  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.81 Test accuracy: 0.575\n",
            "\n",
            "  ---- Epoch  42  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.81 Test accuracy: 0.58\n",
            "\n",
            "  ---- Epoch  43  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.81 Test accuracy: 0.585\n",
            "\n",
            "  ---- Epoch  44  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.81 Test accuracy: 0.58\n",
            "\n",
            "  ---- Epoch  45  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.82 Test accuracy: 0.575\n",
            "\n",
            "  ---- Epoch  46  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.82 Test accuracy: 0.575\n",
            "\n",
            "  ---- Epoch  47  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.82 Test accuracy: 0.57\n",
            "\n",
            "  ---- Epoch  48  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.82 Test accuracy: 0.57\n",
            "\n",
            "  ---- Epoch  49  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.82 Test accuracy: 0.565\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "03DjvJcnNF7v",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Third Experiement"
      ]
    },
    {
      "metadata": {
        "id": "Xfm00BrRNJih",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "3efaef87-30df-4145-a86a-dd0cf8638982"
      },
      "cell_type": "code",
      "source": [
        "CHAKIN_INDEX = 13\n",
        "NUMBER_OF_DIMENSIONS = 200\n",
        "SUBFOLDER_NAME = \"glove.6B\"\n",
        "\n",
        "DATA_FOLDER = \"embeddings\"\n",
        "ZIP_FILE = os.path.join(DATA_FOLDER, \"{}.zip\".format(SUBFOLDER_NAME))\n",
        "ZIP_FILE_ALT = \"glove\" + ZIP_FILE[5:]  # sometimes it's lowercase only...\n",
        "UNZIP_FOLDER = os.path.join(DATA_FOLDER, SUBFOLDER_NAME)\n",
        "if SUBFOLDER_NAME[-1] == \"d\":\n",
        "    GLOVE_FILENAME = os.path.join(\n",
        "        UNZIP_FOLDER, \"{}.txt\".format(SUBFOLDER_NAME))\n",
        "else:\n",
        "    GLOVE_FILENAME = os.path.join(UNZIP_FOLDER, \"{}.{}d.txt\".format(\n",
        "        SUBFOLDER_NAME, NUMBER_OF_DIMENSIONS))\n",
        "\n",
        "\n",
        "if not os.path.exists(ZIP_FILE) and not os.path.exists(UNZIP_FOLDER):\n",
        "    # GloVe by Stanford is licensed Apache 2.0:\n",
        "    #     https://github.com/stanfordnlp/GloVe/blob/master/LICENSE\n",
        "    #     http://nlp.stanford.edu/data/glove.twitter.27B.zip\n",
        "    #     Copyright 2014 The Board of Trustees of The Leland Stanford Junior University\n",
        "    print(\"Downloading embeddings to '{}'\".format(ZIP_FILE))\n",
        "    chakin.download(number=CHAKIN_INDEX, save_dir='./{}'.format(DATA_FOLDER))\n",
        "else:\n",
        "    print(\"Embeddings already downloaded.\")\n",
        "\n",
        "if not os.path.exists(UNZIP_FOLDER):\n",
        "    import zipfile\n",
        "    if not os.path.exists(ZIP_FILE) and os.path.exists(ZIP_FILE_ALT):\n",
        "        ZIP_FILE = ZIP_FILE_ALT\n",
        "    with zipfile.ZipFile(ZIP_FILE, \"r\") as zip_ref:\n",
        "        print(\"Extracting embeddings to '{}'\".format(UNZIP_FOLDER))\n",
        "        zip_ref.extractall(UNZIP_FOLDER)\n",
        "else:\n",
        "    print(\"Embeddings already extracted.\")\n",
        "\n",
        "print('\\nRun complete')"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Embeddings already downloaded.\n",
            "Embeddings already extracted.\n",
            "\n",
            "Run complete\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "j245-TRhNE-L",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fCBtk_pwNKBT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# To make output stable across runs\n",
        "def reset_graph(seed= RANDOM_SEED):\n",
        "    tf.reset_default_graph()\n",
        "    tf.set_random_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "REMOVE_STOPWORDS = False  # no stopword removal \n",
        "\n",
        "EVOCABSIZE = 10000  # specify desired size of pre-defined embedding vocabulary "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fQ14ysOoNKBY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------------- \n",
        "# Select the pre-defined embeddings source        \n",
        "# Define vocabulary size for the language model    \n",
        "# Create a word_to_embedding_dict for GloVe.6B.50d\n",
        "embeddings_directory = 'embeddings/glove.6B'\n",
        "filename = 'glove.6B.200d.txt'\n",
        "embeddings_filename = os.path.join(embeddings_directory, filename)\n",
        "# ------------------------------------------------------------- "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QWomtb1uNKBb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Utility function for loading embeddings follows methods described in\n",
        "# https://github.com/guillaume-chevalier/GloVe-as-a-TensorFlow-Embedding-Layer\n",
        "# Creates the Python defaultdict dictionary word_to_embedding_dict\n",
        "# for the requested pre-trained word embeddings\n",
        "# \n",
        "# Note the use of defaultdict data structure from the Python Standard Library\n",
        "# collections_defaultdict.py lets the caller specify a default value up front\n",
        "# The default value will be retuned if the key is not a known dictionary key\n",
        "# That is, unknown words are represented by a vector of zeros\n",
        "# For word embeddings, this default value is a vector of zeros\n",
        "# Documentation for the Python standard library:\n",
        "#   Hellmann, D. 2017. The Python 3 Standard Library by Example. Boston: \n",
        "#     Addison-Wesley. [ISBN-13: 978-0-13-429105-5]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vmcbCTYnNKBd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "3255e20e-e681-47f8-cd80-96d15adf5aae"
      },
      "cell_type": "code",
      "source": [
        "# Utility function for loading embeddings follows methods described in\n",
        "# https://github.com/guillaume-chevalier/GloVe-as-a-TensorFlow-Embedding-Layer\n",
        "# Creates the Python defaultdict dictionary word_to_embedding_dict\n",
        "# for the requested pre-trained word embeddings\n",
        "# \n",
        "# Note the use of defaultdict data structure from the Python Standard Library\n",
        "# collections_defaultdict.py lets the caller specify a default value up front\n",
        "# The default value will be retuned if the key is not a known dictionary key\n",
        "# That is, unknown words are represented by a vector of zeros\n",
        "# For word embeddings, this default value is a vector of zeros\n",
        "# Documentation for the Python standard library:\n",
        "#   Hellmann, D. 2017. The Python 3 Standard Library by Example. Boston: \n",
        "#     Addison-Wesley. [ISBN-13: 978-0-13-429105-5]\n",
        "def load_embedding_from_disks(embeddings_filename, with_indexes=True):\n",
        "    \"\"\"\n",
        "    Read a embeddings txt file. If `with_indexes=True`, \n",
        "    we return a tuple of two dictionnaries\n",
        "    `(word_to_index_dict, index_to_embedding_array)`, \n",
        "    otherwise we return only a direct \n",
        "    `word_to_embedding_dict` dictionnary mapping \n",
        "    from a string to a numpy array.\n",
        "    \"\"\"\n",
        "    if with_indexes:\n",
        "        word_to_index_dict = dict()\n",
        "        index_to_embedding_array = []\n",
        "  \n",
        "    else:\n",
        "        word_to_embedding_dict = dict()\n",
        "\n",
        "    with open(embeddings_filename, 'r') as embeddings_file:\n",
        "        for (i, line) in enumerate(embeddings_file):\n",
        "\n",
        "            split = line.split(' ')\n",
        "\n",
        "            word = split[0]\n",
        "\n",
        "            representation = split[1:]\n",
        "            representation = np.array(\n",
        "                [float(val) for val in representation]\n",
        "            )\n",
        "\n",
        "            if with_indexes:\n",
        "                word_to_index_dict[word] = i\n",
        "                index_to_embedding_array.append(representation)\n",
        "            else:\n",
        "                word_to_embedding_dict[word] = representation\n",
        "\n",
        "    # Empty representation for unknown words.\n",
        "    _WORD_NOT_FOUND = [0.0] * len(representation)\n",
        "    if with_indexes:\n",
        "        _LAST_INDEX = i + 1\n",
        "        word_to_index_dict = defaultdict(\n",
        "            lambda: _LAST_INDEX, word_to_index_dict)\n",
        "        index_to_embedding_array = np.array(\n",
        "            index_to_embedding_array + [_WORD_NOT_FOUND])\n",
        "        return word_to_index_dict, index_to_embedding_array\n",
        "    else:\n",
        "        word_to_embedding_dict = defaultdict(lambda: _WORD_NOT_FOUND)\n",
        "        return word_to_embedding_dict\n",
        "\n",
        "print('\\nLoading embeddings from', embeddings_filename)\n",
        "word_to_index, index_to_embedding = \\\n",
        "    load_embedding_from_disks(embeddings_filename, with_indexes=True)\n",
        "print(\"Embedding loaded from disks.\")"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Loading embeddings from embeddings/glove.6B/glove.6B.200d.txt\n",
            "Embedding loaded from disks.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ocvrG8UTNKBj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "outputId": "df40bc12-8390-4fba-e9e8-dc3dd74dff22"
      },
      "cell_type": "code",
      "source": [
        "# Note: unknown words have representations with values [0, 0, ..., 0]\n",
        "\n",
        "# Additional background code from\n",
        "# https://github.com/guillaume-chevalier/GloVe-as-a-TensorFlow-Embedding-Layer\n",
        "# shows the general structure of the data structures for word embeddings\n",
        "# This code is modified for our purposes in language modeling \n",
        "vocab_size, embedding_dim = index_to_embedding.shape\n",
        "print(\"Embedding is of shape: {}\".format(index_to_embedding.shape))\n",
        "print(\"This means (number of words, number of dimensions per word)\\n\")\n",
        "print(\"The first words are words that tend occur more often.\")\n",
        "\n",
        "print(\"Note: for unknown words, the representation is an empty vector,\\n\"\n",
        "      \"and the index is the last one. The dictionnary has a limit:\")\n",
        "print(\"    {} --> {} --> {}\".format(\"A word\", \"Index in embedding\", \n",
        "      \"Representation\"))\n",
        "word = \"worsdfkljsdf\"  # a word obviously not in the vocabulary\n",
        "idx = word_to_index[word] # index for word obviously not in the vocabulary\n",
        "complete_vocabulary_size = idx \n",
        "embd = list(np.array(index_to_embedding[idx], dtype=int)) # \"int\" compact print\n",
        "print(\"    {} --> {} --> {}\".format(word, idx, embd))\n",
        "word = \"the\"\n",
        "idx = word_to_index[word]\n",
        "embd = list(index_to_embedding[idx])  # \"int\" for compact print only.\n",
        "print(\"    {} --> {} --> {}\".format(word, idx, embd))"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Embedding is of shape: (400001, 200)\n",
            "This means (number of words, number of dimensions per word)\n",
            "\n",
            "The first words are words that tend occur more often.\n",
            "Note: for unknown words, the representation is an empty vector,\n",
            "and the index is the last one. The dictionnary has a limit:\n",
            "    A word --> Index in embedding --> Representation\n",
            "    worsdfkljsdf --> 400000 --> [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "    the --> 0 --> [-0.071549, 0.093459, 0.023738, -0.090339, 0.056123, 0.32547, -0.39796, -0.092139, 0.061181, -0.1895, 0.13061, 0.14349, 0.011479, 0.38158, 0.5403, -0.14088, 0.24315, 0.23036, -0.55339, 0.048154, 0.45662, 3.2338, 0.020199, 0.049019, -0.014132, 0.076017, -0.11527, 0.2006, -0.077657, 0.24328, 0.16368, -0.34118, -0.06607, 0.10152, 0.038232, -0.17668, -0.88153, -0.33895, -0.035481, -0.55095, -0.016899, -0.43982, 0.039004, 0.40447, -0.2588, 0.64594, 0.26641, 0.28009, -0.024625, 0.63302, -0.317, 0.10271, 0.30886, 0.097792, -0.38227, 0.086552, 0.047075, 0.23511, -0.32127, -0.28538, 0.1667, -0.0049707, -0.62714, -0.24904, 0.29713, 0.14379, -0.12325, -0.058178, -0.001029, -0.082126, 0.36935, -0.00058442, 0.34286, 0.28426, -0.068599, 0.65747, -0.029087, 0.16184, 0.073672, -0.30343, 0.095733, -0.5286, -0.22898, 0.064079, 0.015218, 0.34921, -0.4396, -0.43983, 0.77515, -0.87767, -0.087504, 0.39598, 0.62362, -0.26211, -0.30539, -0.022964, 0.30567, 0.06766, 0.15383, -0.11211, -0.09154, 0.082562, 0.16897, -0.032952, -0.28775, -0.2232, -0.090426, 1.2407, -0.18244, -0.0075219, -0.041388, -0.011083, 0.078186, 0.38511, 0.23334, 0.14414, -0.0009107, -0.26388, -0.20481, 0.10099, 0.14076, 0.28834, -0.045429, 0.37247, 0.13645, -0.67457, 0.22786, 0.12599, 0.029091, 0.030428, -0.13028, 0.19408, 0.49014, -0.39121, -0.075952, 0.074731, 0.18902, -0.16922, -0.26019, -0.039771, -0.24153, 0.10875, 0.30434, 0.036009, 1.4264, 0.12759, -0.073811, -0.20418, 0.0080016, 0.15381, 0.20223, 0.28274, 0.096206, -0.33634, 0.50983, 0.32625, -0.26535, 0.374, -0.30388, -0.40033, -0.04291, -0.067897, -0.29332, 0.10978, -0.045365, 0.23222, -0.31134, -0.28983, -0.66687, 0.53097, 0.19461, 0.3667, 0.26185, -0.65187, 0.10266, 0.11363, -0.12953, -0.68246, -0.18751, 0.1476, 1.0765, -0.22908, -0.0093435, -0.20651, -0.35225, -0.2672, -0.0034307, 0.25906, 0.21759, 0.66158, 0.1218, 0.19957, -0.20303, 0.34474, -0.24328, 0.13139, -0.0088767, 0.33617, 0.030591, 0.25577]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nSBI4cC-NKBo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5304
        },
        "outputId": "c1297186-9cdb-4243-cafb-4d0c36f1ae42"
      },
      "cell_type": "code",
      "source": [
        "# Show how to use embeddings dictionaries with a test sentence\n",
        "# This is a famous typing exercise with all letters of the alphabet\n",
        "# https://en.wikipedia.org/wiki/The_quick_brown_fox_jumps_over_the_lazy_dog\n",
        "a_typing_test_sentence = 'The quick brown fox jumps over the lazy dog'\n",
        "print('\\nTest sentence: ', a_typing_test_sentence, '\\n')\n",
        "words_in_test_sentence = a_typing_test_sentence.split()\n",
        "\n",
        "print('Test sentence embeddings from complete vocabulary of', \n",
        "      complete_vocabulary_size, 'words:\\n')\n",
        "for word in words_in_test_sentence:\n",
        "    word_ = word.lower()\n",
        "    embedding = index_to_embedding[word_to_index[word_]]\n",
        "    print(word_ + \": \", embedding)"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test sentence:  The quick brown fox jumps over the lazy dog \n",
            "\n",
            "Test sentence embeddings from complete vocabulary of 400000 words:\n",
            "\n",
            "the:  [-7.1549e-02  9.3459e-02  2.3738e-02 -9.0339e-02  5.6123e-02  3.2547e-01\n",
            " -3.9796e-01 -9.2139e-02  6.1181e-02 -1.8950e-01  1.3061e-01  1.4349e-01\n",
            "  1.1479e-02  3.8158e-01  5.4030e-01 -1.4088e-01  2.4315e-01  2.3036e-01\n",
            " -5.5339e-01  4.8154e-02  4.5662e-01  3.2338e+00  2.0199e-02  4.9019e-02\n",
            " -1.4132e-02  7.6017e-02 -1.1527e-01  2.0060e-01 -7.7657e-02  2.4328e-01\n",
            "  1.6368e-01 -3.4118e-01 -6.6070e-02  1.0152e-01  3.8232e-02 -1.7668e-01\n",
            " -8.8153e-01 -3.3895e-01 -3.5481e-02 -5.5095e-01 -1.6899e-02 -4.3982e-01\n",
            "  3.9004e-02  4.0447e-01 -2.5880e-01  6.4594e-01  2.6641e-01  2.8009e-01\n",
            " -2.4625e-02  6.3302e-01 -3.1700e-01  1.0271e-01  3.0886e-01  9.7792e-02\n",
            " -3.8227e-01  8.6552e-02  4.7075e-02  2.3511e-01 -3.2127e-01 -2.8538e-01\n",
            "  1.6670e-01 -4.9707e-03 -6.2714e-01 -2.4904e-01  2.9713e-01  1.4379e-01\n",
            " -1.2325e-01 -5.8178e-02 -1.0290e-03 -8.2126e-02  3.6935e-01 -5.8442e-04\n",
            "  3.4286e-01  2.8426e-01 -6.8599e-02  6.5747e-01 -2.9087e-02  1.6184e-01\n",
            "  7.3672e-02 -3.0343e-01  9.5733e-02 -5.2860e-01 -2.2898e-01  6.4079e-02\n",
            "  1.5218e-02  3.4921e-01 -4.3960e-01 -4.3983e-01  7.7515e-01 -8.7767e-01\n",
            " -8.7504e-02  3.9598e-01  6.2362e-01 -2.6211e-01 -3.0539e-01 -2.2964e-02\n",
            "  3.0567e-01  6.7660e-02  1.5383e-01 -1.1211e-01 -9.1540e-02  8.2562e-02\n",
            "  1.6897e-01 -3.2952e-02 -2.8775e-01 -2.2320e-01 -9.0426e-02  1.2407e+00\n",
            " -1.8244e-01 -7.5219e-03 -4.1388e-02 -1.1083e-02  7.8186e-02  3.8511e-01\n",
            "  2.3334e-01  1.4414e-01 -9.1070e-04 -2.6388e-01 -2.0481e-01  1.0099e-01\n",
            "  1.4076e-01  2.8834e-01 -4.5429e-02  3.7247e-01  1.3645e-01 -6.7457e-01\n",
            "  2.2786e-01  1.2599e-01  2.9091e-02  3.0428e-02 -1.3028e-01  1.9408e-01\n",
            "  4.9014e-01 -3.9121e-01 -7.5952e-02  7.4731e-02  1.8902e-01 -1.6922e-01\n",
            " -2.6019e-01 -3.9771e-02 -2.4153e-01  1.0875e-01  3.0434e-01  3.6009e-02\n",
            "  1.4264e+00  1.2759e-01 -7.3811e-02 -2.0418e-01  8.0016e-03  1.5381e-01\n",
            "  2.0223e-01  2.8274e-01  9.6206e-02 -3.3634e-01  5.0983e-01  3.2625e-01\n",
            " -2.6535e-01  3.7400e-01 -3.0388e-01 -4.0033e-01 -4.2910e-02 -6.7897e-02\n",
            " -2.9332e-01  1.0978e-01 -4.5365e-02  2.3222e-01 -3.1134e-01 -2.8983e-01\n",
            " -6.6687e-01  5.3097e-01  1.9461e-01  3.6670e-01  2.6185e-01 -6.5187e-01\n",
            "  1.0266e-01  1.1363e-01 -1.2953e-01 -6.8246e-01 -1.8751e-01  1.4760e-01\n",
            "  1.0765e+00 -2.2908e-01 -9.3435e-03 -2.0651e-01 -3.5225e-01 -2.6720e-01\n",
            " -3.4307e-03  2.5906e-01  2.1759e-01  6.6158e-01  1.2180e-01  1.9957e-01\n",
            " -2.0303e-01  3.4474e-01 -2.4328e-01  1.3139e-01 -8.8767e-03  3.3617e-01\n",
            "  3.0591e-02  2.5577e-01]\n",
            "quick:  [ 0.53142    0.49769   -0.46218   -0.35345   -0.50683   -0.34447\n",
            " -0.81172    0.39306    0.27834    0.62198    0.06481    0.14899\n",
            "  0.43073    0.19718   -0.28265   -0.11391   -0.59028   -0.42922\n",
            " -0.13215   -0.25498   -0.25008    1.7146    -0.1231     0.38283\n",
            "  0.70766    0.16659   -0.32859    0.53923    0.33162   -0.70424\n",
            "  0.0098712  0.24168   -0.078985  -0.2846     0.33886   -0.15721\n",
            " -0.20075   -0.28108   -0.64413    0.26945    0.34069    0.26855\n",
            " -0.084457   0.36366    0.056946   0.025703   0.33665    0.18138\n",
            " -0.17127    0.12221   -0.15343   -0.45456   -0.35414    0.36805\n",
            "  0.35846    0.035429  -0.2158    -0.027401   0.38349   -0.062931\n",
            "  0.38079   -0.48306   -0.35183   -0.36452    0.90971    0.013474\n",
            "  0.17566    0.16205    0.55181    0.38169    0.069562   0.25644\n",
            " -0.10965   -0.41509   -0.147      0.41523   -0.61416    0.26874\n",
            " -0.15854    0.087907   0.50174    0.033713   0.033137  -0.011\n",
            "  0.29725   -0.16901    0.52102   -0.37652    0.33479   -0.73266\n",
            "  0.094927  -0.10657   -0.060679   0.30579    0.10662   -0.44642\n",
            " -0.24021    0.36714    0.26208    0.18919    0.40989   -0.060084\n",
            "  0.60582   -0.27333   -0.39621    0.83985    0.24053    1.1565\n",
            " -0.36838   -0.11829    0.0029027 -0.21962    0.023316  -0.43563\n",
            " -0.090644   0.086079   0.89851   -0.35074   -0.50602    0.51644\n",
            "  0.29897   -0.25667    0.17148    0.21049   -0.59112   -0.035632\n",
            " -0.024478   0.17495    0.57612    0.1432    -0.0088392  0.52941\n",
            "  0.042058  -0.088134   0.23524    0.017935  -0.42959   -0.18663\n",
            "  0.26853   -0.78495    0.12948    0.26742    0.043747   0.59647\n",
            "  1.4267     0.090168  -0.40832   -1.1234    -0.38028    0.27162\n",
            "  0.14629    0.014124   0.43367   -0.10298   -0.05676   -0.2036\n",
            " -0.43859   -0.07543    0.06907   -0.5567     0.012278   0.39384\n",
            " -0.086888   0.49387    0.44799   -0.75701   -0.36998    0.26186\n",
            " -0.20048    0.043936  -0.60191   -0.52284    0.065432   0.27434\n",
            "  0.70803    0.22079   -0.095421  -0.23274   -0.076995  -0.073985\n",
            "  0.65561    0.22114    0.28689    0.079496  -0.63842   -0.0055558\n",
            "  0.028695  -0.16838    0.32392   -0.039101  -0.38377    0.22408\n",
            "  0.057295  -0.48569    0.073566   0.48297   -0.31941    0.36239\n",
            " -0.32581    0.54421  ]\n",
            "brown:  [-3.1442e-01 -4.6750e-01 -8.4407e-01 -1.8710e-01 -1.2632e-01  1.2006e-01\n",
            "  5.6806e-02 -3.4877e-01 -6.7246e-02 -1.0056e-01  2.0937e-01 -2.3444e-01\n",
            " -3.0512e-01  1.2300e-01  6.8152e-01  3.3224e-01 -1.5463e-01 -6.3934e-02\n",
            "  2.9683e-01  2.1666e-01  1.8406e-01  1.6954e+00 -2.8552e-01  3.1491e-01\n",
            "  3.9817e-03 -3.4663e-01 -3.1703e-01 -3.8260e-01 -3.0899e-01 -7.5480e-02\n",
            "  1.9759e-01 -4.5206e-01  2.2378e-02  7.8655e-02  2.3765e-01 -1.2059e-01\n",
            "  3.2246e-01 -6.3736e-01 -3.3414e-01 -4.6758e-01 -5.2968e-01  8.0329e-02\n",
            " -4.2745e-02  4.5676e-01 -3.0410e-02 -4.2255e-02  1.1239e+00 -4.0934e-01\n",
            "  6.2239e-01  1.8442e-01  3.4795e-01  3.9478e-01 -4.1470e-01  3.3069e-01\n",
            "  5.3098e-01 -5.7980e-01 -4.0896e-01 -3.7668e-03  1.6660e-01 -3.7549e-01\n",
            "  5.9203e-01  4.2156e-01 -3.5300e-01 -2.3176e-01 -1.7937e-01 -6.2997e-01\n",
            "  2.4777e-01  1.1116e+00  3.1387e-01  5.1727e-01 -4.7342e-02  7.8297e-02\n",
            " -2.4665e-02  4.7672e-01 -3.4149e-02 -5.8473e-01 -3.0589e-01 -3.0829e-01\n",
            "  1.7088e-02  4.3659e-02  2.5648e-01 -1.7711e-01 -1.8196e-01  6.3430e-02\n",
            "  7.5974e-01  1.9292e-01 -1.3693e-01  4.4574e-01  1.2233e-01 -8.3919e-01\n",
            " -4.2436e-01 -1.0580e-02 -1.5049e-01  7.5005e-01 -8.6058e-02 -4.8509e-01\n",
            " -3.3137e-01 -1.1382e-01  1.0846e-02  3.3227e-01 -1.3328e-01  1.2578e-01\n",
            "  4.8929e-01 -6.9796e-02 -7.2486e-01  2.5884e-01 -2.1520e-01  2.0662e+00\n",
            " -4.7097e-01 -2.5758e-01 -5.4581e-01 -2.4952e-01  8.5905e-02 -1.8132e-01\n",
            "  2.8409e-01 -5.1668e-01 -1.1887e-01  9.5748e-04 -1.9196e-01  3.8302e-03\n",
            "  2.3549e-02  7.0700e-01 -1.4062e-01 -1.6726e-01  4.8694e-01 -6.4805e-01\n",
            "  5.0976e-01  2.7775e-01  5.4095e-01  6.4624e-01 -2.7274e-01 -2.2282e-01\n",
            "  3.0647e-01 -3.3406e-01  4.7047e-01 -5.2202e-01  2.1262e-01 -3.8874e-01\n",
            " -2.4386e-01  3.3994e-01  6.1815e-02 -1.1480e-01  8.0026e-01 -6.2362e-02\n",
            "  1.1721e+00  4.0950e-01  5.1069e-01 -4.6940e-01 -4.5256e-01  4.0319e-01\n",
            "  1.4089e-01  1.5493e-01 -1.0399e-01  2.9251e-01  4.3181e-02 -1.9882e-02\n",
            "  2.2089e-01 -1.6188e-01  2.3468e-02 -3.8417e-01  5.1735e-01  2.1983e-01\n",
            " -2.4505e-01  9.4134e-01 -2.1018e-01  8.2230e-02 -6.3052e-01  9.1849e-02\n",
            "  7.6589e-01  6.2383e-01  5.2695e-01  5.5622e-01 -7.1670e-02  3.5850e-01\n",
            " -5.9761e-01  1.4845e-01 -9.3695e-02 -4.8003e-01  1.6384e-01  2.0724e-01\n",
            "  1.7857e-02 -2.2912e-01 -2.1439e-01  2.9748e-01 -7.7659e-01 -8.4560e-01\n",
            " -3.3909e-01  2.8332e-01  1.1776e-01  1.6187e-01  5.3056e-01  1.0725e-01\n",
            "  2.5875e-01 -2.6135e-01 -2.6731e-01  5.6855e-01 -4.0107e-01  5.1542e-01\n",
            " -1.4011e-01 -2.0160e-01]\n",
            "fox:  [-7.9079e-01  2.5975e-01 -2.2002e-01 -8.2449e-01 -5.5317e-01 -2.2102e-01\n",
            " -1.9796e-01 -3.3495e-02 -5.1390e-01  2.4442e-01  1.4431e-01 -2.7593e-01\n",
            " -9.0063e-02 -6.7059e-01  1.1376e-01  2.1882e-01 -1.7679e-01  1.1152e+00\n",
            "  2.9556e-01  3.8743e-01  7.3205e-01  9.5070e-01 -4.2076e-01  2.6629e-01\n",
            " -1.0438e-01 -1.4570e-01  4.8923e-04 -1.5552e-01 -1.6013e-01 -2.8630e-02\n",
            " -2.8804e-01 -7.8735e-02  4.6452e-01 -7.3855e-01  8.8646e-02 -5.3238e-01\n",
            "  9.8893e-02 -8.1580e-01  1.2033e-01 -3.1299e-01 -3.7875e-01 -3.5234e-01\n",
            "  4.6905e-01  1.3722e-01 -2.5760e-01  4.7174e-02  4.6980e-01 -5.6979e-01\n",
            "  1.0979e-01  4.0741e-02  1.2481e-01 -3.0212e-01  2.6864e-01  4.1942e-01\n",
            "  2.5819e-02  1.6389e-01 -2.5527e-01 -5.5753e-01  4.0756e-01  7.5333e-01\n",
            "  3.7972e-01  2.5484e-02 -4.4053e-01  1.1309e-02  7.3067e-01 -3.0212e-01\n",
            "  1.0309e-01  1.5677e-01  1.1449e+00 -1.7550e-01  2.0491e-01 -4.5462e-01\n",
            " -1.5605e-01  2.3622e-01  3.0375e-01 -1.6485e-01  4.0938e-01  7.0845e-02\n",
            "  2.0811e-01  6.7304e-01  1.1978e-01 -3.1846e-02  3.0240e-01 -7.4044e-01\n",
            " -1.4869e-01  4.9821e-01 -1.3478e+00  5.0864e-02  1.5759e-01 -8.8998e-01\n",
            " -3.4145e-01  4.2453e-01  1.4006e-01  7.3076e-02 -5.8784e-01  4.6956e-01\n",
            " -3.5577e-01 -4.9402e-01  4.0573e-01 -3.6992e-01  5.1449e-01  2.9251e-01\n",
            " -4.9839e-03  3.4426e-02 -6.6246e-01 -3.6776e-01  5.1810e-03  1.2211e+00\n",
            " -4.3950e-01  2.3250e-01  1.4206e-01 -8.7137e-01  2.9829e-01 -4.9394e-01\n",
            " -3.5215e-01 -8.2592e-01 -3.5974e-01 -8.5425e-02 -4.3717e-01 -4.0122e-03\n",
            "  6.2199e-03 -5.6365e-01 -2.0579e-01  1.6423e-01  5.3871e-01 -7.3100e-01\n",
            " -5.0571e-01  1.8094e-01  3.6548e-01  4.0391e-01  9.8332e-02 -8.5674e-01\n",
            " -3.4216e-01  2.3133e-01  2.0996e-01  1.0736e-01  6.4859e-02  3.8116e-01\n",
            " -1.9745e-01 -3.4992e-01 -1.4024e-01 -1.1248e-01  4.0959e-01  3.9222e-02\n",
            "  9.7528e-01  2.8322e-01  6.9825e-02 -7.8264e-01 -3.5349e-01  1.0494e-01\n",
            " -1.3360e-01  6.3501e-01 -2.6349e-01  7.4375e-02  6.5436e-01 -5.5198e-01\n",
            " -2.3475e-02 -1.7402e-01 -3.6973e-01 -3.9919e-01  5.6042e-01 -2.8520e-01\n",
            " -5.5555e-01  9.7751e-02 -2.6530e-03 -4.9564e-01  4.1002e-01 -4.5144e-01\n",
            "  6.1865e-01  6.5570e-01  3.2134e-01 -2.5954e-01 -5.8615e-01  1.8095e-01\n",
            "  5.5883e-01 -2.8088e-01  2.1256e-01 -3.4411e-01 -5.3353e-01  4.3147e-01\n",
            "  8.9076e-01  2.0905e-01 -1.7124e-02  3.5795e-01 -6.2475e-01 -1.6113e-01\n",
            "  1.3008e-01 -3.1728e-01  6.8400e-02  1.1515e-02  4.5719e-01 -4.8818e-01\n",
            " -1.7250e-01 -3.6966e-01 -4.2067e-01  2.6162e-01 -3.1324e-01  5.1430e-01\n",
            "  5.3977e-01  5.3872e-01]\n",
            "jumps:  [ 0.34995    0.095074  -0.64607   -0.25639    0.17252   -0.47998\n",
            " -0.25145   -0.35619   -0.40904   -0.26207   -0.075424  -0.3558\n",
            "  0.36008    0.032778  -0.20451    0.2594    -0.28017   -0.19266\n",
            "  0.7978    -0.061735   0.43611    0.75488    0.71363   -0.16999\n",
            " -0.018755   0.33876   -0.18916    0.054519  -0.70085   -0.50739\n",
            " -0.17996   -0.21199    0.16489   -0.90909    0.72333    0.096124\n",
            " -0.75995   -0.43706   -0.92707    0.15698    0.25441    0.25025\n",
            " -0.28406   -0.37363    0.67765    0.3535    -0.29235   -0.56071\n",
            " -0.58276    0.10647   -0.42709   -0.14311    0.31088    0.26132\n",
            "  0.53098    0.45227   -0.72973   -0.30574   -0.10712    0.20832\n",
            " -0.63229    0.12498    0.42135   -0.094986   0.18732    0.17619\n",
            "  0.10098   -0.077144   0.22384   -0.23426   -0.016561   0.11803\n",
            " -0.3579     0.083167   0.39282    0.20462   -0.10714   -0.2095\n",
            "  0.047883   0.87777    0.46483    0.12565   -0.59199   -0.1897\n",
            " -0.76223   -0.92849    0.14238    0.10795    0.13299   -1.0459\n",
            " -0.40652   -0.0069797 -0.62038    0.68819    0.32092   -0.034481\n",
            "  0.41429    0.080192   0.36452   -0.51149   -0.17595    0.32034\n",
            " -0.33943    0.48514    0.30028   -0.29691    0.19679    0.042799\n",
            "  0.062819   0.32962    0.0024064 -0.55518    0.52484   -0.37855\n",
            " -0.16013    0.18067    0.75056   -0.37793    0.4326     0.25855\n",
            " -0.14586   -0.25838   -0.26509   -0.090255  -0.84919    0.038647\n",
            " -0.56398    0.26634    0.37917    0.15311   -0.23534   -0.1771\n",
            "  0.043473  -0.87943   -0.017739  -0.035083   0.18803   -0.063969\n",
            "  0.060301  -0.54672    0.25768    0.403     -0.18811   -0.34368\n",
            "  0.91118    0.12708    0.10243   -0.2001    -0.050847   0.45843\n",
            " -0.022441  -0.13771    0.31546    0.31103   -0.18422   -0.11485\n",
            "  0.065411   0.046909   0.079907  -0.24623    0.42528   -0.01039\n",
            "  0.38679    0.31717   -0.042185  -0.31751   -0.13434   -0.12508\n",
            " -0.96391    0.033417  -0.48754   -0.3622    -0.45879    0.5039\n",
            "  0.22754   -0.060078  -0.34449   -0.57871    0.036803  -0.15362\n",
            "  0.70166    0.078452   0.15758    1.3976    -0.33147   -0.20967\n",
            "  0.52176    0.3201     0.42299   -0.27026    0.40665   -0.63597\n",
            " -0.65809    0.22565   -0.087924  -0.18587   -0.083251   0.33991\n",
            " -0.28863   -0.30402  ]\n",
            "over:  [-1.8136e-01  5.4384e-02 -1.6638e-02 -1.0872e-01  8.8530e-03  6.2523e-01\n",
            " -5.8821e-02  3.3254e-02  2.9468e-02  2.5949e-01  1.6012e-01  4.5017e-01\n",
            " -2.6535e-02  2.5883e-01  6.2274e-01  3.9724e-01 -4.2582e-01  7.8368e-02\n",
            " -1.7108e-01  9.3726e-02  5.5443e-01  2.9364e+00 -2.9856e-01 -2.6919e-01\n",
            "  2.6883e-01 -4.5052e-02 -2.6976e-01  1.2477e-01  3.2422e-02  2.1593e-01\n",
            "  1.2230e-01 -7.0885e-01 -1.3760e-01  3.8023e-01 -2.8467e-01 -4.2573e-01\n",
            " -1.2020e+00 -4.2784e-01 -1.8816e-01 -1.9955e-01  2.3510e-01 -3.0719e-01\n",
            "  1.1255e-01 -1.1238e-01 -3.0902e-01  2.9093e-01  5.8251e-01 -6.7236e-02\n",
            "  1.1878e-01  4.7467e-01 -9.4490e-03  2.6370e-01  6.7172e-02  8.6137e-01\n",
            "  7.9867e-01 -1.1649e-01 -3.4968e-01  3.5306e-01 -1.6667e-01  9.5608e-02\n",
            " -5.3847e-02  2.6636e-01 -8.5124e-02 -3.4143e-01  5.2658e-01 -2.5312e-01\n",
            "  3.6868e-01  1.4808e-01  5.0457e-01 -5.4672e-01 -5.7193e-03 -2.8087e-01\n",
            "  2.7664e-01 -1.1067e-01 -5.6938e-02  8.4386e-01 -1.5502e-02 -1.4716e-01\n",
            " -2.4319e-01  1.5463e-01 -1.4038e-01 -2.8210e-02  4.0895e-02 -6.9638e-02\n",
            "  7.3984e-03 -2.0785e-02 -1.9026e-01 -3.4531e-01  1.3974e-01 -1.7654e-01\n",
            "  3.6421e-01  3.5446e-02  8.9064e-03  3.2949e-01  7.8715e-02  1.4295e-01\n",
            "  3.4110e-01 -3.0039e-01  1.5895e-01 -4.4791e-01  2.8847e-01 -4.9616e-01\n",
            " -1.2712e-01  5.9769e-01 -1.7956e-01  1.2578e-01  2.2449e-01  1.3406e+00\n",
            "  3.1268e-01 -5.1212e-01 -2.8810e-01 -1.2124e-01 -3.6539e-01  2.3938e-01\n",
            "  6.0236e-01  5.5483e-01 -1.6237e-01 -9.7217e-03 -3.9532e-01 -8.0846e-02\n",
            " -2.0053e-01  4.3234e-01  3.7900e-01  2.1911e-01  5.0871e-01 -9.6578e-01\n",
            "  4.0803e-01 -8.7810e-02  4.1018e-01  3.3282e-01 -2.6322e-01 -3.1882e-01\n",
            " -4.6560e-01 -4.9194e-01  1.3927e-01  6.7400e-01  3.7538e-01 -4.7802e-01\n",
            " -9.5047e-02  4.3122e-01 -3.3629e-01 -2.8367e-01  2.4260e-01 -1.6096e-01\n",
            "  1.3671e+00  2.6446e-01  1.8355e-01 -2.1050e-01  2.1915e-03  3.2011e-01\n",
            " -2.2010e-01  2.1907e-01 -1.6236e-01 -1.5451e-01  1.8775e-01 -1.5687e-01\n",
            " -1.1443e-01  7.4750e-02 -8.2473e-03 -4.4988e-01 -1.2534e-01  1.0274e-03\n",
            " -2.6980e-01  3.8504e-01  1.2015e-01  1.8465e-01 -5.7222e-01  2.6703e-01\n",
            " -3.6049e-02  1.0055e+00 -1.2874e-01  3.1265e-02  1.3461e-01  1.4928e-01\n",
            " -2.8600e-01  2.3514e-01 -2.9374e-01 -2.9196e-01 -2.9380e-01 -3.0427e-01\n",
            "  1.2025e+00  5.6008e-02  8.0408e-02  2.3824e-01  4.0132e-02  5.8081e-02\n",
            " -1.9350e-01  3.4714e-01  1.3727e-02 -1.3661e-02 -2.9019e-01  1.1244e-01\n",
            " -2.6544e-01  9.0986e-02  9.8320e-02 -2.1316e-01  6.3920e-01  1.6703e-01\n",
            " -3.7282e-01  9.7439e-02]\n",
            "the:  [-7.1549e-02  9.3459e-02  2.3738e-02 -9.0339e-02  5.6123e-02  3.2547e-01\n",
            " -3.9796e-01 -9.2139e-02  6.1181e-02 -1.8950e-01  1.3061e-01  1.4349e-01\n",
            "  1.1479e-02  3.8158e-01  5.4030e-01 -1.4088e-01  2.4315e-01  2.3036e-01\n",
            " -5.5339e-01  4.8154e-02  4.5662e-01  3.2338e+00  2.0199e-02  4.9019e-02\n",
            " -1.4132e-02  7.6017e-02 -1.1527e-01  2.0060e-01 -7.7657e-02  2.4328e-01\n",
            "  1.6368e-01 -3.4118e-01 -6.6070e-02  1.0152e-01  3.8232e-02 -1.7668e-01\n",
            " -8.8153e-01 -3.3895e-01 -3.5481e-02 -5.5095e-01 -1.6899e-02 -4.3982e-01\n",
            "  3.9004e-02  4.0447e-01 -2.5880e-01  6.4594e-01  2.6641e-01  2.8009e-01\n",
            " -2.4625e-02  6.3302e-01 -3.1700e-01  1.0271e-01  3.0886e-01  9.7792e-02\n",
            " -3.8227e-01  8.6552e-02  4.7075e-02  2.3511e-01 -3.2127e-01 -2.8538e-01\n",
            "  1.6670e-01 -4.9707e-03 -6.2714e-01 -2.4904e-01  2.9713e-01  1.4379e-01\n",
            " -1.2325e-01 -5.8178e-02 -1.0290e-03 -8.2126e-02  3.6935e-01 -5.8442e-04\n",
            "  3.4286e-01  2.8426e-01 -6.8599e-02  6.5747e-01 -2.9087e-02  1.6184e-01\n",
            "  7.3672e-02 -3.0343e-01  9.5733e-02 -5.2860e-01 -2.2898e-01  6.4079e-02\n",
            "  1.5218e-02  3.4921e-01 -4.3960e-01 -4.3983e-01  7.7515e-01 -8.7767e-01\n",
            " -8.7504e-02  3.9598e-01  6.2362e-01 -2.6211e-01 -3.0539e-01 -2.2964e-02\n",
            "  3.0567e-01  6.7660e-02  1.5383e-01 -1.1211e-01 -9.1540e-02  8.2562e-02\n",
            "  1.6897e-01 -3.2952e-02 -2.8775e-01 -2.2320e-01 -9.0426e-02  1.2407e+00\n",
            " -1.8244e-01 -7.5219e-03 -4.1388e-02 -1.1083e-02  7.8186e-02  3.8511e-01\n",
            "  2.3334e-01  1.4414e-01 -9.1070e-04 -2.6388e-01 -2.0481e-01  1.0099e-01\n",
            "  1.4076e-01  2.8834e-01 -4.5429e-02  3.7247e-01  1.3645e-01 -6.7457e-01\n",
            "  2.2786e-01  1.2599e-01  2.9091e-02  3.0428e-02 -1.3028e-01  1.9408e-01\n",
            "  4.9014e-01 -3.9121e-01 -7.5952e-02  7.4731e-02  1.8902e-01 -1.6922e-01\n",
            " -2.6019e-01 -3.9771e-02 -2.4153e-01  1.0875e-01  3.0434e-01  3.6009e-02\n",
            "  1.4264e+00  1.2759e-01 -7.3811e-02 -2.0418e-01  8.0016e-03  1.5381e-01\n",
            "  2.0223e-01  2.8274e-01  9.6206e-02 -3.3634e-01  5.0983e-01  3.2625e-01\n",
            " -2.6535e-01  3.7400e-01 -3.0388e-01 -4.0033e-01 -4.2910e-02 -6.7897e-02\n",
            " -2.9332e-01  1.0978e-01 -4.5365e-02  2.3222e-01 -3.1134e-01 -2.8983e-01\n",
            " -6.6687e-01  5.3097e-01  1.9461e-01  3.6670e-01  2.6185e-01 -6.5187e-01\n",
            "  1.0266e-01  1.1363e-01 -1.2953e-01 -6.8246e-01 -1.8751e-01  1.4760e-01\n",
            "  1.0765e+00 -2.2908e-01 -9.3435e-03 -2.0651e-01 -3.5225e-01 -2.6720e-01\n",
            " -3.4307e-03  2.5906e-01  2.1759e-01  6.6158e-01  1.2180e-01  1.9957e-01\n",
            " -2.0303e-01  3.4474e-01 -2.4328e-01  1.3139e-01 -8.8767e-03  3.3617e-01\n",
            "  3.0591e-02  2.5577e-01]\n",
            "lazy:  [ 2.4360e-01 -3.4947e-01 -3.6866e-01 -1.2049e-01  1.5666e-01 -2.8681e-01\n",
            " -2.7496e-01 -1.7186e-01  4.1094e-02  2.7805e-01  4.7063e-01 -7.6601e-01\n",
            "  9.3984e-01  3.1374e-01 -1.1558e-01  3.7066e-01 -7.3339e-01 -1.4001e-01\n",
            "  3.8372e-01 -1.1413e-01  2.2680e-01  1.8525e-01 -4.8286e-01  6.8405e-01\n",
            "  4.2411e-01 -5.3271e-01  2.1549e-01 -5.1370e-01 -2.0095e-01  3.6552e-01\n",
            "  7.3175e-02  4.7971e-01 -7.4379e-01  2.5743e-02  5.1195e-01 -9.6689e-02\n",
            " -3.1828e-01 -5.0514e-01 -7.7848e-02 -5.9908e-02  4.1613e-01  5.6312e-01\n",
            "  6.2281e-01 -1.1399e-01  5.7956e-01  5.4217e-01 -1.5364e-02 -1.8366e-01\n",
            " -9.4557e-02  9.7040e-01 -6.9829e-01 -2.8877e-01 -1.0291e-01  1.6857e-01\n",
            "  3.9540e-01 -2.6136e-01 -2.0364e-01 -4.3896e-01  1.5321e-01  2.0340e-01\n",
            "  2.3708e-01 -4.4019e-01 -2.5953e-01 -2.9465e-01 -7.0392e-01  4.5277e-01\n",
            "  4.8609e-01  2.4371e-01  1.0621e+00 -1.1893e-02  1.6272e-01  4.7658e-02\n",
            " -6.0591e-01  1.7743e-01 -2.3131e-01 -5.2727e-01 -4.4549e-01  2.2854e-01\n",
            "  4.5121e-02  1.9055e-01  4.8879e-02 -1.6769e-01 -1.5795e-01 -7.7439e-01\n",
            "  4.8220e-01 -2.1924e-01  1.5959e-01 -2.3288e-01 -2.1956e-01 -5.6318e-01\n",
            " -3.0414e-01 -4.9283e-01 -1.0453e-01  5.2286e-01  1.6010e-01  1.5194e-01\n",
            "  1.4254e-01 -2.3295e-01  4.2537e-01  5.8918e-01 -3.2303e-03 -2.0670e-01\n",
            "  1.1983e-01 -1.1128e-01  2.1679e-01  2.0885e-02  2.3287e-02  5.7264e-04\n",
            " -3.7682e-01  3.8070e-01  1.7784e-01 -3.3404e-01  4.4448e-01 -2.1758e-01\n",
            " -5.8585e-02  2.9275e-01  6.3896e-01  4.5773e-02 -5.1163e-01  2.3018e-01\n",
            "  3.5605e-02 -3.1697e-01 -1.7654e-02 -7.9892e-01  1.0807e-01  6.0575e-01\n",
            "  6.1157e-01  2.0286e-01  5.6747e-01 -3.7504e-02  8.8656e-02 -2.2181e-01\n",
            " -8.2162e-02  2.4194e-01  9.2096e-02 -5.4754e-01 -4.4629e-01  6.1179e-01\n",
            "  2.5916e-01 -7.8731e-01  8.8334e-02 -5.4586e-01 -1.6790e-01 -2.9476e-01\n",
            "  1.2879e+00 -3.0823e-02 -2.1768e-01 -4.3510e-01  2.8531e-01  1.6418e-02\n",
            "  4.0111e-01  1.0992e+00 -2.2871e-01 -9.6040e-02  2.4002e-01 -2.2189e-03\n",
            "  2.2825e-01 -8.8586e-02 -6.9752e-01  4.5946e-02  3.6988e-01 -1.2353e-01\n",
            " -8.9403e-02  8.0708e-01  4.2137e-02 -6.1002e-01 -2.7454e-01  7.8604e-01\n",
            " -1.8031e-01 -1.2763e-01 -5.1040e-01  2.1468e-01 -1.2239e-01  4.3400e-01\n",
            "  4.1641e-01 -4.0750e-01 -4.5797e-01 -3.3990e-02  7.3162e-02  7.9555e-01\n",
            "  3.5424e-01  3.9136e-01 -7.2358e-01  4.7071e-01 -7.6863e-01  5.8818e-01\n",
            "  5.5482e-01 -1.5730e-01  1.0282e-01 -1.1142e-01 -8.3167e-02 -1.1425e+00\n",
            "  4.8521e-02 -3.0276e-03  3.6737e-01  1.9888e-01  4.6231e-02  4.9800e-01\n",
            " -9.2291e-02 -2.4074e-02]\n",
            "dog:  [-1.3791e-01 -4.7601e-01 -5.6369e-02 -3.9082e-01 -1.7544e-01 -6.2244e-01\n",
            " -3.9816e-01  2.9620e-01 -6.0647e-02 -6.7017e-02  1.1466e-01 -3.3015e-01\n",
            " -2.0318e-02  6.0616e-01 -1.3920e-01  1.3896e-01 -5.4781e-01  3.0864e-01\n",
            "  1.7354e-01  3.9927e-01  2.1137e-01  1.3004e+00  8.8030e-01  2.3946e-01\n",
            "  2.8838e-01 -4.6336e-01  2.5745e-01 -3.1755e-01 -3.2877e-01 -5.9534e-01\n",
            "  2.3983e-01  3.4159e-01  1.2754e-01 -8.8208e-01  1.4258e-01 -1.8857e-01\n",
            " -1.6961e-01  2.7808e-01 -2.4600e-01  1.9122e-01  5.0244e-01  5.3660e-01\n",
            " -5.3568e-01  2.4827e-01  3.2561e-01  6.7882e-01  9.6401e-01 -2.8892e-01\n",
            "  5.1206e-01  5.8496e-01 -3.1934e-02 -2.4849e-02  8.8564e-02  1.7360e-01\n",
            "  5.4166e-01 -8.6743e-02 -3.8412e-01  1.3974e-01 -7.4122e-03  9.2210e-01\n",
            " -2.5799e-01 -4.7018e-01 -5.5742e-01 -2.1213e-02 -7.1072e-01  8.0995e-02\n",
            " -4.7254e-01 -3.2925e-01  6.8052e-01  1.7242e-01  8.7783e-02 -2.6560e-01\n",
            " -6.0070e-01 -8.5217e-02 -3.6977e-02 -3.6593e-01 -6.2576e-01 -3.4162e-01\n",
            "  5.4672e-02 -1.1734e-01  1.9686e-01  8.3758e-02  4.3157e-01 -8.2195e-01\n",
            " -5.7756e-01  6.7821e-02 -4.9520e-01  1.4769e-01  3.2863e-01 -1.0649e+00\n",
            " -3.9756e-01 -3.4890e-01 -6.1548e-02  7.5400e-01  5.2457e-01  1.3657e-01\n",
            " -4.1904e-02 -4.4660e-01  8.4754e-02  3.7516e-01 -6.2374e-02 -8.1762e-02\n",
            " -4.1776e-01  3.0157e-02 -7.7967e-01  8.7627e-02  9.0542e-02  7.5266e-01\n",
            "  2.9235e-02 -1.8324e-01  5.5433e-01 -3.4632e-01 -7.8019e-02 -1.2078e-01\n",
            " -6.8377e-01  2.8826e-02 -4.1618e-01  2.2341e-01 -8.0811e-01 -5.9707e-01\n",
            "  4.6835e-01 -3.8246e-01 -2.3549e-01 -6.2565e-01  6.1201e-01  1.4221e-01\n",
            "  2.4076e-02  4.2106e-01  5.1978e-01  2.7811e-01  1.6885e-01 -4.9293e-01\n",
            "  3.1563e-01 -5.9663e-01  1.5091e-01 -6.3300e-01  1.1324e-03 -6.2031e-02\n",
            " -3.8694e-02 -2.6038e-01  2.1907e-01  2.3103e-01  8.2427e-01  1.4963e-01\n",
            "  8.5767e-01  1.5706e-01 -2.9116e-01 -4.2033e-01  4.5080e-01  3.9614e-01\n",
            " -2.0271e-01  1.0702e+00 -4.1153e-01  2.2282e-01  1.3287e-01  9.3896e-01\n",
            "  1.6088e-01 -2.2247e-01 -1.1443e+00 -5.0556e-01 -1.9619e-01  3.4685e-01\n",
            "  5.9883e-01  2.7666e-02 -2.1223e-01 -5.4970e-01 -1.6784e-01  5.2375e-01\n",
            " -9.8196e-02  1.5559e-01 -2.3997e-01 -1.1526e-01  1.6577e-02  6.1643e-02\n",
            "  9.2234e-02  2.5817e-02  2.9163e-01 -5.0848e-01 -1.5164e-01  1.8360e-01\n",
            "  1.6434e+00 -3.1567e-01 -5.3165e-01 -4.4914e-01 -7.2425e-01 -4.1122e-01\n",
            "  1.8799e-01  1.6130e-01  7.4169e-01 -2.1597e-02 -3.7775e-01 -5.5265e-01\n",
            "  7.3535e-02  8.5621e-02 -6.6452e-02  5.4982e-02 -4.0218e-01  4.3235e-01\n",
            " -7.5727e-02  7.0530e-02]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3twN0B_JNKBv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------------- \n",
        "# Define vocabulary size for the language model    \n",
        "# To reduce the size of the vocabulary to the n most frequently used words\n",
        "\n",
        "def default_factory():\n",
        "    return EVOCABSIZE  # last/unknown-word row in limited_index_to_embedding\n",
        "# dictionary has the items() function, returns list of (key, value) tuples\n",
        "limited_word_to_index = defaultdict(default_factory, \\\n",
        "    {k: v for k, v in word_to_index.items() if v < EVOCABSIZE})\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2Xa4ypLWNKBy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Select the first EVOCABSIZE rows to the index_to_embedding\n",
        "limited_index_to_embedding = index_to_embedding[0:EVOCABSIZE,:]\n",
        "# Set the unknown-word row to be all zeros as previously\n",
        "limited_index_to_embedding = np.append(limited_index_to_embedding, \n",
        "    index_to_embedding[index_to_embedding.shape[0] - 1, :].\\\n",
        "        reshape(1,embedding_dim), \n",
        "    axis = 0)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YemXZpbPNKB1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Delete large numpy array to clear some CPU RAM\n",
        "del index_to_embedding"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5lBf9hm9NKB7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4420
        },
        "outputId": "fe8834bc-b31f-429b-db62-31f1e0ab6fc6"
      },
      "cell_type": "code",
      "source": [
        "# Verify the new vocabulary: should get same embeddings for test sentence\n",
        "# Note that a small EVOCABSIZE may yield some zero vectors for embeddings\n",
        "print('\\nTest sentence embeddings from vocabulary of', EVOCABSIZE, 'words:\\n')\n",
        "for word in words_in_test_sentence:\n",
        "    word_ = word.lower()\n",
        "    embedding = limited_index_to_embedding[limited_word_to_index[word_]]\n",
        "    print(word_ + \": \", embedding)\n",
        "\n"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test sentence embeddings from vocabulary of 10000 words:\n",
            "\n",
            "the:  [-7.1549e-02  9.3459e-02  2.3738e-02 -9.0339e-02  5.6123e-02  3.2547e-01\n",
            " -3.9796e-01 -9.2139e-02  6.1181e-02 -1.8950e-01  1.3061e-01  1.4349e-01\n",
            "  1.1479e-02  3.8158e-01  5.4030e-01 -1.4088e-01  2.4315e-01  2.3036e-01\n",
            " -5.5339e-01  4.8154e-02  4.5662e-01  3.2338e+00  2.0199e-02  4.9019e-02\n",
            " -1.4132e-02  7.6017e-02 -1.1527e-01  2.0060e-01 -7.7657e-02  2.4328e-01\n",
            "  1.6368e-01 -3.4118e-01 -6.6070e-02  1.0152e-01  3.8232e-02 -1.7668e-01\n",
            " -8.8153e-01 -3.3895e-01 -3.5481e-02 -5.5095e-01 -1.6899e-02 -4.3982e-01\n",
            "  3.9004e-02  4.0447e-01 -2.5880e-01  6.4594e-01  2.6641e-01  2.8009e-01\n",
            " -2.4625e-02  6.3302e-01 -3.1700e-01  1.0271e-01  3.0886e-01  9.7792e-02\n",
            " -3.8227e-01  8.6552e-02  4.7075e-02  2.3511e-01 -3.2127e-01 -2.8538e-01\n",
            "  1.6670e-01 -4.9707e-03 -6.2714e-01 -2.4904e-01  2.9713e-01  1.4379e-01\n",
            " -1.2325e-01 -5.8178e-02 -1.0290e-03 -8.2126e-02  3.6935e-01 -5.8442e-04\n",
            "  3.4286e-01  2.8426e-01 -6.8599e-02  6.5747e-01 -2.9087e-02  1.6184e-01\n",
            "  7.3672e-02 -3.0343e-01  9.5733e-02 -5.2860e-01 -2.2898e-01  6.4079e-02\n",
            "  1.5218e-02  3.4921e-01 -4.3960e-01 -4.3983e-01  7.7515e-01 -8.7767e-01\n",
            " -8.7504e-02  3.9598e-01  6.2362e-01 -2.6211e-01 -3.0539e-01 -2.2964e-02\n",
            "  3.0567e-01  6.7660e-02  1.5383e-01 -1.1211e-01 -9.1540e-02  8.2562e-02\n",
            "  1.6897e-01 -3.2952e-02 -2.8775e-01 -2.2320e-01 -9.0426e-02  1.2407e+00\n",
            " -1.8244e-01 -7.5219e-03 -4.1388e-02 -1.1083e-02  7.8186e-02  3.8511e-01\n",
            "  2.3334e-01  1.4414e-01 -9.1070e-04 -2.6388e-01 -2.0481e-01  1.0099e-01\n",
            "  1.4076e-01  2.8834e-01 -4.5429e-02  3.7247e-01  1.3645e-01 -6.7457e-01\n",
            "  2.2786e-01  1.2599e-01  2.9091e-02  3.0428e-02 -1.3028e-01  1.9408e-01\n",
            "  4.9014e-01 -3.9121e-01 -7.5952e-02  7.4731e-02  1.8902e-01 -1.6922e-01\n",
            " -2.6019e-01 -3.9771e-02 -2.4153e-01  1.0875e-01  3.0434e-01  3.6009e-02\n",
            "  1.4264e+00  1.2759e-01 -7.3811e-02 -2.0418e-01  8.0016e-03  1.5381e-01\n",
            "  2.0223e-01  2.8274e-01  9.6206e-02 -3.3634e-01  5.0983e-01  3.2625e-01\n",
            " -2.6535e-01  3.7400e-01 -3.0388e-01 -4.0033e-01 -4.2910e-02 -6.7897e-02\n",
            " -2.9332e-01  1.0978e-01 -4.5365e-02  2.3222e-01 -3.1134e-01 -2.8983e-01\n",
            " -6.6687e-01  5.3097e-01  1.9461e-01  3.6670e-01  2.6185e-01 -6.5187e-01\n",
            "  1.0266e-01  1.1363e-01 -1.2953e-01 -6.8246e-01 -1.8751e-01  1.4760e-01\n",
            "  1.0765e+00 -2.2908e-01 -9.3435e-03 -2.0651e-01 -3.5225e-01 -2.6720e-01\n",
            " -3.4307e-03  2.5906e-01  2.1759e-01  6.6158e-01  1.2180e-01  1.9957e-01\n",
            " -2.0303e-01  3.4474e-01 -2.4328e-01  1.3139e-01 -8.8767e-03  3.3617e-01\n",
            "  3.0591e-02  2.5577e-01]\n",
            "quick:  [ 0.53142    0.49769   -0.46218   -0.35345   -0.50683   -0.34447\n",
            " -0.81172    0.39306    0.27834    0.62198    0.06481    0.14899\n",
            "  0.43073    0.19718   -0.28265   -0.11391   -0.59028   -0.42922\n",
            " -0.13215   -0.25498   -0.25008    1.7146    -0.1231     0.38283\n",
            "  0.70766    0.16659   -0.32859    0.53923    0.33162   -0.70424\n",
            "  0.0098712  0.24168   -0.078985  -0.2846     0.33886   -0.15721\n",
            " -0.20075   -0.28108   -0.64413    0.26945    0.34069    0.26855\n",
            " -0.084457   0.36366    0.056946   0.025703   0.33665    0.18138\n",
            " -0.17127    0.12221   -0.15343   -0.45456   -0.35414    0.36805\n",
            "  0.35846    0.035429  -0.2158    -0.027401   0.38349   -0.062931\n",
            "  0.38079   -0.48306   -0.35183   -0.36452    0.90971    0.013474\n",
            "  0.17566    0.16205    0.55181    0.38169    0.069562   0.25644\n",
            " -0.10965   -0.41509   -0.147      0.41523   -0.61416    0.26874\n",
            " -0.15854    0.087907   0.50174    0.033713   0.033137  -0.011\n",
            "  0.29725   -0.16901    0.52102   -0.37652    0.33479   -0.73266\n",
            "  0.094927  -0.10657   -0.060679   0.30579    0.10662   -0.44642\n",
            " -0.24021    0.36714    0.26208    0.18919    0.40989   -0.060084\n",
            "  0.60582   -0.27333   -0.39621    0.83985    0.24053    1.1565\n",
            " -0.36838   -0.11829    0.0029027 -0.21962    0.023316  -0.43563\n",
            " -0.090644   0.086079   0.89851   -0.35074   -0.50602    0.51644\n",
            "  0.29897   -0.25667    0.17148    0.21049   -0.59112   -0.035632\n",
            " -0.024478   0.17495    0.57612    0.1432    -0.0088392  0.52941\n",
            "  0.042058  -0.088134   0.23524    0.017935  -0.42959   -0.18663\n",
            "  0.26853   -0.78495    0.12948    0.26742    0.043747   0.59647\n",
            "  1.4267     0.090168  -0.40832   -1.1234    -0.38028    0.27162\n",
            "  0.14629    0.014124   0.43367   -0.10298   -0.05676   -0.2036\n",
            " -0.43859   -0.07543    0.06907   -0.5567     0.012278   0.39384\n",
            " -0.086888   0.49387    0.44799   -0.75701   -0.36998    0.26186\n",
            " -0.20048    0.043936  -0.60191   -0.52284    0.065432   0.27434\n",
            "  0.70803    0.22079   -0.095421  -0.23274   -0.076995  -0.073985\n",
            "  0.65561    0.22114    0.28689    0.079496  -0.63842   -0.0055558\n",
            "  0.028695  -0.16838    0.32392   -0.039101  -0.38377    0.22408\n",
            "  0.057295  -0.48569    0.073566   0.48297   -0.31941    0.36239\n",
            " -0.32581    0.54421  ]\n",
            "brown:  [-3.1442e-01 -4.6750e-01 -8.4407e-01 -1.8710e-01 -1.2632e-01  1.2006e-01\n",
            "  5.6806e-02 -3.4877e-01 -6.7246e-02 -1.0056e-01  2.0937e-01 -2.3444e-01\n",
            " -3.0512e-01  1.2300e-01  6.8152e-01  3.3224e-01 -1.5463e-01 -6.3934e-02\n",
            "  2.9683e-01  2.1666e-01  1.8406e-01  1.6954e+00 -2.8552e-01  3.1491e-01\n",
            "  3.9817e-03 -3.4663e-01 -3.1703e-01 -3.8260e-01 -3.0899e-01 -7.5480e-02\n",
            "  1.9759e-01 -4.5206e-01  2.2378e-02  7.8655e-02  2.3765e-01 -1.2059e-01\n",
            "  3.2246e-01 -6.3736e-01 -3.3414e-01 -4.6758e-01 -5.2968e-01  8.0329e-02\n",
            " -4.2745e-02  4.5676e-01 -3.0410e-02 -4.2255e-02  1.1239e+00 -4.0934e-01\n",
            "  6.2239e-01  1.8442e-01  3.4795e-01  3.9478e-01 -4.1470e-01  3.3069e-01\n",
            "  5.3098e-01 -5.7980e-01 -4.0896e-01 -3.7668e-03  1.6660e-01 -3.7549e-01\n",
            "  5.9203e-01  4.2156e-01 -3.5300e-01 -2.3176e-01 -1.7937e-01 -6.2997e-01\n",
            "  2.4777e-01  1.1116e+00  3.1387e-01  5.1727e-01 -4.7342e-02  7.8297e-02\n",
            " -2.4665e-02  4.7672e-01 -3.4149e-02 -5.8473e-01 -3.0589e-01 -3.0829e-01\n",
            "  1.7088e-02  4.3659e-02  2.5648e-01 -1.7711e-01 -1.8196e-01  6.3430e-02\n",
            "  7.5974e-01  1.9292e-01 -1.3693e-01  4.4574e-01  1.2233e-01 -8.3919e-01\n",
            " -4.2436e-01 -1.0580e-02 -1.5049e-01  7.5005e-01 -8.6058e-02 -4.8509e-01\n",
            " -3.3137e-01 -1.1382e-01  1.0846e-02  3.3227e-01 -1.3328e-01  1.2578e-01\n",
            "  4.8929e-01 -6.9796e-02 -7.2486e-01  2.5884e-01 -2.1520e-01  2.0662e+00\n",
            " -4.7097e-01 -2.5758e-01 -5.4581e-01 -2.4952e-01  8.5905e-02 -1.8132e-01\n",
            "  2.8409e-01 -5.1668e-01 -1.1887e-01  9.5748e-04 -1.9196e-01  3.8302e-03\n",
            "  2.3549e-02  7.0700e-01 -1.4062e-01 -1.6726e-01  4.8694e-01 -6.4805e-01\n",
            "  5.0976e-01  2.7775e-01  5.4095e-01  6.4624e-01 -2.7274e-01 -2.2282e-01\n",
            "  3.0647e-01 -3.3406e-01  4.7047e-01 -5.2202e-01  2.1262e-01 -3.8874e-01\n",
            " -2.4386e-01  3.3994e-01  6.1815e-02 -1.1480e-01  8.0026e-01 -6.2362e-02\n",
            "  1.1721e+00  4.0950e-01  5.1069e-01 -4.6940e-01 -4.5256e-01  4.0319e-01\n",
            "  1.4089e-01  1.5493e-01 -1.0399e-01  2.9251e-01  4.3181e-02 -1.9882e-02\n",
            "  2.2089e-01 -1.6188e-01  2.3468e-02 -3.8417e-01  5.1735e-01  2.1983e-01\n",
            " -2.4505e-01  9.4134e-01 -2.1018e-01  8.2230e-02 -6.3052e-01  9.1849e-02\n",
            "  7.6589e-01  6.2383e-01  5.2695e-01  5.5622e-01 -7.1670e-02  3.5850e-01\n",
            " -5.9761e-01  1.4845e-01 -9.3695e-02 -4.8003e-01  1.6384e-01  2.0724e-01\n",
            "  1.7857e-02 -2.2912e-01 -2.1439e-01  2.9748e-01 -7.7659e-01 -8.4560e-01\n",
            " -3.3909e-01  2.8332e-01  1.1776e-01  1.6187e-01  5.3056e-01  1.0725e-01\n",
            "  2.5875e-01 -2.6135e-01 -2.6731e-01  5.6855e-01 -4.0107e-01  5.1542e-01\n",
            " -1.4011e-01 -2.0160e-01]\n",
            "fox:  [-7.9079e-01  2.5975e-01 -2.2002e-01 -8.2449e-01 -5.5317e-01 -2.2102e-01\n",
            " -1.9796e-01 -3.3495e-02 -5.1390e-01  2.4442e-01  1.4431e-01 -2.7593e-01\n",
            " -9.0063e-02 -6.7059e-01  1.1376e-01  2.1882e-01 -1.7679e-01  1.1152e+00\n",
            "  2.9556e-01  3.8743e-01  7.3205e-01  9.5070e-01 -4.2076e-01  2.6629e-01\n",
            " -1.0438e-01 -1.4570e-01  4.8923e-04 -1.5552e-01 -1.6013e-01 -2.8630e-02\n",
            " -2.8804e-01 -7.8735e-02  4.6452e-01 -7.3855e-01  8.8646e-02 -5.3238e-01\n",
            "  9.8893e-02 -8.1580e-01  1.2033e-01 -3.1299e-01 -3.7875e-01 -3.5234e-01\n",
            "  4.6905e-01  1.3722e-01 -2.5760e-01  4.7174e-02  4.6980e-01 -5.6979e-01\n",
            "  1.0979e-01  4.0741e-02  1.2481e-01 -3.0212e-01  2.6864e-01  4.1942e-01\n",
            "  2.5819e-02  1.6389e-01 -2.5527e-01 -5.5753e-01  4.0756e-01  7.5333e-01\n",
            "  3.7972e-01  2.5484e-02 -4.4053e-01  1.1309e-02  7.3067e-01 -3.0212e-01\n",
            "  1.0309e-01  1.5677e-01  1.1449e+00 -1.7550e-01  2.0491e-01 -4.5462e-01\n",
            " -1.5605e-01  2.3622e-01  3.0375e-01 -1.6485e-01  4.0938e-01  7.0845e-02\n",
            "  2.0811e-01  6.7304e-01  1.1978e-01 -3.1846e-02  3.0240e-01 -7.4044e-01\n",
            " -1.4869e-01  4.9821e-01 -1.3478e+00  5.0864e-02  1.5759e-01 -8.8998e-01\n",
            " -3.4145e-01  4.2453e-01  1.4006e-01  7.3076e-02 -5.8784e-01  4.6956e-01\n",
            " -3.5577e-01 -4.9402e-01  4.0573e-01 -3.6992e-01  5.1449e-01  2.9251e-01\n",
            " -4.9839e-03  3.4426e-02 -6.6246e-01 -3.6776e-01  5.1810e-03  1.2211e+00\n",
            " -4.3950e-01  2.3250e-01  1.4206e-01 -8.7137e-01  2.9829e-01 -4.9394e-01\n",
            " -3.5215e-01 -8.2592e-01 -3.5974e-01 -8.5425e-02 -4.3717e-01 -4.0122e-03\n",
            "  6.2199e-03 -5.6365e-01 -2.0579e-01  1.6423e-01  5.3871e-01 -7.3100e-01\n",
            " -5.0571e-01  1.8094e-01  3.6548e-01  4.0391e-01  9.8332e-02 -8.5674e-01\n",
            " -3.4216e-01  2.3133e-01  2.0996e-01  1.0736e-01  6.4859e-02  3.8116e-01\n",
            " -1.9745e-01 -3.4992e-01 -1.4024e-01 -1.1248e-01  4.0959e-01  3.9222e-02\n",
            "  9.7528e-01  2.8322e-01  6.9825e-02 -7.8264e-01 -3.5349e-01  1.0494e-01\n",
            " -1.3360e-01  6.3501e-01 -2.6349e-01  7.4375e-02  6.5436e-01 -5.5198e-01\n",
            " -2.3475e-02 -1.7402e-01 -3.6973e-01 -3.9919e-01  5.6042e-01 -2.8520e-01\n",
            " -5.5555e-01  9.7751e-02 -2.6530e-03 -4.9564e-01  4.1002e-01 -4.5144e-01\n",
            "  6.1865e-01  6.5570e-01  3.2134e-01 -2.5954e-01 -5.8615e-01  1.8095e-01\n",
            "  5.5883e-01 -2.8088e-01  2.1256e-01 -3.4411e-01 -5.3353e-01  4.3147e-01\n",
            "  8.9076e-01  2.0905e-01 -1.7124e-02  3.5795e-01 -6.2475e-01 -1.6113e-01\n",
            "  1.3008e-01 -3.1728e-01  6.8400e-02  1.1515e-02  4.5719e-01 -4.8818e-01\n",
            " -1.7250e-01 -3.6966e-01 -4.2067e-01  2.6162e-01 -3.1324e-01  5.1430e-01\n",
            "  5.3977e-01  5.3872e-01]\n",
            "jumps:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "over:  [-1.8136e-01  5.4384e-02 -1.6638e-02 -1.0872e-01  8.8530e-03  6.2523e-01\n",
            " -5.8821e-02  3.3254e-02  2.9468e-02  2.5949e-01  1.6012e-01  4.5017e-01\n",
            " -2.6535e-02  2.5883e-01  6.2274e-01  3.9724e-01 -4.2582e-01  7.8368e-02\n",
            " -1.7108e-01  9.3726e-02  5.5443e-01  2.9364e+00 -2.9856e-01 -2.6919e-01\n",
            "  2.6883e-01 -4.5052e-02 -2.6976e-01  1.2477e-01  3.2422e-02  2.1593e-01\n",
            "  1.2230e-01 -7.0885e-01 -1.3760e-01  3.8023e-01 -2.8467e-01 -4.2573e-01\n",
            " -1.2020e+00 -4.2784e-01 -1.8816e-01 -1.9955e-01  2.3510e-01 -3.0719e-01\n",
            "  1.1255e-01 -1.1238e-01 -3.0902e-01  2.9093e-01  5.8251e-01 -6.7236e-02\n",
            "  1.1878e-01  4.7467e-01 -9.4490e-03  2.6370e-01  6.7172e-02  8.6137e-01\n",
            "  7.9867e-01 -1.1649e-01 -3.4968e-01  3.5306e-01 -1.6667e-01  9.5608e-02\n",
            " -5.3847e-02  2.6636e-01 -8.5124e-02 -3.4143e-01  5.2658e-01 -2.5312e-01\n",
            "  3.6868e-01  1.4808e-01  5.0457e-01 -5.4672e-01 -5.7193e-03 -2.8087e-01\n",
            "  2.7664e-01 -1.1067e-01 -5.6938e-02  8.4386e-01 -1.5502e-02 -1.4716e-01\n",
            " -2.4319e-01  1.5463e-01 -1.4038e-01 -2.8210e-02  4.0895e-02 -6.9638e-02\n",
            "  7.3984e-03 -2.0785e-02 -1.9026e-01 -3.4531e-01  1.3974e-01 -1.7654e-01\n",
            "  3.6421e-01  3.5446e-02  8.9064e-03  3.2949e-01  7.8715e-02  1.4295e-01\n",
            "  3.4110e-01 -3.0039e-01  1.5895e-01 -4.4791e-01  2.8847e-01 -4.9616e-01\n",
            " -1.2712e-01  5.9769e-01 -1.7956e-01  1.2578e-01  2.2449e-01  1.3406e+00\n",
            "  3.1268e-01 -5.1212e-01 -2.8810e-01 -1.2124e-01 -3.6539e-01  2.3938e-01\n",
            "  6.0236e-01  5.5483e-01 -1.6237e-01 -9.7217e-03 -3.9532e-01 -8.0846e-02\n",
            " -2.0053e-01  4.3234e-01  3.7900e-01  2.1911e-01  5.0871e-01 -9.6578e-01\n",
            "  4.0803e-01 -8.7810e-02  4.1018e-01  3.3282e-01 -2.6322e-01 -3.1882e-01\n",
            " -4.6560e-01 -4.9194e-01  1.3927e-01  6.7400e-01  3.7538e-01 -4.7802e-01\n",
            " -9.5047e-02  4.3122e-01 -3.3629e-01 -2.8367e-01  2.4260e-01 -1.6096e-01\n",
            "  1.3671e+00  2.6446e-01  1.8355e-01 -2.1050e-01  2.1915e-03  3.2011e-01\n",
            " -2.2010e-01  2.1907e-01 -1.6236e-01 -1.5451e-01  1.8775e-01 -1.5687e-01\n",
            " -1.1443e-01  7.4750e-02 -8.2473e-03 -4.4988e-01 -1.2534e-01  1.0274e-03\n",
            " -2.6980e-01  3.8504e-01  1.2015e-01  1.8465e-01 -5.7222e-01  2.6703e-01\n",
            " -3.6049e-02  1.0055e+00 -1.2874e-01  3.1265e-02  1.3461e-01  1.4928e-01\n",
            " -2.8600e-01  2.3514e-01 -2.9374e-01 -2.9196e-01 -2.9380e-01 -3.0427e-01\n",
            "  1.2025e+00  5.6008e-02  8.0408e-02  2.3824e-01  4.0132e-02  5.8081e-02\n",
            " -1.9350e-01  3.4714e-01  1.3727e-02 -1.3661e-02 -2.9019e-01  1.1244e-01\n",
            " -2.6544e-01  9.0986e-02  9.8320e-02 -2.1316e-01  6.3920e-01  1.6703e-01\n",
            " -3.7282e-01  9.7439e-02]\n",
            "the:  [-7.1549e-02  9.3459e-02  2.3738e-02 -9.0339e-02  5.6123e-02  3.2547e-01\n",
            " -3.9796e-01 -9.2139e-02  6.1181e-02 -1.8950e-01  1.3061e-01  1.4349e-01\n",
            "  1.1479e-02  3.8158e-01  5.4030e-01 -1.4088e-01  2.4315e-01  2.3036e-01\n",
            " -5.5339e-01  4.8154e-02  4.5662e-01  3.2338e+00  2.0199e-02  4.9019e-02\n",
            " -1.4132e-02  7.6017e-02 -1.1527e-01  2.0060e-01 -7.7657e-02  2.4328e-01\n",
            "  1.6368e-01 -3.4118e-01 -6.6070e-02  1.0152e-01  3.8232e-02 -1.7668e-01\n",
            " -8.8153e-01 -3.3895e-01 -3.5481e-02 -5.5095e-01 -1.6899e-02 -4.3982e-01\n",
            "  3.9004e-02  4.0447e-01 -2.5880e-01  6.4594e-01  2.6641e-01  2.8009e-01\n",
            " -2.4625e-02  6.3302e-01 -3.1700e-01  1.0271e-01  3.0886e-01  9.7792e-02\n",
            " -3.8227e-01  8.6552e-02  4.7075e-02  2.3511e-01 -3.2127e-01 -2.8538e-01\n",
            "  1.6670e-01 -4.9707e-03 -6.2714e-01 -2.4904e-01  2.9713e-01  1.4379e-01\n",
            " -1.2325e-01 -5.8178e-02 -1.0290e-03 -8.2126e-02  3.6935e-01 -5.8442e-04\n",
            "  3.4286e-01  2.8426e-01 -6.8599e-02  6.5747e-01 -2.9087e-02  1.6184e-01\n",
            "  7.3672e-02 -3.0343e-01  9.5733e-02 -5.2860e-01 -2.2898e-01  6.4079e-02\n",
            "  1.5218e-02  3.4921e-01 -4.3960e-01 -4.3983e-01  7.7515e-01 -8.7767e-01\n",
            " -8.7504e-02  3.9598e-01  6.2362e-01 -2.6211e-01 -3.0539e-01 -2.2964e-02\n",
            "  3.0567e-01  6.7660e-02  1.5383e-01 -1.1211e-01 -9.1540e-02  8.2562e-02\n",
            "  1.6897e-01 -3.2952e-02 -2.8775e-01 -2.2320e-01 -9.0426e-02  1.2407e+00\n",
            " -1.8244e-01 -7.5219e-03 -4.1388e-02 -1.1083e-02  7.8186e-02  3.8511e-01\n",
            "  2.3334e-01  1.4414e-01 -9.1070e-04 -2.6388e-01 -2.0481e-01  1.0099e-01\n",
            "  1.4076e-01  2.8834e-01 -4.5429e-02  3.7247e-01  1.3645e-01 -6.7457e-01\n",
            "  2.2786e-01  1.2599e-01  2.9091e-02  3.0428e-02 -1.3028e-01  1.9408e-01\n",
            "  4.9014e-01 -3.9121e-01 -7.5952e-02  7.4731e-02  1.8902e-01 -1.6922e-01\n",
            " -2.6019e-01 -3.9771e-02 -2.4153e-01  1.0875e-01  3.0434e-01  3.6009e-02\n",
            "  1.4264e+00  1.2759e-01 -7.3811e-02 -2.0418e-01  8.0016e-03  1.5381e-01\n",
            "  2.0223e-01  2.8274e-01  9.6206e-02 -3.3634e-01  5.0983e-01  3.2625e-01\n",
            " -2.6535e-01  3.7400e-01 -3.0388e-01 -4.0033e-01 -4.2910e-02 -6.7897e-02\n",
            " -2.9332e-01  1.0978e-01 -4.5365e-02  2.3222e-01 -3.1134e-01 -2.8983e-01\n",
            " -6.6687e-01  5.3097e-01  1.9461e-01  3.6670e-01  2.6185e-01 -6.5187e-01\n",
            "  1.0266e-01  1.1363e-01 -1.2953e-01 -6.8246e-01 -1.8751e-01  1.4760e-01\n",
            "  1.0765e+00 -2.2908e-01 -9.3435e-03 -2.0651e-01 -3.5225e-01 -2.6720e-01\n",
            " -3.4307e-03  2.5906e-01  2.1759e-01  6.6158e-01  1.2180e-01  1.9957e-01\n",
            " -2.0303e-01  3.4474e-01 -2.4328e-01  1.3139e-01 -8.8767e-03  3.3617e-01\n",
            "  3.0591e-02  2.5577e-01]\n",
            "lazy:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "dog:  [-1.3791e-01 -4.7601e-01 -5.6369e-02 -3.9082e-01 -1.7544e-01 -6.2244e-01\n",
            " -3.9816e-01  2.9620e-01 -6.0647e-02 -6.7017e-02  1.1466e-01 -3.3015e-01\n",
            " -2.0318e-02  6.0616e-01 -1.3920e-01  1.3896e-01 -5.4781e-01  3.0864e-01\n",
            "  1.7354e-01  3.9927e-01  2.1137e-01  1.3004e+00  8.8030e-01  2.3946e-01\n",
            "  2.8838e-01 -4.6336e-01  2.5745e-01 -3.1755e-01 -3.2877e-01 -5.9534e-01\n",
            "  2.3983e-01  3.4159e-01  1.2754e-01 -8.8208e-01  1.4258e-01 -1.8857e-01\n",
            " -1.6961e-01  2.7808e-01 -2.4600e-01  1.9122e-01  5.0244e-01  5.3660e-01\n",
            " -5.3568e-01  2.4827e-01  3.2561e-01  6.7882e-01  9.6401e-01 -2.8892e-01\n",
            "  5.1206e-01  5.8496e-01 -3.1934e-02 -2.4849e-02  8.8564e-02  1.7360e-01\n",
            "  5.4166e-01 -8.6743e-02 -3.8412e-01  1.3974e-01 -7.4122e-03  9.2210e-01\n",
            " -2.5799e-01 -4.7018e-01 -5.5742e-01 -2.1213e-02 -7.1072e-01  8.0995e-02\n",
            " -4.7254e-01 -3.2925e-01  6.8052e-01  1.7242e-01  8.7783e-02 -2.6560e-01\n",
            " -6.0070e-01 -8.5217e-02 -3.6977e-02 -3.6593e-01 -6.2576e-01 -3.4162e-01\n",
            "  5.4672e-02 -1.1734e-01  1.9686e-01  8.3758e-02  4.3157e-01 -8.2195e-01\n",
            " -5.7756e-01  6.7821e-02 -4.9520e-01  1.4769e-01  3.2863e-01 -1.0649e+00\n",
            " -3.9756e-01 -3.4890e-01 -6.1548e-02  7.5400e-01  5.2457e-01  1.3657e-01\n",
            " -4.1904e-02 -4.4660e-01  8.4754e-02  3.7516e-01 -6.2374e-02 -8.1762e-02\n",
            " -4.1776e-01  3.0157e-02 -7.7967e-01  8.7627e-02  9.0542e-02  7.5266e-01\n",
            "  2.9235e-02 -1.8324e-01  5.5433e-01 -3.4632e-01 -7.8019e-02 -1.2078e-01\n",
            " -6.8377e-01  2.8826e-02 -4.1618e-01  2.2341e-01 -8.0811e-01 -5.9707e-01\n",
            "  4.6835e-01 -3.8246e-01 -2.3549e-01 -6.2565e-01  6.1201e-01  1.4221e-01\n",
            "  2.4076e-02  4.2106e-01  5.1978e-01  2.7811e-01  1.6885e-01 -4.9293e-01\n",
            "  3.1563e-01 -5.9663e-01  1.5091e-01 -6.3300e-01  1.1324e-03 -6.2031e-02\n",
            " -3.8694e-02 -2.6038e-01  2.1907e-01  2.3103e-01  8.2427e-01  1.4963e-01\n",
            "  8.5767e-01  1.5706e-01 -2.9116e-01 -4.2033e-01  4.5080e-01  3.9614e-01\n",
            " -2.0271e-01  1.0702e+00 -4.1153e-01  2.2282e-01  1.3287e-01  9.3896e-01\n",
            "  1.6088e-01 -2.2247e-01 -1.1443e+00 -5.0556e-01 -1.9619e-01  3.4685e-01\n",
            "  5.9883e-01  2.7666e-02 -2.1223e-01 -5.4970e-01 -1.6784e-01  5.2375e-01\n",
            " -9.8196e-02  1.5559e-01 -2.3997e-01 -1.1526e-01  1.6577e-02  6.1643e-02\n",
            "  9.2234e-02  2.5817e-02  2.9163e-01 -5.0848e-01 -1.5164e-01  1.8360e-01\n",
            "  1.6434e+00 -3.1567e-01 -5.3165e-01 -4.4914e-01 -7.2425e-01 -4.1122e-01\n",
            "  1.8799e-01  1.6130e-01  7.4169e-01 -2.1597e-02 -3.7775e-01 -5.5265e-01\n",
            "  7.3535e-02  8.5621e-02 -6.6452e-02  5.4982e-02 -4.0218e-01  4.3235e-01\n",
            " -7.5727e-02  7.0530e-02]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mmp1l_CoNKB-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------------\n",
        "# code for working with movie reviews data \n",
        "# Source: Miller, T. W. (2016). Web and Network Data Science.\n",
        "#    Upper Saddle River, N.J.: Pearson Education.\n",
        "#    ISBN-13: 978-0-13-388644-3\n",
        "# This original study used a simple bag-of-words approach\n",
        "# to sentiment analysis, along with pre-defined lists of\n",
        "# negative and positive words.        \n",
        "# Code available at:  https://github.com/mtpa/wnds       \n",
        "# ------------------------------------------------------------"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PqNYnQ9pNKCB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Utility function to get file names within a directory\n",
        "def listdir_no_hidden(path):\n",
        "    start_list = os.listdir(path)\n",
        "    end_list = []\n",
        "    for file in start_list:\n",
        "        if (not file.startswith('.')):\n",
        "            end_list.append(file)\n",
        "    return(end_list)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UK48GoOLNKCH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# define list of codes to be dropped from document\n",
        "# carriage-returns, line-feeds, tabs\n",
        "codelist = ['\\r', '\\n', '\\t']   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VWD35yR5NKCJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# We will not remove stopwords in this exercise because they are\n",
        "# important to keeping sentences intact\n",
        "if REMOVE_STOPWORDS:\n",
        "    print(nltk.corpus.stopwords.words('english'))\n",
        "\n",
        "# previous analysis of a list of top terms showed a number of words, along \n",
        "# with contractions and other word strings to drop from further analysis, add\n",
        "# these to the usual English stopwords to be dropped from a document collection\n",
        "    more_stop_words = ['cant','didnt','doesnt','dont','goes','isnt','hes',\\\n",
        "        'shes','thats','theres','theyre','wont','youll','youre','youve', 'br'\\\n",
        "        've', 're', 'vs'] \n",
        "\n",
        "    some_proper_nouns_to_remove = ['dick','ginger','hollywood','jack',\\\n",
        "        'jill','john','karloff','kudrow','orson','peter','tcm','tom',\\\n",
        "        'toni','welles','william','wolheim','nikita']\n",
        "\n",
        "    # start with the initial list and add to it for movie text work \n",
        "    stoplist = nltk.corpus.stopwords.words('english') + more_stop_words +\\\n",
        "        some_proper_nouns_to_remove"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GLz3hQ-JNKCK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# text parsing function for creating text documents \n",
        "# there is more we could do for data preparation \n",
        "# stemming... looking for contractions... possessives... \n",
        "# but we will work with what we have in this parsing function\n",
        "# if we want to do stemming at a later time, we can use\n",
        "#     porter = nltk.PorterStemmer()  \n",
        "# in a construction like this\n",
        "#     words_stemmed =  [porter.stem(word) for word in initial_words]  \n",
        "def text_parse(string):\n",
        "    # replace non-alphanumeric with space \n",
        "    temp_string = re.sub('[^a-zA-Z]', '  ', string)    \n",
        "    # replace codes with space\n",
        "    for i in range(len(codelist)):\n",
        "        stopstring = ' ' + codelist[i] + '  '\n",
        "        temp_string = re.sub(stopstring, '  ', temp_string)      \n",
        "    # replace single-character words with space\n",
        "    temp_string = re.sub('\\s.\\s', ' ', temp_string)   \n",
        "    # convert uppercase to lowercase\n",
        "    temp_string = temp_string.lower()    \n",
        "    if REMOVE_STOPWORDS:\n",
        "        # replace selected character strings/stop-words with space\n",
        "        for i in range(len(stoplist)):\n",
        "            stopstring = ' ' + str(stoplist[i]) + ' '\n",
        "            temp_string = re.sub(stopstring, ' ', temp_string)        \n",
        "    # replace multiple blank characters with one blank character\n",
        "    temp_string = re.sub('\\s+', ' ', temp_string)    \n",
        "    return(temp_string)    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YnnMXmZ9NKCL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Manually import these"
      ]
    },
    {
      "metadata": {
        "id": "VJoN2vqLNKCW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "68c8c5b8-fe8d-4020-c997-4f1062cdbe6d"
      },
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------\n",
        "# gather data for 500 negative movie reviews\n",
        "# -----------------------------------------------\n",
        "dir_name = 'movie-reviews-negative'\n",
        "    \n",
        "filenames = listdir_no_hidden(path=dir_name)\n",
        "num_files = len(filenames)\n",
        "\n",
        "for i in range(len(filenames)):\n",
        "    file_exists = os.path.isfile(os.path.join(dir_name, filenames[i]))\n",
        "    assert file_exists\n",
        "print('\\nDirectory:',dir_name)    \n",
        "print('%d files found' % len(filenames))"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Directory: movie-reviews-negative\n",
            "500 files found\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "FszOoiI6NKCd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "15d9a520-5285-467e-c9ec-d756f5b7c948"
      },
      "cell_type": "code",
      "source": [
        "# Read data for negative movie reviews\n",
        "# Data will be stored in a list of lists where the each list represents \n",
        "# a document and document is a list of words.\n",
        "# We then break the text into words.\n",
        "\n",
        "def read_data(filename):\n",
        "\n",
        "  #with open(filename, encoding='utf-8') as f:\n",
        "  with open(filename) as f:\n",
        "    data = tf.compat.as_str(f.read())\n",
        "    data = data.lower()\n",
        "    data = text_parse(data)\n",
        "    data = TreebankWordTokenizer().tokenize(data)  # The Penn Treebank\n",
        "\n",
        "  return data\n",
        "\n",
        "negative_documents = []\n",
        "\n",
        "print('\\nProcessing document files under', dir_name)\n",
        "for i in range(num_files):\n",
        "    ## print(' ', filenames[i])\n",
        "\n",
        "    words = read_data(os.path.join(dir_name, filenames[i]))\n",
        "\n",
        "    negative_documents.append(words)\n",
        "    # print('Data size (Characters) (Document %d) %d' %(i,len(words)))\n",
        "    # print('Sample string (Document %d) %s'%(i,words[:50]))"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Processing document files under movie-reviews-negative\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "F3N-1zNhNKCq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "0c3a5182-37eb-49d6-c583-d5cf0f2814b7"
      },
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------\n",
        "# gather data for 500 positive movie reviews\n",
        "# -----------------------------------------------\n",
        "dir_name = 'movie-reviews-positive'  \n",
        "filenames = listdir_no_hidden(path=dir_name)\n",
        "num_files = len(filenames)\n",
        "\n",
        "for i in range(len(filenames)):\n",
        "    file_exists = os.path.isfile(os.path.join(dir_name, filenames[i]))\n",
        "    assert file_exists\n",
        "print('\\nDirectory:',dir_name)    \n",
        "print('%d files found' % len(filenames))"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Directory: movie-reviews-positive\n",
            "500 files found\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "L-vS1IAPNKCs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "a9b140f0-5a4a-44a2-ed7c-b38db184ec8e"
      },
      "cell_type": "code",
      "source": [
        "# Read data for positive movie reviews\n",
        "# Data will be stored in a list of lists where the each list \n",
        "# represents a document and document is a list of words.\n",
        "# We then break the text into words.\n",
        "\n",
        "def read_data(filename):\n",
        "\n",
        "  with open(filename) as f:\n",
        "    data = tf.compat.as_str(f.read())\n",
        "    data = data.lower()\n",
        "    data = text_parse(data)\n",
        "    data = TreebankWordTokenizer().tokenize(data)  # The Penn Treebank\n",
        "\n",
        "  return data\n",
        "\n",
        "positive_documents = []\n",
        "\n",
        "print('\\nProcessing document files under', dir_name)\n",
        "for i in range(num_files):\n",
        "    ## print(' ', filenames[i])\n",
        "\n",
        "    words = read_data(os.path.join(dir_name, filenames[i]))\n",
        "\n",
        "    positive_documents.append(words)\n",
        "    # print('Data size (Characters) (Document %d) %d' %(i,len(words)))\n",
        "    # print('Sample string (Document %d) %s'%(i,words[:50]))\n",
        "\n"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Processing document files under movie-reviews-positive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "XXoKl237NKCu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "aac74185-6b46-4dd9-c77b-8c95f59948c5"
      },
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------------\n",
        "# convert positive/negative documents into numpy array\n",
        "# note that reviews vary from 22 to 1052 words   \n",
        "# so we use the first 20 and last 20 words of each review \n",
        "# as our word sequences for analysis\n",
        "# -----------------------------------------------------\n",
        "max_review_length = 0  # initialize\n",
        "for doc in negative_documents:\n",
        "    max_review_length = max(max_review_length, len(doc))    \n",
        "for doc in positive_documents:\n",
        "    max_review_length = max(max_review_length, len(doc)) \n",
        "print('max_review_length:', max_review_length) \n",
        "\n",
        "min_review_length = max_review_length  # initialize\n",
        "for doc in negative_documents:\n",
        "    min_review_length = min(min_review_length, len(doc))    \n",
        "for doc in positive_documents:\n",
        "    min_review_length = min(min_review_length, len(doc)) \n",
        "print('min_review_length:', min_review_length) \n",
        "\n"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "max_review_length: 1052\n",
            "min_review_length: 22\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2lw2oWAdNKCz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# construct list of 1000 lists with 40 words in each list\n",
        "from itertools import chain\n",
        "documents = []\n",
        "for doc in negative_documents:\n",
        "    doc_begin = doc[0:20]\n",
        "    doc_end = doc[len(doc) - 20: len(doc)]\n",
        "    documents.append(list(chain(*[doc_begin, doc_end])))    \n",
        "for doc in positive_documents:\n",
        "    doc_begin = doc[0:20]\n",
        "    doc_end = doc[len(doc) - 20: len(doc)]\n",
        "    documents.append(list(chain(*[doc_begin, doc_end])))    \n",
        "\n",
        "# create list of lists of lists for embeddings\n",
        "embeddings = []    \n",
        "for doc in documents:\n",
        "    embedding = []\n",
        "    for word in doc:\n",
        "       embedding.append(limited_index_to_embedding[limited_word_to_index[word]]) \n",
        "    embeddings.append(embedding)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SC7jQzQ6NKC5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1224
        },
        "outputId": "571b4d2e-3e05-4288-d8b6-73d0a33a1ffd"
      },
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------------    \n",
        "# Check on the embeddings list of list of lists \n",
        "# -----------------------------------------------------\n",
        "# Show the first word in the first document\n",
        "test_word = documents[0][0]    \n",
        "print('First word in first document:', test_word)    \n",
        "print('Embedding for this word:\\n', \n",
        "      limited_index_to_embedding[limited_word_to_index[test_word]])\n",
        "print('Corresponding embedding from embeddings list of list of lists\\n',\n",
        "      embeddings[0][0][:])\n",
        "\n"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First word in first document: strained\n",
            "Embedding for this word:\n",
            " [-9.6104e-02 -4.9919e-01  3.1536e-01  1.9711e-01 -4.3080e-01  3.0870e-01\n",
            " -1.4987e-01  3.0923e-01 -8.2258e-01  3.7316e-01 -1.3906e-01  5.4338e-01\n",
            " -4.5029e-01  8.3690e-02  6.2485e-01  4.3483e-01 -8.6542e-01  7.1274e-01\n",
            " -4.5097e-01 -1.7131e-01  6.3485e-01  1.5939e+00 -6.0457e-01  2.0648e-01\n",
            " -1.5717e-01  1.5143e-01  6.6534e-01  1.0821e-01 -3.4151e-01 -1.0388e-01\n",
            " -4.2171e-01 -4.7619e-01 -5.4476e-01  2.7599e-01 -1.5306e-01 -2.3153e-02\n",
            " -2.1823e-01  1.1273e-01 -8.9172e-01 -7.4871e-01 -3.4198e-01  1.4960e-01\n",
            " -5.1809e-01  3.5296e-01  7.0884e-01 -1.5524e-01 -2.3003e-01 -5.0443e-01\n",
            " -1.5025e-01  1.6477e-01  1.5172e-01  5.2337e-01 -2.8257e-01  3.6122e-01\n",
            "  3.9183e-01 -4.0019e-01  6.3946e-01  3.7599e-02 -1.6185e-01 -3.4232e-01\n",
            "  3.8010e-01  3.7329e-01 -5.5415e-01 -2.0802e-01  3.4501e-01 -3.9860e-01\n",
            "  3.5179e-01 -4.5948e-01  4.3508e-01 -2.3781e-02  2.5629e-01  2.3058e-01\n",
            " -3.6185e-01  1.9051e-01 -7.2532e-01  4.0383e-01 -4.6919e-01 -4.6535e-01\n",
            "  4.9748e-02  2.8736e-02 -7.5741e-01 -7.2535e-01  2.3552e-01 -6.3291e-01\n",
            "  3.4161e-01 -3.1012e-01  9.8412e-01 -3.9369e-01 -2.3284e-02  3.5040e-01\n",
            " -1.1184e-01 -3.1817e-01  1.2922e-01 -3.3598e-01 -5.8844e-01  2.1254e-01\n",
            " -4.9977e-01  3.4264e-02  4.0339e-02  5.1240e-01  1.5904e-01 -2.2130e-01\n",
            " -2.6743e-01  8.4299e-04 -4.2637e-01  4.2900e-01 -6.2602e-01 -5.0934e-01\n",
            " -6.4174e-03 -5.4732e-01 -4.7422e-01  9.5203e-02 -5.0472e-01 -1.5143e-02\n",
            "  1.6225e-01 -7.0379e-02 -1.1214e+00  4.3672e-02  1.3457e-01 -2.2615e-02\n",
            " -1.5405e-01  3.1841e-01 -9.4757e-02  1.1208e+00  2.0000e-01  1.8864e-01\n",
            "  1.1271e-01  4.2490e-02 -3.4035e-01  1.0283e+00  7.2587e-01 -7.5562e-02\n",
            " -2.1494e-01 -5.4325e-01 -2.0568e-01 -1.2925e-01 -6.1079e-02 -1.3631e-02\n",
            " -7.1170e-01 -3.0614e-01 -6.6467e-01 -2.0466e-01 -3.0216e-01  1.7461e-01\n",
            "  1.1361e+00  4.9711e-01 -2.8688e-01 -2.7638e-01  6.3191e-01 -4.7838e-01\n",
            " -7.9403e-02 -6.8140e-01 -1.4690e-01 -2.4596e-01 -5.8379e-01 -1.7419e-01\n",
            " -3.3996e-01  2.6822e-01  5.5732e-01 -3.3832e-02  5.0178e-01 -7.2444e-01\n",
            " -5.8312e-01  7.2225e-01 -5.6705e-01  1.8360e-01  5.9092e-01  1.0928e+00\n",
            " -9.9976e-02  1.4157e-01 -3.7656e-02 -3.7221e-01 -3.9105e-01  4.6243e-01\n",
            "  2.1704e-01  2.6170e-01  5.3678e-02 -6.6062e-01  1.1492e+00 -3.8682e-02\n",
            "  1.3699e-01 -7.9457e-02 -3.0951e-01  8.0134e-01 -7.5186e-01 -1.4870e-01\n",
            "  9.8889e-02 -2.3245e-01  1.8529e-01  2.4994e-01  1.9563e-01 -7.7170e-01\n",
            " -5.0174e-01  2.7482e-01 -3.9986e-01 -2.1185e-01 -2.1390e-01  2.9093e-01\n",
            " -3.3584e-01  8.6414e-02]\n",
            "Corresponding embedding from embeddings list of list of lists\n",
            " [-9.6104e-02 -4.9919e-01  3.1536e-01  1.9711e-01 -4.3080e-01  3.0870e-01\n",
            " -1.4987e-01  3.0923e-01 -8.2258e-01  3.7316e-01 -1.3906e-01  5.4338e-01\n",
            " -4.5029e-01  8.3690e-02  6.2485e-01  4.3483e-01 -8.6542e-01  7.1274e-01\n",
            " -4.5097e-01 -1.7131e-01  6.3485e-01  1.5939e+00 -6.0457e-01  2.0648e-01\n",
            " -1.5717e-01  1.5143e-01  6.6534e-01  1.0821e-01 -3.4151e-01 -1.0388e-01\n",
            " -4.2171e-01 -4.7619e-01 -5.4476e-01  2.7599e-01 -1.5306e-01 -2.3153e-02\n",
            " -2.1823e-01  1.1273e-01 -8.9172e-01 -7.4871e-01 -3.4198e-01  1.4960e-01\n",
            " -5.1809e-01  3.5296e-01  7.0884e-01 -1.5524e-01 -2.3003e-01 -5.0443e-01\n",
            " -1.5025e-01  1.6477e-01  1.5172e-01  5.2337e-01 -2.8257e-01  3.6122e-01\n",
            "  3.9183e-01 -4.0019e-01  6.3946e-01  3.7599e-02 -1.6185e-01 -3.4232e-01\n",
            "  3.8010e-01  3.7329e-01 -5.5415e-01 -2.0802e-01  3.4501e-01 -3.9860e-01\n",
            "  3.5179e-01 -4.5948e-01  4.3508e-01 -2.3781e-02  2.5629e-01  2.3058e-01\n",
            " -3.6185e-01  1.9051e-01 -7.2532e-01  4.0383e-01 -4.6919e-01 -4.6535e-01\n",
            "  4.9748e-02  2.8736e-02 -7.5741e-01 -7.2535e-01  2.3552e-01 -6.3291e-01\n",
            "  3.4161e-01 -3.1012e-01  9.8412e-01 -3.9369e-01 -2.3284e-02  3.5040e-01\n",
            " -1.1184e-01 -3.1817e-01  1.2922e-01 -3.3598e-01 -5.8844e-01  2.1254e-01\n",
            " -4.9977e-01  3.4264e-02  4.0339e-02  5.1240e-01  1.5904e-01 -2.2130e-01\n",
            " -2.6743e-01  8.4299e-04 -4.2637e-01  4.2900e-01 -6.2602e-01 -5.0934e-01\n",
            " -6.4174e-03 -5.4732e-01 -4.7422e-01  9.5203e-02 -5.0472e-01 -1.5143e-02\n",
            "  1.6225e-01 -7.0379e-02 -1.1214e+00  4.3672e-02  1.3457e-01 -2.2615e-02\n",
            " -1.5405e-01  3.1841e-01 -9.4757e-02  1.1208e+00  2.0000e-01  1.8864e-01\n",
            "  1.1271e-01  4.2490e-02 -3.4035e-01  1.0283e+00  7.2587e-01 -7.5562e-02\n",
            " -2.1494e-01 -5.4325e-01 -2.0568e-01 -1.2925e-01 -6.1079e-02 -1.3631e-02\n",
            " -7.1170e-01 -3.0614e-01 -6.6467e-01 -2.0466e-01 -3.0216e-01  1.7461e-01\n",
            "  1.1361e+00  4.9711e-01 -2.8688e-01 -2.7638e-01  6.3191e-01 -4.7838e-01\n",
            " -7.9403e-02 -6.8140e-01 -1.4690e-01 -2.4596e-01 -5.8379e-01 -1.7419e-01\n",
            " -3.3996e-01  2.6822e-01  5.5732e-01 -3.3832e-02  5.0178e-01 -7.2444e-01\n",
            " -5.8312e-01  7.2225e-01 -5.6705e-01  1.8360e-01  5.9092e-01  1.0928e+00\n",
            " -9.9976e-02  1.4157e-01 -3.7656e-02 -3.7221e-01 -3.9105e-01  4.6243e-01\n",
            "  2.1704e-01  2.6170e-01  5.3678e-02 -6.6062e-01  1.1492e+00 -3.8682e-02\n",
            "  1.3699e-01 -7.9457e-02 -3.0951e-01  8.0134e-01 -7.5186e-01 -1.4870e-01\n",
            "  9.8889e-02 -2.3245e-01  1.8529e-01  2.4994e-01  1.9563e-01 -7.7170e-01\n",
            " -5.0174e-01  2.7482e-01 -3.9986e-01 -2.1185e-01 -2.1390e-01  2.9093e-01\n",
            " -3.3584e-01  8.6414e-02]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "U5ZHALRXNKC9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1224
        },
        "outputId": "f53a53c3-f765-4b11-ac64-ac3404ca982a"
      },
      "cell_type": "code",
      "source": [
        "# Show the seventh word in the tenth document\n",
        "test_word = documents[6][9]    \n",
        "print('First word in first document:', test_word)    \n",
        "print('Embedding for this word:\\n', \n",
        "      limited_index_to_embedding[limited_word_to_index[test_word]])\n",
        "print('Corresponding embedding from embeddings list of list of lists\\n',\n",
        "      embeddings[6][9][:])\n",
        "\n"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First word in first document: ever\n",
            "Embedding for this word:\n",
            " [ 2.1330e-01  1.0159e-01 -7.9846e-01 -3.1258e-01 -5.2301e-01  8.5879e-02\n",
            " -3.9847e-01  1.2413e-01  5.3686e-01 -2.8413e-01 -2.2199e-01  8.3012e-02\n",
            " -3.2399e-01  7.8373e-02  4.1602e-01  3.5626e-01  2.7240e-02  6.1429e-01\n",
            "  2.5173e-01  1.5019e-01  1.7121e-01  2.8553e+00  2.0633e-01  2.4040e-02\n",
            " -4.4565e-02 -2.0433e-01  2.5073e-01  2.6887e-01  2.3178e-02 -4.4504e-01\n",
            " -2.2790e-01 -2.6710e-01  5.3275e-01  6.8353e-02 -2.2983e-01  2.5972e-01\n",
            " -5.9040e-01 -3.0783e-02  1.3726e-01 -9.6350e-02 -4.2854e-01 -3.7986e-01\n",
            " -1.6216e-01  4.2504e-01 -4.1500e-01  2.3998e-03  1.0266e-01  1.8879e-01\n",
            "  4.0432e-01 -2.7399e-01 -2.6548e-01 -1.6831e-01 -1.1130e-01  4.1575e-01\n",
            "  5.1421e-01  1.4271e-01 -4.9686e-02 -8.0578e-02 -7.9020e-01  1.2625e-01\n",
            " -2.5764e-01  2.1986e-01 -3.8605e-01 -1.0071e-01  3.4754e-01  1.5648e-02\n",
            " -5.2139e-02  1.4517e-01  1.3641e-01  1.5839e-03  7.0667e-01 -2.1576e-02\n",
            "  3.8876e-01  2.2115e-01 -1.9469e-01  2.6829e-01 -2.8725e-01 -1.1961e-01\n",
            " -8.8836e-02 -1.0590e-01 -5.6784e-02 -1.3934e-01 -7.1522e-02 -5.7851e-02\n",
            " -1.2869e-01 -2.5081e-01 -2.2837e-02 -9.1676e-03  4.9658e-01 -9.0234e-01\n",
            " -2.5347e-01 -1.6432e-01  4.0843e-01 -2.3894e-01 -1.2900e-01 -1.5628e-01\n",
            "  3.2472e-01 -5.7837e-04 -1.8881e-01 -1.5174e-01  2.1750e-01 -3.0693e-01\n",
            " -3.1859e-01 -3.0137e-01  2.5286e-01 -1.0822e-01 -3.0438e-01  9.4749e-01\n",
            " -4.9148e-01 -3.5039e-02  3.8469e-01 -2.9068e-01 -3.8954e-02 -2.8539e-01\n",
            "  1.1991e-01  1.2586e-01 -2.1127e-01 -1.6648e-01 -6.2480e-01 -1.2853e-01\n",
            "  2.3134e-01 -2.9600e-01  2.8460e-01 -1.1783e-01 -3.2090e-01 -6.4879e-01\n",
            " -1.2510e-01 -1.0019e-02  5.2994e-01 -4.7399e-01 -2.5558e-01 -1.4615e-01\n",
            "  2.5803e-01 -2.4356e-01 -1.1342e-01  9.3975e-02 -8.4328e-02  3.9791e-01\n",
            "  2.1943e-02  1.3591e-01 -2.7038e-01 -3.7911e-01  1.3448e-01 -6.9800e-02\n",
            "  1.1493e+00  2.1612e-01  2.6659e-01 -4.3342e-01  1.0608e-01  1.8793e-01\n",
            "  2.0271e-01 -3.0976e-01 -3.3623e-01  2.4832e-01 -4.6154e-01  1.9787e-01\n",
            "  9.5651e-02  1.3149e-01 -1.0407e-01  4.4642e-01 -2.8652e-01  1.8765e-01\n",
            "  9.6509e-02 -1.7640e-01 -4.0949e-02 -3.2538e-02 -2.4234e-01 -1.9571e-01\n",
            " -5.8448e-01 -6.2755e-02 -1.0115e-01  2.0660e-02  1.0226e-01 -3.2458e-03\n",
            "  6.6059e-02 -7.9512e-02 -5.2104e-01 -2.2644e-01  2.0442e-01  2.7293e-01\n",
            "  1.5559e+00  6.3341e-02 -4.9536e-01  1.5236e-02 -6.1479e-01 -3.6319e-01\n",
            "  3.7308e-01  8.4875e-01  2.1099e-01 -1.1354e-01 -5.5955e-01 -8.2692e-02\n",
            " -2.0785e-01 -6.1039e-02  2.7334e-01  1.2062e-01 -6.6524e-02  2.5887e-01\n",
            "  2.4723e-01 -7.0231e-02]\n",
            "Corresponding embedding from embeddings list of list of lists\n",
            " [ 2.1330e-01  1.0159e-01 -7.9846e-01 -3.1258e-01 -5.2301e-01  8.5879e-02\n",
            " -3.9847e-01  1.2413e-01  5.3686e-01 -2.8413e-01 -2.2199e-01  8.3012e-02\n",
            " -3.2399e-01  7.8373e-02  4.1602e-01  3.5626e-01  2.7240e-02  6.1429e-01\n",
            "  2.5173e-01  1.5019e-01  1.7121e-01  2.8553e+00  2.0633e-01  2.4040e-02\n",
            " -4.4565e-02 -2.0433e-01  2.5073e-01  2.6887e-01  2.3178e-02 -4.4504e-01\n",
            " -2.2790e-01 -2.6710e-01  5.3275e-01  6.8353e-02 -2.2983e-01  2.5972e-01\n",
            " -5.9040e-01 -3.0783e-02  1.3726e-01 -9.6350e-02 -4.2854e-01 -3.7986e-01\n",
            " -1.6216e-01  4.2504e-01 -4.1500e-01  2.3998e-03  1.0266e-01  1.8879e-01\n",
            "  4.0432e-01 -2.7399e-01 -2.6548e-01 -1.6831e-01 -1.1130e-01  4.1575e-01\n",
            "  5.1421e-01  1.4271e-01 -4.9686e-02 -8.0578e-02 -7.9020e-01  1.2625e-01\n",
            " -2.5764e-01  2.1986e-01 -3.8605e-01 -1.0071e-01  3.4754e-01  1.5648e-02\n",
            " -5.2139e-02  1.4517e-01  1.3641e-01  1.5839e-03  7.0667e-01 -2.1576e-02\n",
            "  3.8876e-01  2.2115e-01 -1.9469e-01  2.6829e-01 -2.8725e-01 -1.1961e-01\n",
            " -8.8836e-02 -1.0590e-01 -5.6784e-02 -1.3934e-01 -7.1522e-02 -5.7851e-02\n",
            " -1.2869e-01 -2.5081e-01 -2.2837e-02 -9.1676e-03  4.9658e-01 -9.0234e-01\n",
            " -2.5347e-01 -1.6432e-01  4.0843e-01 -2.3894e-01 -1.2900e-01 -1.5628e-01\n",
            "  3.2472e-01 -5.7837e-04 -1.8881e-01 -1.5174e-01  2.1750e-01 -3.0693e-01\n",
            " -3.1859e-01 -3.0137e-01  2.5286e-01 -1.0822e-01 -3.0438e-01  9.4749e-01\n",
            " -4.9148e-01 -3.5039e-02  3.8469e-01 -2.9068e-01 -3.8954e-02 -2.8539e-01\n",
            "  1.1991e-01  1.2586e-01 -2.1127e-01 -1.6648e-01 -6.2480e-01 -1.2853e-01\n",
            "  2.3134e-01 -2.9600e-01  2.8460e-01 -1.1783e-01 -3.2090e-01 -6.4879e-01\n",
            " -1.2510e-01 -1.0019e-02  5.2994e-01 -4.7399e-01 -2.5558e-01 -1.4615e-01\n",
            "  2.5803e-01 -2.4356e-01 -1.1342e-01  9.3975e-02 -8.4328e-02  3.9791e-01\n",
            "  2.1943e-02  1.3591e-01 -2.7038e-01 -3.7911e-01  1.3448e-01 -6.9800e-02\n",
            "  1.1493e+00  2.1612e-01  2.6659e-01 -4.3342e-01  1.0608e-01  1.8793e-01\n",
            "  2.0271e-01 -3.0976e-01 -3.3623e-01  2.4832e-01 -4.6154e-01  1.9787e-01\n",
            "  9.5651e-02  1.3149e-01 -1.0407e-01  4.4642e-01 -2.8652e-01  1.8765e-01\n",
            "  9.6509e-02 -1.7640e-01 -4.0949e-02 -3.2538e-02 -2.4234e-01 -1.9571e-01\n",
            " -5.8448e-01 -6.2755e-02 -1.0115e-01  2.0660e-02  1.0226e-01 -3.2458e-03\n",
            "  6.6059e-02 -7.9512e-02 -5.2104e-01 -2.2644e-01  2.0442e-01  2.7293e-01\n",
            "  1.5559e+00  6.3341e-02 -4.9536e-01  1.5236e-02 -6.1479e-01 -3.6319e-01\n",
            "  3.7308e-01  8.4875e-01  2.1099e-01 -1.1354e-01 -5.5955e-01 -8.2692e-02\n",
            " -2.0785e-01 -6.1039e-02  2.7334e-01  1.2062e-01 -6.6524e-02  2.5887e-01\n",
            "  2.4723e-01 -7.0231e-02]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "G3Ixn207NKDA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1224
        },
        "outputId": "87bd23db-0dfa-444e-9081-63ee0265ca8c"
      },
      "cell_type": "code",
      "source": [
        "# Show the last word in the last document\n",
        "test_word = documents[999][39]    \n",
        "print('First word in first document:', test_word)    \n",
        "print('Embedding for this word:\\n', \n",
        "      limited_index_to_embedding[limited_word_to_index[test_word]])\n",
        "print('Corresponding embedding from embeddings list of list of lists\\n',\n",
        "      embeddings[999][39][:])        \n",
        "\n"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First word in first document: out\n",
            "Embedding for this word:\n",
            " [ 0.08278    0.047335  -0.38854   -0.030124  -0.25428    0.351\n",
            " -0.26002   -0.46838   -0.048807   0.99838   -0.11526    0.45842\n",
            "  0.29279   -0.058347   0.11      -0.11706   -0.28233    0.1506\n",
            " -0.31632   -0.0068872  0.38673    3.1392     0.13653   -0.03454\n",
            "  0.35863   -0.12377   -0.45192   -0.38827   -0.25126   -0.12714\n",
            " -0.16456   -0.0213    -0.20211    0.082716   0.024684  -0.029757\n",
            " -0.64346   -0.3954     0.1412     0.3469     0.35425   -0.22096\n",
            "  0.06306    0.26536   -0.02977    0.27786    0.50287    0.12727\n",
            "  0.014117   0.25642   -0.045204   0.06792   -0.087254   0.44804\n",
            "  0.15815   -0.023931  -0.56487   -0.069249   0.095865   0.34341\n",
            " -0.017929  -0.2697    -0.42827   -0.21422    0.7592    -0.30571\n",
            " -0.37242   -0.032854  -0.20689   -0.15047    0.40052    0.087469\n",
            " -0.17631   -0.061099  -0.22828    0.13397   -0.14535   -0.24022\n",
            " -0.34951   -0.17367    0.41097   -0.34589   -0.1284     0.069622\n",
            "  0.15335   -0.21237   -0.35279   -0.5199     0.20454   -0.8524\n",
            " -0.43651   -0.20606    0.41662   -0.032318  -0.21709    0.42287\n",
            " -0.10071   -0.5374     0.014013  -0.31488    0.36917   -0.092383\n",
            " -0.0064015  0.45894    0.10606   -0.040475   0.15918    1.3422\n",
            " -0.47419   -0.12446   -0.10035   -0.17992    0.35506   -0.11075\n",
            "  0.33538    0.22004    0.12629    0.16089   -0.29405   -0.62881\n",
            "  0.54672    0.19233    0.21447    0.14141   -0.097707  -0.80096\n",
            "  0.40245    0.024641   0.3603    -0.32832   -0.10543    0.087132\n",
            "  0.25838   -0.93438   -0.071264   0.29401   -0.088105  -0.26681\n",
            " -0.19677    0.12828    0.35249   -0.05541    0.067679  -0.10851\n",
            "  1.4911     0.40962    0.03795   -0.45417    0.081169  -0.040032\n",
            "  0.31238    0.41232   -0.22689   -0.16883   -0.16087   -0.27297\n",
            " -0.75234   -0.14401   -0.12639   -0.34394   -0.39058    0.072552\n",
            "  0.25473    0.24631    0.35457    0.21095   -0.17613    0.051294\n",
            " -0.30058    0.078074  -0.097081   0.2568     0.33171    0.37339\n",
            "  0.1818    -0.16684   -0.76578   -0.092136   0.10767   -0.12707\n",
            "  1.0557    -0.67527   -0.33764    0.27431   -0.39686    0.016443\n",
            " -0.31431   -0.10314   -0.056415   0.23018   -0.073648  -0.30886\n",
            " -0.090317  -0.11952    0.29949   -0.087195   0.20275    0.21454\n",
            " -0.24295    0.048424 ]\n",
            "Corresponding embedding from embeddings list of list of lists\n",
            " [ 0.08278    0.047335  -0.38854   -0.030124  -0.25428    0.351\n",
            " -0.26002   -0.46838   -0.048807   0.99838   -0.11526    0.45842\n",
            "  0.29279   -0.058347   0.11      -0.11706   -0.28233    0.1506\n",
            " -0.31632   -0.0068872  0.38673    3.1392     0.13653   -0.03454\n",
            "  0.35863   -0.12377   -0.45192   -0.38827   -0.25126   -0.12714\n",
            " -0.16456   -0.0213    -0.20211    0.082716   0.024684  -0.029757\n",
            " -0.64346   -0.3954     0.1412     0.3469     0.35425   -0.22096\n",
            "  0.06306    0.26536   -0.02977    0.27786    0.50287    0.12727\n",
            "  0.014117   0.25642   -0.045204   0.06792   -0.087254   0.44804\n",
            "  0.15815   -0.023931  -0.56487   -0.069249   0.095865   0.34341\n",
            " -0.017929  -0.2697    -0.42827   -0.21422    0.7592    -0.30571\n",
            " -0.37242   -0.032854  -0.20689   -0.15047    0.40052    0.087469\n",
            " -0.17631   -0.061099  -0.22828    0.13397   -0.14535   -0.24022\n",
            " -0.34951   -0.17367    0.41097   -0.34589   -0.1284     0.069622\n",
            "  0.15335   -0.21237   -0.35279   -0.5199     0.20454   -0.8524\n",
            " -0.43651   -0.20606    0.41662   -0.032318  -0.21709    0.42287\n",
            " -0.10071   -0.5374     0.014013  -0.31488    0.36917   -0.092383\n",
            " -0.0064015  0.45894    0.10606   -0.040475   0.15918    1.3422\n",
            " -0.47419   -0.12446   -0.10035   -0.17992    0.35506   -0.11075\n",
            "  0.33538    0.22004    0.12629    0.16089   -0.29405   -0.62881\n",
            "  0.54672    0.19233    0.21447    0.14141   -0.097707  -0.80096\n",
            "  0.40245    0.024641   0.3603    -0.32832   -0.10543    0.087132\n",
            "  0.25838   -0.93438   -0.071264   0.29401   -0.088105  -0.26681\n",
            " -0.19677    0.12828    0.35249   -0.05541    0.067679  -0.10851\n",
            "  1.4911     0.40962    0.03795   -0.45417    0.081169  -0.040032\n",
            "  0.31238    0.41232   -0.22689   -0.16883   -0.16087   -0.27297\n",
            " -0.75234   -0.14401   -0.12639   -0.34394   -0.39058    0.072552\n",
            "  0.25473    0.24631    0.35457    0.21095   -0.17613    0.051294\n",
            " -0.30058    0.078074  -0.097081   0.2568     0.33171    0.37339\n",
            "  0.1818    -0.16684   -0.76578   -0.092136   0.10767   -0.12707\n",
            "  1.0557    -0.67527   -0.33764    0.27431   -0.39686    0.016443\n",
            " -0.31431   -0.10314   -0.056415   0.23018   -0.073648  -0.30886\n",
            " -0.090317  -0.11952    0.29949   -0.087195   0.20275    0.21454\n",
            " -0.24295    0.048424 ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "S0DgBr-9NKDE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------------    \n",
        "# Make embeddings a numpy array for use in an RNN \n",
        "# Create training and test sets with Scikit Learn\n",
        "# -----------------------------------------------------\n",
        "embeddings_array = np.array(embeddings)\n",
        "\n",
        "# Define the labels to be used 500 negative (0) and 500 positive (1)\n",
        "thumbs_down_up = np.concatenate((np.zeros((500), dtype = np.int32), \n",
        "                      np.ones((500), dtype = np.int32)), axis = 0)\n",
        "\n",
        "# Scikit Learn for random splitting of the data  \n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Random splitting of the data in to training (80%) and test (20%)  \n",
        "X_train, X_test, y_train, y_test = \\\n",
        "    train_test_split(embeddings_array, thumbs_down_up, test_size=0.20, \n",
        "                     random_state = RANDOM_SEED)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "g2g8SCgMNKDF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# --------------------------------------------------------------------------      \n",
        "# We use a very simple Recurrent Neural Network for this assignment\n",
        "# Gron, A. 2017. Hands-On Machine Learning with Scikit-Learn & TensorFlow: \n",
        "#    Concepts, Tools, and Techniques to Build Intelligent Systems. \n",
        "#    Sebastopol, Calif.: O'Reilly. [ISBN-13 978-1-491-96229-9] \n",
        "#    Chapter 14 Recurrent Neural Networks, pages 390-391\n",
        "#    Source code available at https://github.com/ageron/handson-ml\n",
        "#    Jupyter notebook file 14_recurrent_neural_networks.ipynb\n",
        "#    See section on Training an sequence Classifier, # In [34]:\n",
        "#    which uses the MNIST case data...  we revise to accommodate\n",
        "#    the movie review data in this assignment    \n",
        "# --------------------------------------------------------------------------  \n",
        "reset_graph()\n",
        "\n",
        "n_steps = embeddings_array.shape[1]  # number of words per document \n",
        "n_inputs = embeddings_array.shape[2]  # dimension of  pre-trained embeddings\n",
        "n_neurons = 20  # analyst specified number of neurons\n",
        "n_outputs = 2  # thumbs-down or thumbs-up\n",
        "\n",
        "learning_rate = 0.001"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MMCPh5TTNKDG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
        "y = tf.placeholder(tf.int32, [None])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "37_r8s6YNKDH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "basic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)\n",
        "outputs, states = tf.nn.dynamic_rnn(basic_cell, X, dtype=tf.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JMs58okvNKDK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "logits = tf.layers.dense(states, n_outputs)\n",
        "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,\n",
        "                                                          logits=logits)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9AUd85paNKDN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "loss = tf.reduce_mean(xentropy)\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "training_op = optimizer.minimize(loss)\n",
        "correct = tf.nn.in_top_k(logits, y, 1)\n",
        "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NnDikdaUNKDO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "init = tf.global_variables_initializer()\n",
        "\n",
        "n_epochs = 50\n",
        "batch_size = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gzbAFzKoNKDW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 11067
        },
        "outputId": "087a8ef8-af6c-4769-abe8-7fec63cb9ff4"
      },
      "cell_type": "code",
      "source": [
        "with tf.Session() as sess:\n",
        "    init.run()\n",
        "    for epoch in range(n_epochs):\n",
        "        print('\\n  ---- Epoch ', epoch, ' ----\\n')\n",
        "        for iteration in range(y_train.shape[0] // batch_size):          \n",
        "            X_batch = X_train[iteration*batch_size:(iteration + 1)*batch_size,:]\n",
        "            y_batch = y_train[iteration*batch_size:(iteration + 1)*batch_size]\n",
        "            print('  Batch ', iteration, ' training observations from ',  \n",
        "                  iteration*batch_size, ' to ', (iteration + 1)*batch_size-1,)\n",
        "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
        "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
        "        acc_test = accuracy.eval(feed_dict={X: X_test, y: y_test})\n",
        "        print('\\n  Train accuracy:', acc_train, 'Test accuracy:', acc_test)\n",
        "\n",
        "\n"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  ---- Epoch  0  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.56 Test accuracy: 0.49\n",
            "\n",
            "  ---- Epoch  1  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.62 Test accuracy: 0.57\n",
            "\n",
            "  ---- Epoch  2  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.62 Test accuracy: 0.54\n",
            "\n",
            "  ---- Epoch  3  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.64 Test accuracy: 0.595\n",
            "\n",
            "  ---- Epoch  4  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.66 Test accuracy: 0.59\n",
            "\n",
            "  ---- Epoch  5  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.69 Test accuracy: 0.585\n",
            "\n",
            "  ---- Epoch  6  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.72 Test accuracy: 0.59\n",
            "\n",
            "  ---- Epoch  7  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.74 Test accuracy: 0.595\n",
            "\n",
            "  ---- Epoch  8  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.76 Test accuracy: 0.595\n",
            "\n",
            "  ---- Epoch  9  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.75 Test accuracy: 0.59\n",
            "\n",
            "  ---- Epoch  10  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.76 Test accuracy: 0.595\n",
            "\n",
            "  ---- Epoch  11  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.76 Test accuracy: 0.6\n",
            "\n",
            "  ---- Epoch  12  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.75 Test accuracy: 0.585\n",
            "\n",
            "  ---- Epoch  13  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.76 Test accuracy: 0.565\n",
            "\n",
            "  ---- Epoch  14  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.79 Test accuracy: 0.565\n",
            "\n",
            "  ---- Epoch  15  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.79 Test accuracy: 0.555\n",
            "\n",
            "  ---- Epoch  16  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.82 Test accuracy: 0.56\n",
            "\n",
            "  ---- Epoch  17  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.83 Test accuracy: 0.57\n",
            "\n",
            "  ---- Epoch  18  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.83 Test accuracy: 0.565\n",
            "\n",
            "  ---- Epoch  19  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.83 Test accuracy: 0.56\n",
            "\n",
            "  ---- Epoch  20  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.83 Test accuracy: 0.55\n",
            "\n",
            "  ---- Epoch  21  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.84 Test accuracy: 0.55\n",
            "\n",
            "  ---- Epoch  22  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.84 Test accuracy: 0.545\n",
            "\n",
            "  ---- Epoch  23  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.84 Test accuracy: 0.535\n",
            "\n",
            "  ---- Epoch  24  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.84 Test accuracy: 0.54\n",
            "\n",
            "  ---- Epoch  25  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.84 Test accuracy: 0.54\n",
            "\n",
            "  ---- Epoch  26  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.84 Test accuracy: 0.545\n",
            "\n",
            "  ---- Epoch  27  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.85 Test accuracy: 0.55\n",
            "\n",
            "  ---- Epoch  28  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.86 Test accuracy: 0.555\n",
            "\n",
            "  ---- Epoch  29  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.89 Test accuracy: 0.545\n",
            "\n",
            "  ---- Epoch  30  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.89 Test accuracy: 0.555\n",
            "\n",
            "  ---- Epoch  31  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.89 Test accuracy: 0.56\n",
            "\n",
            "  ---- Epoch  32  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.91 Test accuracy: 0.55\n",
            "\n",
            "  ---- Epoch  33  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.92 Test accuracy: 0.56\n",
            "\n",
            "  ---- Epoch  34  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.92 Test accuracy: 0.555\n",
            "\n",
            "  ---- Epoch  35  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.91 Test accuracy: 0.55\n",
            "\n",
            "  ---- Epoch  36  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.91 Test accuracy: 0.555\n",
            "\n",
            "  ---- Epoch  37  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.89 Test accuracy: 0.535\n",
            "\n",
            "  ---- Epoch  38  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.88 Test accuracy: 0.54\n",
            "\n",
            "  ---- Epoch  39  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.88 Test accuracy: 0.525\n",
            "\n",
            "  ---- Epoch  40  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.88 Test accuracy: 0.55\n",
            "\n",
            "  ---- Epoch  41  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.85 Test accuracy: 0.565\n",
            "\n",
            "  ---- Epoch  42  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.89 Test accuracy: 0.565\n",
            "\n",
            "  ---- Epoch  43  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.91 Test accuracy: 0.57\n",
            "\n",
            "  ---- Epoch  44  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.93 Test accuracy: 0.57\n",
            "\n",
            "  ---- Epoch  45  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.92 Test accuracy: 0.55\n",
            "\n",
            "  ---- Epoch  46  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.9 Test accuracy: 0.545\n",
            "\n",
            "  ---- Epoch  47  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.9 Test accuracy: 0.535\n",
            "\n",
            "  ---- Epoch  48  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.91 Test accuracy: 0.54\n",
            "\n",
            "  ---- Epoch  49  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.9 Test accuracy: 0.575\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JXBoG7qQO6fs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Fourth Experiment"
      ]
    },
    {
      "metadata": {
        "id": "H_dp9ZHzO9Qm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8jkC68uzO9v8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "c176862a-3487-4710-aa27-14900ef1dd50"
      },
      "cell_type": "code",
      "source": [
        "CHAKIN_INDEX = 13\n",
        "NUMBER_OF_DIMENSIONS = 200\n",
        "SUBFOLDER_NAME = \"glove.6B\"\n",
        "\n",
        "DATA_FOLDER = \"embeddings\"\n",
        "ZIP_FILE = os.path.join(DATA_FOLDER, \"{}.zip\".format(SUBFOLDER_NAME))\n",
        "ZIP_FILE_ALT = \"glove\" + ZIP_FILE[5:]  # sometimes it's lowercase only...\n",
        "UNZIP_FOLDER = os.path.join(DATA_FOLDER, SUBFOLDER_NAME)\n",
        "if SUBFOLDER_NAME[-1] == \"d\":\n",
        "    GLOVE_FILENAME = os.path.join(\n",
        "        UNZIP_FOLDER, \"{}.txt\".format(SUBFOLDER_NAME))\n",
        "else:\n",
        "    GLOVE_FILENAME = os.path.join(UNZIP_FOLDER, \"{}.{}d.txt\".format(\n",
        "        SUBFOLDER_NAME, NUMBER_OF_DIMENSIONS))\n",
        "\n",
        "\n",
        "if not os.path.exists(ZIP_FILE) and not os.path.exists(UNZIP_FOLDER):\n",
        "    # GloVe by Stanford is licensed Apache 2.0:\n",
        "    #     https://github.com/stanfordnlp/GloVe/blob/master/LICENSE\n",
        "    #     http://nlp.stanford.edu/data/glove.twitter.27B.zip\n",
        "    #     Copyright 2014 The Board of Trustees of The Leland Stanford Junior University\n",
        "    print(\"Downloading embeddings to '{}'\".format(ZIP_FILE))\n",
        "    chakin.download(number=CHAKIN_INDEX, save_dir='./{}'.format(DATA_FOLDER))\n",
        "else:\n",
        "    print(\"Embeddings already downloaded.\")\n",
        "\n",
        "if not os.path.exists(UNZIP_FOLDER):\n",
        "    import zipfile\n",
        "    if not os.path.exists(ZIP_FILE) and os.path.exists(ZIP_FILE_ALT):\n",
        "        ZIP_FILE = ZIP_FILE_ALT\n",
        "    with zipfile.ZipFile(ZIP_FILE, \"r\") as zip_ref:\n",
        "        print(\"Extracting embeddings to '{}'\".format(UNZIP_FOLDER))\n",
        "        zip_ref.extractall(UNZIP_FOLDER)\n",
        "else:\n",
        "    print(\"Embeddings already extracted.\")\n",
        "\n",
        "print('\\nRun complete')"
      ],
      "execution_count": 249,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Embeddings already downloaded.\n",
            "Embeddings already extracted.\n",
            "\n",
            "Run complete\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HkKle47VO9wD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SB4HYdpoO9wK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# To make output stable across runs\n",
        "def reset_graph(seed= RANDOM_SEED):\n",
        "    tf.reset_default_graph()\n",
        "    tf.set_random_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "REMOVE_STOPWORDS = False  # no stopword removal \n",
        "\n",
        "EVOCABSIZE = 100000  # specify desired size of pre-defined embedding vocabulary "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7YWB3BCjO9wV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------------- \n",
        "# Select the pre-defined embeddings source        \n",
        "# Define vocabulary size for the language model    \n",
        "# Create a word_to_embedding_dict for GloVe.6B.50d\n",
        "embeddings_directory = 'embeddings/glove.6B'\n",
        "filename = 'glove.6B.200d.txt'\n",
        "embeddings_filename = os.path.join(embeddings_directory, filename)\n",
        "# ------------------------------------------------------------- "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lC7eYbZkO9wa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Utility function for loading embeddings follows methods described in\n",
        "# https://github.com/guillaume-chevalier/GloVe-as-a-TensorFlow-Embedding-Layer\n",
        "# Creates the Python defaultdict dictionary word_to_embedding_dict\n",
        "# for the requested pre-trained word embeddings\n",
        "# \n",
        "# Note the use of defaultdict data structure from the Python Standard Library\n",
        "# collections_defaultdict.py lets the caller specify a default value up front\n",
        "# The default value will be retuned if the key is not a known dictionary key\n",
        "# That is, unknown words are represented by a vector of zeros\n",
        "# For word embeddings, this default value is a vector of zeros\n",
        "# Documentation for the Python standard library:\n",
        "#   Hellmann, D. 2017. The Python 3 Standard Library by Example. Boston: \n",
        "#     Addison-Wesley. [ISBN-13: 978-0-13-429105-5]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dI2aaqsbO9wb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "0c834cdc-8272-4136-8a26-0978376f7ae8"
      },
      "cell_type": "code",
      "source": [
        "# Utility function for loading embeddings follows methods described in\n",
        "# https://github.com/guillaume-chevalier/GloVe-as-a-TensorFlow-Embedding-Layer\n",
        "# Creates the Python defaultdict dictionary word_to_embedding_dict\n",
        "# for the requested pre-trained word embeddings\n",
        "# \n",
        "# Note the use of defaultdict data structure from the Python Standard Library\n",
        "# collections_defaultdict.py lets the caller specify a default value up front\n",
        "# The default value will be retuned if the key is not a known dictionary key\n",
        "# That is, unknown words are represented by a vector of zeros\n",
        "# For word embeddings, this default value is a vector of zeros\n",
        "# Documentation for the Python standard library:\n",
        "#   Hellmann, D. 2017. The Python 3 Standard Library by Example. Boston: \n",
        "#     Addison-Wesley. [ISBN-13: 978-0-13-429105-5]\n",
        "def load_embedding_from_disks(embeddings_filename, with_indexes=True):\n",
        "    \"\"\"\n",
        "    Read a embeddings txt file. If `with_indexes=True`, \n",
        "    we return a tuple of two dictionnaries\n",
        "    `(word_to_index_dict, index_to_embedding_array)`, \n",
        "    otherwise we return only a direct \n",
        "    `word_to_embedding_dict` dictionnary mapping \n",
        "    from a string to a numpy array.\n",
        "    \"\"\"\n",
        "    if with_indexes:\n",
        "        word_to_index_dict = dict()\n",
        "        index_to_embedding_array = []\n",
        "  \n",
        "    else:\n",
        "        word_to_embedding_dict = dict()\n",
        "\n",
        "    with open(embeddings_filename, 'r') as embeddings_file:\n",
        "        for (i, line) in enumerate(embeddings_file):\n",
        "\n",
        "            split = line.split(' ')\n",
        "\n",
        "            word = split[0]\n",
        "\n",
        "            representation = split[1:]\n",
        "            representation = np.array(\n",
        "                [float(val) for val in representation]\n",
        "            )\n",
        "\n",
        "            if with_indexes:\n",
        "                word_to_index_dict[word] = i\n",
        "                index_to_embedding_array.append(representation)\n",
        "            else:\n",
        "                word_to_embedding_dict[word] = representation\n",
        "\n",
        "    # Empty representation for unknown words.\n",
        "    _WORD_NOT_FOUND = [0.0] * len(representation)\n",
        "    if with_indexes:\n",
        "        _LAST_INDEX = i + 1\n",
        "        word_to_index_dict = defaultdict(\n",
        "            lambda: _LAST_INDEX, word_to_index_dict)\n",
        "        index_to_embedding_array = np.array(\n",
        "            index_to_embedding_array + [_WORD_NOT_FOUND])\n",
        "        return word_to_index_dict, index_to_embedding_array\n",
        "    else:\n",
        "        word_to_embedding_dict = defaultdict(lambda: _WORD_NOT_FOUND)\n",
        "        return word_to_embedding_dict\n",
        "\n",
        "print('\\nLoading embeddings from', embeddings_filename)\n",
        "word_to_index, index_to_embedding = \\\n",
        "    load_embedding_from_disks(embeddings_filename, with_indexes=True)\n",
        "print(\"Embedding loaded from disks.\")"
      ],
      "execution_count": 253,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Loading embeddings from embeddings/glove.6B/glove.6B.200d.txt\n",
            "Embedding loaded from disks.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HBztWx62O9wf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "outputId": "3473e102-14f2-435e-b576-11dd6e068723"
      },
      "cell_type": "code",
      "source": [
        "# Note: unknown words have representations with values [0, 0, ..., 0]\n",
        "\n",
        "# Additional background code from\n",
        "# https://github.com/guillaume-chevalier/GloVe-as-a-TensorFlow-Embedding-Layer\n",
        "# shows the general structure of the data structures for word embeddings\n",
        "# This code is modified for our purposes in language modeling \n",
        "vocab_size, embedding_dim = index_to_embedding.shape\n",
        "print(\"Embedding is of shape: {}\".format(index_to_embedding.shape))\n",
        "print(\"This means (number of words, number of dimensions per word)\\n\")\n",
        "print(\"The first words are words that tend occur more often.\")\n",
        "\n",
        "print(\"Note: for unknown words, the representation is an empty vector,\\n\"\n",
        "      \"and the index is the last one. The dictionnary has a limit:\")\n",
        "print(\"    {} --> {} --> {}\".format(\"A word\", \"Index in embedding\", \n",
        "      \"Representation\"))\n",
        "word = \"worsdfkljsdf\"  # a word obviously not in the vocabulary\n",
        "idx = word_to_index[word] # index for word obviously not in the vocabulary\n",
        "complete_vocabulary_size = idx \n",
        "embd = list(np.array(index_to_embedding[idx], dtype=int)) # \"int\" compact print\n",
        "print(\"    {} --> {} --> {}\".format(word, idx, embd))\n",
        "word = \"the\"\n",
        "idx = word_to_index[word]\n",
        "embd = list(index_to_embedding[idx])  # \"int\" for compact print only.\n",
        "print(\"    {} --> {} --> {}\".format(word, idx, embd))"
      ],
      "execution_count": 254,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Embedding is of shape: (400001, 200)\n",
            "This means (number of words, number of dimensions per word)\n",
            "\n",
            "The first words are words that tend occur more often.\n",
            "Note: for unknown words, the representation is an empty vector,\n",
            "and the index is the last one. The dictionnary has a limit:\n",
            "    A word --> Index in embedding --> Representation\n",
            "    worsdfkljsdf --> 400000 --> [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "    the --> 0 --> [-0.071549, 0.093459, 0.023738, -0.090339, 0.056123, 0.32547, -0.39796, -0.092139, 0.061181, -0.1895, 0.13061, 0.14349, 0.011479, 0.38158, 0.5403, -0.14088, 0.24315, 0.23036, -0.55339, 0.048154, 0.45662, 3.2338, 0.020199, 0.049019, -0.014132, 0.076017, -0.11527, 0.2006, -0.077657, 0.24328, 0.16368, -0.34118, -0.06607, 0.10152, 0.038232, -0.17668, -0.88153, -0.33895, -0.035481, -0.55095, -0.016899, -0.43982, 0.039004, 0.40447, -0.2588, 0.64594, 0.26641, 0.28009, -0.024625, 0.63302, -0.317, 0.10271, 0.30886, 0.097792, -0.38227, 0.086552, 0.047075, 0.23511, -0.32127, -0.28538, 0.1667, -0.0049707, -0.62714, -0.24904, 0.29713, 0.14379, -0.12325, -0.058178, -0.001029, -0.082126, 0.36935, -0.00058442, 0.34286, 0.28426, -0.068599, 0.65747, -0.029087, 0.16184, 0.073672, -0.30343, 0.095733, -0.5286, -0.22898, 0.064079, 0.015218, 0.34921, -0.4396, -0.43983, 0.77515, -0.87767, -0.087504, 0.39598, 0.62362, -0.26211, -0.30539, -0.022964, 0.30567, 0.06766, 0.15383, -0.11211, -0.09154, 0.082562, 0.16897, -0.032952, -0.28775, -0.2232, -0.090426, 1.2407, -0.18244, -0.0075219, -0.041388, -0.011083, 0.078186, 0.38511, 0.23334, 0.14414, -0.0009107, -0.26388, -0.20481, 0.10099, 0.14076, 0.28834, -0.045429, 0.37247, 0.13645, -0.67457, 0.22786, 0.12599, 0.029091, 0.030428, -0.13028, 0.19408, 0.49014, -0.39121, -0.075952, 0.074731, 0.18902, -0.16922, -0.26019, -0.039771, -0.24153, 0.10875, 0.30434, 0.036009, 1.4264, 0.12759, -0.073811, -0.20418, 0.0080016, 0.15381, 0.20223, 0.28274, 0.096206, -0.33634, 0.50983, 0.32625, -0.26535, 0.374, -0.30388, -0.40033, -0.04291, -0.067897, -0.29332, 0.10978, -0.045365, 0.23222, -0.31134, -0.28983, -0.66687, 0.53097, 0.19461, 0.3667, 0.26185, -0.65187, 0.10266, 0.11363, -0.12953, -0.68246, -0.18751, 0.1476, 1.0765, -0.22908, -0.0093435, -0.20651, -0.35225, -0.2672, -0.0034307, 0.25906, 0.21759, 0.66158, 0.1218, 0.19957, -0.20303, 0.34474, -0.24328, 0.13139, -0.0088767, 0.33617, 0.030591, 0.25577]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "U1TW0M1_O9wj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5304
        },
        "outputId": "79c37229-fe31-49ec-b3af-c1d1f4bfcbb1"
      },
      "cell_type": "code",
      "source": [
        "# Show how to use embeddings dictionaries with a test sentence\n",
        "# This is a famous typing exercise with all letters of the alphabet\n",
        "# https://en.wikipedia.org/wiki/The_quick_brown_fox_jumps_over_the_lazy_dog\n",
        "a_typing_test_sentence = 'The quick brown fox jumps over the lazy dog'\n",
        "print('\\nTest sentence: ', a_typing_test_sentence, '\\n')\n",
        "words_in_test_sentence = a_typing_test_sentence.split()\n",
        "\n",
        "print('Test sentence embeddings from complete vocabulary of', \n",
        "      complete_vocabulary_size, 'words:\\n')\n",
        "for word in words_in_test_sentence:\n",
        "    word_ = word.lower()\n",
        "    embedding = index_to_embedding[word_to_index[word_]]\n",
        "    print(word_ + \": \", embedding)"
      ],
      "execution_count": 255,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test sentence:  The quick brown fox jumps over the lazy dog \n",
            "\n",
            "Test sentence embeddings from complete vocabulary of 400000 words:\n",
            "\n",
            "the:  [-7.1549e-02  9.3459e-02  2.3738e-02 -9.0339e-02  5.6123e-02  3.2547e-01\n",
            " -3.9796e-01 -9.2139e-02  6.1181e-02 -1.8950e-01  1.3061e-01  1.4349e-01\n",
            "  1.1479e-02  3.8158e-01  5.4030e-01 -1.4088e-01  2.4315e-01  2.3036e-01\n",
            " -5.5339e-01  4.8154e-02  4.5662e-01  3.2338e+00  2.0199e-02  4.9019e-02\n",
            " -1.4132e-02  7.6017e-02 -1.1527e-01  2.0060e-01 -7.7657e-02  2.4328e-01\n",
            "  1.6368e-01 -3.4118e-01 -6.6070e-02  1.0152e-01  3.8232e-02 -1.7668e-01\n",
            " -8.8153e-01 -3.3895e-01 -3.5481e-02 -5.5095e-01 -1.6899e-02 -4.3982e-01\n",
            "  3.9004e-02  4.0447e-01 -2.5880e-01  6.4594e-01  2.6641e-01  2.8009e-01\n",
            " -2.4625e-02  6.3302e-01 -3.1700e-01  1.0271e-01  3.0886e-01  9.7792e-02\n",
            " -3.8227e-01  8.6552e-02  4.7075e-02  2.3511e-01 -3.2127e-01 -2.8538e-01\n",
            "  1.6670e-01 -4.9707e-03 -6.2714e-01 -2.4904e-01  2.9713e-01  1.4379e-01\n",
            " -1.2325e-01 -5.8178e-02 -1.0290e-03 -8.2126e-02  3.6935e-01 -5.8442e-04\n",
            "  3.4286e-01  2.8426e-01 -6.8599e-02  6.5747e-01 -2.9087e-02  1.6184e-01\n",
            "  7.3672e-02 -3.0343e-01  9.5733e-02 -5.2860e-01 -2.2898e-01  6.4079e-02\n",
            "  1.5218e-02  3.4921e-01 -4.3960e-01 -4.3983e-01  7.7515e-01 -8.7767e-01\n",
            " -8.7504e-02  3.9598e-01  6.2362e-01 -2.6211e-01 -3.0539e-01 -2.2964e-02\n",
            "  3.0567e-01  6.7660e-02  1.5383e-01 -1.1211e-01 -9.1540e-02  8.2562e-02\n",
            "  1.6897e-01 -3.2952e-02 -2.8775e-01 -2.2320e-01 -9.0426e-02  1.2407e+00\n",
            " -1.8244e-01 -7.5219e-03 -4.1388e-02 -1.1083e-02  7.8186e-02  3.8511e-01\n",
            "  2.3334e-01  1.4414e-01 -9.1070e-04 -2.6388e-01 -2.0481e-01  1.0099e-01\n",
            "  1.4076e-01  2.8834e-01 -4.5429e-02  3.7247e-01  1.3645e-01 -6.7457e-01\n",
            "  2.2786e-01  1.2599e-01  2.9091e-02  3.0428e-02 -1.3028e-01  1.9408e-01\n",
            "  4.9014e-01 -3.9121e-01 -7.5952e-02  7.4731e-02  1.8902e-01 -1.6922e-01\n",
            " -2.6019e-01 -3.9771e-02 -2.4153e-01  1.0875e-01  3.0434e-01  3.6009e-02\n",
            "  1.4264e+00  1.2759e-01 -7.3811e-02 -2.0418e-01  8.0016e-03  1.5381e-01\n",
            "  2.0223e-01  2.8274e-01  9.6206e-02 -3.3634e-01  5.0983e-01  3.2625e-01\n",
            " -2.6535e-01  3.7400e-01 -3.0388e-01 -4.0033e-01 -4.2910e-02 -6.7897e-02\n",
            " -2.9332e-01  1.0978e-01 -4.5365e-02  2.3222e-01 -3.1134e-01 -2.8983e-01\n",
            " -6.6687e-01  5.3097e-01  1.9461e-01  3.6670e-01  2.6185e-01 -6.5187e-01\n",
            "  1.0266e-01  1.1363e-01 -1.2953e-01 -6.8246e-01 -1.8751e-01  1.4760e-01\n",
            "  1.0765e+00 -2.2908e-01 -9.3435e-03 -2.0651e-01 -3.5225e-01 -2.6720e-01\n",
            " -3.4307e-03  2.5906e-01  2.1759e-01  6.6158e-01  1.2180e-01  1.9957e-01\n",
            " -2.0303e-01  3.4474e-01 -2.4328e-01  1.3139e-01 -8.8767e-03  3.3617e-01\n",
            "  3.0591e-02  2.5577e-01]\n",
            "quick:  [ 0.53142    0.49769   -0.46218   -0.35345   -0.50683   -0.34447\n",
            " -0.81172    0.39306    0.27834    0.62198    0.06481    0.14899\n",
            "  0.43073    0.19718   -0.28265   -0.11391   -0.59028   -0.42922\n",
            " -0.13215   -0.25498   -0.25008    1.7146    -0.1231     0.38283\n",
            "  0.70766    0.16659   -0.32859    0.53923    0.33162   -0.70424\n",
            "  0.0098712  0.24168   -0.078985  -0.2846     0.33886   -0.15721\n",
            " -0.20075   -0.28108   -0.64413    0.26945    0.34069    0.26855\n",
            " -0.084457   0.36366    0.056946   0.025703   0.33665    0.18138\n",
            " -0.17127    0.12221   -0.15343   -0.45456   -0.35414    0.36805\n",
            "  0.35846    0.035429  -0.2158    -0.027401   0.38349   -0.062931\n",
            "  0.38079   -0.48306   -0.35183   -0.36452    0.90971    0.013474\n",
            "  0.17566    0.16205    0.55181    0.38169    0.069562   0.25644\n",
            " -0.10965   -0.41509   -0.147      0.41523   -0.61416    0.26874\n",
            " -0.15854    0.087907   0.50174    0.033713   0.033137  -0.011\n",
            "  0.29725   -0.16901    0.52102   -0.37652    0.33479   -0.73266\n",
            "  0.094927  -0.10657   -0.060679   0.30579    0.10662   -0.44642\n",
            " -0.24021    0.36714    0.26208    0.18919    0.40989   -0.060084\n",
            "  0.60582   -0.27333   -0.39621    0.83985    0.24053    1.1565\n",
            " -0.36838   -0.11829    0.0029027 -0.21962    0.023316  -0.43563\n",
            " -0.090644   0.086079   0.89851   -0.35074   -0.50602    0.51644\n",
            "  0.29897   -0.25667    0.17148    0.21049   -0.59112   -0.035632\n",
            " -0.024478   0.17495    0.57612    0.1432    -0.0088392  0.52941\n",
            "  0.042058  -0.088134   0.23524    0.017935  -0.42959   -0.18663\n",
            "  0.26853   -0.78495    0.12948    0.26742    0.043747   0.59647\n",
            "  1.4267     0.090168  -0.40832   -1.1234    -0.38028    0.27162\n",
            "  0.14629    0.014124   0.43367   -0.10298   -0.05676   -0.2036\n",
            " -0.43859   -0.07543    0.06907   -0.5567     0.012278   0.39384\n",
            " -0.086888   0.49387    0.44799   -0.75701   -0.36998    0.26186\n",
            " -0.20048    0.043936  -0.60191   -0.52284    0.065432   0.27434\n",
            "  0.70803    0.22079   -0.095421  -0.23274   -0.076995  -0.073985\n",
            "  0.65561    0.22114    0.28689    0.079496  -0.63842   -0.0055558\n",
            "  0.028695  -0.16838    0.32392   -0.039101  -0.38377    0.22408\n",
            "  0.057295  -0.48569    0.073566   0.48297   -0.31941    0.36239\n",
            " -0.32581    0.54421  ]\n",
            "brown:  [-3.1442e-01 -4.6750e-01 -8.4407e-01 -1.8710e-01 -1.2632e-01  1.2006e-01\n",
            "  5.6806e-02 -3.4877e-01 -6.7246e-02 -1.0056e-01  2.0937e-01 -2.3444e-01\n",
            " -3.0512e-01  1.2300e-01  6.8152e-01  3.3224e-01 -1.5463e-01 -6.3934e-02\n",
            "  2.9683e-01  2.1666e-01  1.8406e-01  1.6954e+00 -2.8552e-01  3.1491e-01\n",
            "  3.9817e-03 -3.4663e-01 -3.1703e-01 -3.8260e-01 -3.0899e-01 -7.5480e-02\n",
            "  1.9759e-01 -4.5206e-01  2.2378e-02  7.8655e-02  2.3765e-01 -1.2059e-01\n",
            "  3.2246e-01 -6.3736e-01 -3.3414e-01 -4.6758e-01 -5.2968e-01  8.0329e-02\n",
            " -4.2745e-02  4.5676e-01 -3.0410e-02 -4.2255e-02  1.1239e+00 -4.0934e-01\n",
            "  6.2239e-01  1.8442e-01  3.4795e-01  3.9478e-01 -4.1470e-01  3.3069e-01\n",
            "  5.3098e-01 -5.7980e-01 -4.0896e-01 -3.7668e-03  1.6660e-01 -3.7549e-01\n",
            "  5.9203e-01  4.2156e-01 -3.5300e-01 -2.3176e-01 -1.7937e-01 -6.2997e-01\n",
            "  2.4777e-01  1.1116e+00  3.1387e-01  5.1727e-01 -4.7342e-02  7.8297e-02\n",
            " -2.4665e-02  4.7672e-01 -3.4149e-02 -5.8473e-01 -3.0589e-01 -3.0829e-01\n",
            "  1.7088e-02  4.3659e-02  2.5648e-01 -1.7711e-01 -1.8196e-01  6.3430e-02\n",
            "  7.5974e-01  1.9292e-01 -1.3693e-01  4.4574e-01  1.2233e-01 -8.3919e-01\n",
            " -4.2436e-01 -1.0580e-02 -1.5049e-01  7.5005e-01 -8.6058e-02 -4.8509e-01\n",
            " -3.3137e-01 -1.1382e-01  1.0846e-02  3.3227e-01 -1.3328e-01  1.2578e-01\n",
            "  4.8929e-01 -6.9796e-02 -7.2486e-01  2.5884e-01 -2.1520e-01  2.0662e+00\n",
            " -4.7097e-01 -2.5758e-01 -5.4581e-01 -2.4952e-01  8.5905e-02 -1.8132e-01\n",
            "  2.8409e-01 -5.1668e-01 -1.1887e-01  9.5748e-04 -1.9196e-01  3.8302e-03\n",
            "  2.3549e-02  7.0700e-01 -1.4062e-01 -1.6726e-01  4.8694e-01 -6.4805e-01\n",
            "  5.0976e-01  2.7775e-01  5.4095e-01  6.4624e-01 -2.7274e-01 -2.2282e-01\n",
            "  3.0647e-01 -3.3406e-01  4.7047e-01 -5.2202e-01  2.1262e-01 -3.8874e-01\n",
            " -2.4386e-01  3.3994e-01  6.1815e-02 -1.1480e-01  8.0026e-01 -6.2362e-02\n",
            "  1.1721e+00  4.0950e-01  5.1069e-01 -4.6940e-01 -4.5256e-01  4.0319e-01\n",
            "  1.4089e-01  1.5493e-01 -1.0399e-01  2.9251e-01  4.3181e-02 -1.9882e-02\n",
            "  2.2089e-01 -1.6188e-01  2.3468e-02 -3.8417e-01  5.1735e-01  2.1983e-01\n",
            " -2.4505e-01  9.4134e-01 -2.1018e-01  8.2230e-02 -6.3052e-01  9.1849e-02\n",
            "  7.6589e-01  6.2383e-01  5.2695e-01  5.5622e-01 -7.1670e-02  3.5850e-01\n",
            " -5.9761e-01  1.4845e-01 -9.3695e-02 -4.8003e-01  1.6384e-01  2.0724e-01\n",
            "  1.7857e-02 -2.2912e-01 -2.1439e-01  2.9748e-01 -7.7659e-01 -8.4560e-01\n",
            " -3.3909e-01  2.8332e-01  1.1776e-01  1.6187e-01  5.3056e-01  1.0725e-01\n",
            "  2.5875e-01 -2.6135e-01 -2.6731e-01  5.6855e-01 -4.0107e-01  5.1542e-01\n",
            " -1.4011e-01 -2.0160e-01]\n",
            "fox:  [-7.9079e-01  2.5975e-01 -2.2002e-01 -8.2449e-01 -5.5317e-01 -2.2102e-01\n",
            " -1.9796e-01 -3.3495e-02 -5.1390e-01  2.4442e-01  1.4431e-01 -2.7593e-01\n",
            " -9.0063e-02 -6.7059e-01  1.1376e-01  2.1882e-01 -1.7679e-01  1.1152e+00\n",
            "  2.9556e-01  3.8743e-01  7.3205e-01  9.5070e-01 -4.2076e-01  2.6629e-01\n",
            " -1.0438e-01 -1.4570e-01  4.8923e-04 -1.5552e-01 -1.6013e-01 -2.8630e-02\n",
            " -2.8804e-01 -7.8735e-02  4.6452e-01 -7.3855e-01  8.8646e-02 -5.3238e-01\n",
            "  9.8893e-02 -8.1580e-01  1.2033e-01 -3.1299e-01 -3.7875e-01 -3.5234e-01\n",
            "  4.6905e-01  1.3722e-01 -2.5760e-01  4.7174e-02  4.6980e-01 -5.6979e-01\n",
            "  1.0979e-01  4.0741e-02  1.2481e-01 -3.0212e-01  2.6864e-01  4.1942e-01\n",
            "  2.5819e-02  1.6389e-01 -2.5527e-01 -5.5753e-01  4.0756e-01  7.5333e-01\n",
            "  3.7972e-01  2.5484e-02 -4.4053e-01  1.1309e-02  7.3067e-01 -3.0212e-01\n",
            "  1.0309e-01  1.5677e-01  1.1449e+00 -1.7550e-01  2.0491e-01 -4.5462e-01\n",
            " -1.5605e-01  2.3622e-01  3.0375e-01 -1.6485e-01  4.0938e-01  7.0845e-02\n",
            "  2.0811e-01  6.7304e-01  1.1978e-01 -3.1846e-02  3.0240e-01 -7.4044e-01\n",
            " -1.4869e-01  4.9821e-01 -1.3478e+00  5.0864e-02  1.5759e-01 -8.8998e-01\n",
            " -3.4145e-01  4.2453e-01  1.4006e-01  7.3076e-02 -5.8784e-01  4.6956e-01\n",
            " -3.5577e-01 -4.9402e-01  4.0573e-01 -3.6992e-01  5.1449e-01  2.9251e-01\n",
            " -4.9839e-03  3.4426e-02 -6.6246e-01 -3.6776e-01  5.1810e-03  1.2211e+00\n",
            " -4.3950e-01  2.3250e-01  1.4206e-01 -8.7137e-01  2.9829e-01 -4.9394e-01\n",
            " -3.5215e-01 -8.2592e-01 -3.5974e-01 -8.5425e-02 -4.3717e-01 -4.0122e-03\n",
            "  6.2199e-03 -5.6365e-01 -2.0579e-01  1.6423e-01  5.3871e-01 -7.3100e-01\n",
            " -5.0571e-01  1.8094e-01  3.6548e-01  4.0391e-01  9.8332e-02 -8.5674e-01\n",
            " -3.4216e-01  2.3133e-01  2.0996e-01  1.0736e-01  6.4859e-02  3.8116e-01\n",
            " -1.9745e-01 -3.4992e-01 -1.4024e-01 -1.1248e-01  4.0959e-01  3.9222e-02\n",
            "  9.7528e-01  2.8322e-01  6.9825e-02 -7.8264e-01 -3.5349e-01  1.0494e-01\n",
            " -1.3360e-01  6.3501e-01 -2.6349e-01  7.4375e-02  6.5436e-01 -5.5198e-01\n",
            " -2.3475e-02 -1.7402e-01 -3.6973e-01 -3.9919e-01  5.6042e-01 -2.8520e-01\n",
            " -5.5555e-01  9.7751e-02 -2.6530e-03 -4.9564e-01  4.1002e-01 -4.5144e-01\n",
            "  6.1865e-01  6.5570e-01  3.2134e-01 -2.5954e-01 -5.8615e-01  1.8095e-01\n",
            "  5.5883e-01 -2.8088e-01  2.1256e-01 -3.4411e-01 -5.3353e-01  4.3147e-01\n",
            "  8.9076e-01  2.0905e-01 -1.7124e-02  3.5795e-01 -6.2475e-01 -1.6113e-01\n",
            "  1.3008e-01 -3.1728e-01  6.8400e-02  1.1515e-02  4.5719e-01 -4.8818e-01\n",
            " -1.7250e-01 -3.6966e-01 -4.2067e-01  2.6162e-01 -3.1324e-01  5.1430e-01\n",
            "  5.3977e-01  5.3872e-01]\n",
            "jumps:  [ 0.34995    0.095074  -0.64607   -0.25639    0.17252   -0.47998\n",
            " -0.25145   -0.35619   -0.40904   -0.26207   -0.075424  -0.3558\n",
            "  0.36008    0.032778  -0.20451    0.2594    -0.28017   -0.19266\n",
            "  0.7978    -0.061735   0.43611    0.75488    0.71363   -0.16999\n",
            " -0.018755   0.33876   -0.18916    0.054519  -0.70085   -0.50739\n",
            " -0.17996   -0.21199    0.16489   -0.90909    0.72333    0.096124\n",
            " -0.75995   -0.43706   -0.92707    0.15698    0.25441    0.25025\n",
            " -0.28406   -0.37363    0.67765    0.3535    -0.29235   -0.56071\n",
            " -0.58276    0.10647   -0.42709   -0.14311    0.31088    0.26132\n",
            "  0.53098    0.45227   -0.72973   -0.30574   -0.10712    0.20832\n",
            " -0.63229    0.12498    0.42135   -0.094986   0.18732    0.17619\n",
            "  0.10098   -0.077144   0.22384   -0.23426   -0.016561   0.11803\n",
            " -0.3579     0.083167   0.39282    0.20462   -0.10714   -0.2095\n",
            "  0.047883   0.87777    0.46483    0.12565   -0.59199   -0.1897\n",
            " -0.76223   -0.92849    0.14238    0.10795    0.13299   -1.0459\n",
            " -0.40652   -0.0069797 -0.62038    0.68819    0.32092   -0.034481\n",
            "  0.41429    0.080192   0.36452   -0.51149   -0.17595    0.32034\n",
            " -0.33943    0.48514    0.30028   -0.29691    0.19679    0.042799\n",
            "  0.062819   0.32962    0.0024064 -0.55518    0.52484   -0.37855\n",
            " -0.16013    0.18067    0.75056   -0.37793    0.4326     0.25855\n",
            " -0.14586   -0.25838   -0.26509   -0.090255  -0.84919    0.038647\n",
            " -0.56398    0.26634    0.37917    0.15311   -0.23534   -0.1771\n",
            "  0.043473  -0.87943   -0.017739  -0.035083   0.18803   -0.063969\n",
            "  0.060301  -0.54672    0.25768    0.403     -0.18811   -0.34368\n",
            "  0.91118    0.12708    0.10243   -0.2001    -0.050847   0.45843\n",
            " -0.022441  -0.13771    0.31546    0.31103   -0.18422   -0.11485\n",
            "  0.065411   0.046909   0.079907  -0.24623    0.42528   -0.01039\n",
            "  0.38679    0.31717   -0.042185  -0.31751   -0.13434   -0.12508\n",
            " -0.96391    0.033417  -0.48754   -0.3622    -0.45879    0.5039\n",
            "  0.22754   -0.060078  -0.34449   -0.57871    0.036803  -0.15362\n",
            "  0.70166    0.078452   0.15758    1.3976    -0.33147   -0.20967\n",
            "  0.52176    0.3201     0.42299   -0.27026    0.40665   -0.63597\n",
            " -0.65809    0.22565   -0.087924  -0.18587   -0.083251   0.33991\n",
            " -0.28863   -0.30402  ]\n",
            "over:  [-1.8136e-01  5.4384e-02 -1.6638e-02 -1.0872e-01  8.8530e-03  6.2523e-01\n",
            " -5.8821e-02  3.3254e-02  2.9468e-02  2.5949e-01  1.6012e-01  4.5017e-01\n",
            " -2.6535e-02  2.5883e-01  6.2274e-01  3.9724e-01 -4.2582e-01  7.8368e-02\n",
            " -1.7108e-01  9.3726e-02  5.5443e-01  2.9364e+00 -2.9856e-01 -2.6919e-01\n",
            "  2.6883e-01 -4.5052e-02 -2.6976e-01  1.2477e-01  3.2422e-02  2.1593e-01\n",
            "  1.2230e-01 -7.0885e-01 -1.3760e-01  3.8023e-01 -2.8467e-01 -4.2573e-01\n",
            " -1.2020e+00 -4.2784e-01 -1.8816e-01 -1.9955e-01  2.3510e-01 -3.0719e-01\n",
            "  1.1255e-01 -1.1238e-01 -3.0902e-01  2.9093e-01  5.8251e-01 -6.7236e-02\n",
            "  1.1878e-01  4.7467e-01 -9.4490e-03  2.6370e-01  6.7172e-02  8.6137e-01\n",
            "  7.9867e-01 -1.1649e-01 -3.4968e-01  3.5306e-01 -1.6667e-01  9.5608e-02\n",
            " -5.3847e-02  2.6636e-01 -8.5124e-02 -3.4143e-01  5.2658e-01 -2.5312e-01\n",
            "  3.6868e-01  1.4808e-01  5.0457e-01 -5.4672e-01 -5.7193e-03 -2.8087e-01\n",
            "  2.7664e-01 -1.1067e-01 -5.6938e-02  8.4386e-01 -1.5502e-02 -1.4716e-01\n",
            " -2.4319e-01  1.5463e-01 -1.4038e-01 -2.8210e-02  4.0895e-02 -6.9638e-02\n",
            "  7.3984e-03 -2.0785e-02 -1.9026e-01 -3.4531e-01  1.3974e-01 -1.7654e-01\n",
            "  3.6421e-01  3.5446e-02  8.9064e-03  3.2949e-01  7.8715e-02  1.4295e-01\n",
            "  3.4110e-01 -3.0039e-01  1.5895e-01 -4.4791e-01  2.8847e-01 -4.9616e-01\n",
            " -1.2712e-01  5.9769e-01 -1.7956e-01  1.2578e-01  2.2449e-01  1.3406e+00\n",
            "  3.1268e-01 -5.1212e-01 -2.8810e-01 -1.2124e-01 -3.6539e-01  2.3938e-01\n",
            "  6.0236e-01  5.5483e-01 -1.6237e-01 -9.7217e-03 -3.9532e-01 -8.0846e-02\n",
            " -2.0053e-01  4.3234e-01  3.7900e-01  2.1911e-01  5.0871e-01 -9.6578e-01\n",
            "  4.0803e-01 -8.7810e-02  4.1018e-01  3.3282e-01 -2.6322e-01 -3.1882e-01\n",
            " -4.6560e-01 -4.9194e-01  1.3927e-01  6.7400e-01  3.7538e-01 -4.7802e-01\n",
            " -9.5047e-02  4.3122e-01 -3.3629e-01 -2.8367e-01  2.4260e-01 -1.6096e-01\n",
            "  1.3671e+00  2.6446e-01  1.8355e-01 -2.1050e-01  2.1915e-03  3.2011e-01\n",
            " -2.2010e-01  2.1907e-01 -1.6236e-01 -1.5451e-01  1.8775e-01 -1.5687e-01\n",
            " -1.1443e-01  7.4750e-02 -8.2473e-03 -4.4988e-01 -1.2534e-01  1.0274e-03\n",
            " -2.6980e-01  3.8504e-01  1.2015e-01  1.8465e-01 -5.7222e-01  2.6703e-01\n",
            " -3.6049e-02  1.0055e+00 -1.2874e-01  3.1265e-02  1.3461e-01  1.4928e-01\n",
            " -2.8600e-01  2.3514e-01 -2.9374e-01 -2.9196e-01 -2.9380e-01 -3.0427e-01\n",
            "  1.2025e+00  5.6008e-02  8.0408e-02  2.3824e-01  4.0132e-02  5.8081e-02\n",
            " -1.9350e-01  3.4714e-01  1.3727e-02 -1.3661e-02 -2.9019e-01  1.1244e-01\n",
            " -2.6544e-01  9.0986e-02  9.8320e-02 -2.1316e-01  6.3920e-01  1.6703e-01\n",
            " -3.7282e-01  9.7439e-02]\n",
            "the:  [-7.1549e-02  9.3459e-02  2.3738e-02 -9.0339e-02  5.6123e-02  3.2547e-01\n",
            " -3.9796e-01 -9.2139e-02  6.1181e-02 -1.8950e-01  1.3061e-01  1.4349e-01\n",
            "  1.1479e-02  3.8158e-01  5.4030e-01 -1.4088e-01  2.4315e-01  2.3036e-01\n",
            " -5.5339e-01  4.8154e-02  4.5662e-01  3.2338e+00  2.0199e-02  4.9019e-02\n",
            " -1.4132e-02  7.6017e-02 -1.1527e-01  2.0060e-01 -7.7657e-02  2.4328e-01\n",
            "  1.6368e-01 -3.4118e-01 -6.6070e-02  1.0152e-01  3.8232e-02 -1.7668e-01\n",
            " -8.8153e-01 -3.3895e-01 -3.5481e-02 -5.5095e-01 -1.6899e-02 -4.3982e-01\n",
            "  3.9004e-02  4.0447e-01 -2.5880e-01  6.4594e-01  2.6641e-01  2.8009e-01\n",
            " -2.4625e-02  6.3302e-01 -3.1700e-01  1.0271e-01  3.0886e-01  9.7792e-02\n",
            " -3.8227e-01  8.6552e-02  4.7075e-02  2.3511e-01 -3.2127e-01 -2.8538e-01\n",
            "  1.6670e-01 -4.9707e-03 -6.2714e-01 -2.4904e-01  2.9713e-01  1.4379e-01\n",
            " -1.2325e-01 -5.8178e-02 -1.0290e-03 -8.2126e-02  3.6935e-01 -5.8442e-04\n",
            "  3.4286e-01  2.8426e-01 -6.8599e-02  6.5747e-01 -2.9087e-02  1.6184e-01\n",
            "  7.3672e-02 -3.0343e-01  9.5733e-02 -5.2860e-01 -2.2898e-01  6.4079e-02\n",
            "  1.5218e-02  3.4921e-01 -4.3960e-01 -4.3983e-01  7.7515e-01 -8.7767e-01\n",
            " -8.7504e-02  3.9598e-01  6.2362e-01 -2.6211e-01 -3.0539e-01 -2.2964e-02\n",
            "  3.0567e-01  6.7660e-02  1.5383e-01 -1.1211e-01 -9.1540e-02  8.2562e-02\n",
            "  1.6897e-01 -3.2952e-02 -2.8775e-01 -2.2320e-01 -9.0426e-02  1.2407e+00\n",
            " -1.8244e-01 -7.5219e-03 -4.1388e-02 -1.1083e-02  7.8186e-02  3.8511e-01\n",
            "  2.3334e-01  1.4414e-01 -9.1070e-04 -2.6388e-01 -2.0481e-01  1.0099e-01\n",
            "  1.4076e-01  2.8834e-01 -4.5429e-02  3.7247e-01  1.3645e-01 -6.7457e-01\n",
            "  2.2786e-01  1.2599e-01  2.9091e-02  3.0428e-02 -1.3028e-01  1.9408e-01\n",
            "  4.9014e-01 -3.9121e-01 -7.5952e-02  7.4731e-02  1.8902e-01 -1.6922e-01\n",
            " -2.6019e-01 -3.9771e-02 -2.4153e-01  1.0875e-01  3.0434e-01  3.6009e-02\n",
            "  1.4264e+00  1.2759e-01 -7.3811e-02 -2.0418e-01  8.0016e-03  1.5381e-01\n",
            "  2.0223e-01  2.8274e-01  9.6206e-02 -3.3634e-01  5.0983e-01  3.2625e-01\n",
            " -2.6535e-01  3.7400e-01 -3.0388e-01 -4.0033e-01 -4.2910e-02 -6.7897e-02\n",
            " -2.9332e-01  1.0978e-01 -4.5365e-02  2.3222e-01 -3.1134e-01 -2.8983e-01\n",
            " -6.6687e-01  5.3097e-01  1.9461e-01  3.6670e-01  2.6185e-01 -6.5187e-01\n",
            "  1.0266e-01  1.1363e-01 -1.2953e-01 -6.8246e-01 -1.8751e-01  1.4760e-01\n",
            "  1.0765e+00 -2.2908e-01 -9.3435e-03 -2.0651e-01 -3.5225e-01 -2.6720e-01\n",
            " -3.4307e-03  2.5906e-01  2.1759e-01  6.6158e-01  1.2180e-01  1.9957e-01\n",
            " -2.0303e-01  3.4474e-01 -2.4328e-01  1.3139e-01 -8.8767e-03  3.3617e-01\n",
            "  3.0591e-02  2.5577e-01]\n",
            "lazy:  [ 2.4360e-01 -3.4947e-01 -3.6866e-01 -1.2049e-01  1.5666e-01 -2.8681e-01\n",
            " -2.7496e-01 -1.7186e-01  4.1094e-02  2.7805e-01  4.7063e-01 -7.6601e-01\n",
            "  9.3984e-01  3.1374e-01 -1.1558e-01  3.7066e-01 -7.3339e-01 -1.4001e-01\n",
            "  3.8372e-01 -1.1413e-01  2.2680e-01  1.8525e-01 -4.8286e-01  6.8405e-01\n",
            "  4.2411e-01 -5.3271e-01  2.1549e-01 -5.1370e-01 -2.0095e-01  3.6552e-01\n",
            "  7.3175e-02  4.7971e-01 -7.4379e-01  2.5743e-02  5.1195e-01 -9.6689e-02\n",
            " -3.1828e-01 -5.0514e-01 -7.7848e-02 -5.9908e-02  4.1613e-01  5.6312e-01\n",
            "  6.2281e-01 -1.1399e-01  5.7956e-01  5.4217e-01 -1.5364e-02 -1.8366e-01\n",
            " -9.4557e-02  9.7040e-01 -6.9829e-01 -2.8877e-01 -1.0291e-01  1.6857e-01\n",
            "  3.9540e-01 -2.6136e-01 -2.0364e-01 -4.3896e-01  1.5321e-01  2.0340e-01\n",
            "  2.3708e-01 -4.4019e-01 -2.5953e-01 -2.9465e-01 -7.0392e-01  4.5277e-01\n",
            "  4.8609e-01  2.4371e-01  1.0621e+00 -1.1893e-02  1.6272e-01  4.7658e-02\n",
            " -6.0591e-01  1.7743e-01 -2.3131e-01 -5.2727e-01 -4.4549e-01  2.2854e-01\n",
            "  4.5121e-02  1.9055e-01  4.8879e-02 -1.6769e-01 -1.5795e-01 -7.7439e-01\n",
            "  4.8220e-01 -2.1924e-01  1.5959e-01 -2.3288e-01 -2.1956e-01 -5.6318e-01\n",
            " -3.0414e-01 -4.9283e-01 -1.0453e-01  5.2286e-01  1.6010e-01  1.5194e-01\n",
            "  1.4254e-01 -2.3295e-01  4.2537e-01  5.8918e-01 -3.2303e-03 -2.0670e-01\n",
            "  1.1983e-01 -1.1128e-01  2.1679e-01  2.0885e-02  2.3287e-02  5.7264e-04\n",
            " -3.7682e-01  3.8070e-01  1.7784e-01 -3.3404e-01  4.4448e-01 -2.1758e-01\n",
            " -5.8585e-02  2.9275e-01  6.3896e-01  4.5773e-02 -5.1163e-01  2.3018e-01\n",
            "  3.5605e-02 -3.1697e-01 -1.7654e-02 -7.9892e-01  1.0807e-01  6.0575e-01\n",
            "  6.1157e-01  2.0286e-01  5.6747e-01 -3.7504e-02  8.8656e-02 -2.2181e-01\n",
            " -8.2162e-02  2.4194e-01  9.2096e-02 -5.4754e-01 -4.4629e-01  6.1179e-01\n",
            "  2.5916e-01 -7.8731e-01  8.8334e-02 -5.4586e-01 -1.6790e-01 -2.9476e-01\n",
            "  1.2879e+00 -3.0823e-02 -2.1768e-01 -4.3510e-01  2.8531e-01  1.6418e-02\n",
            "  4.0111e-01  1.0992e+00 -2.2871e-01 -9.6040e-02  2.4002e-01 -2.2189e-03\n",
            "  2.2825e-01 -8.8586e-02 -6.9752e-01  4.5946e-02  3.6988e-01 -1.2353e-01\n",
            " -8.9403e-02  8.0708e-01  4.2137e-02 -6.1002e-01 -2.7454e-01  7.8604e-01\n",
            " -1.8031e-01 -1.2763e-01 -5.1040e-01  2.1468e-01 -1.2239e-01  4.3400e-01\n",
            "  4.1641e-01 -4.0750e-01 -4.5797e-01 -3.3990e-02  7.3162e-02  7.9555e-01\n",
            "  3.5424e-01  3.9136e-01 -7.2358e-01  4.7071e-01 -7.6863e-01  5.8818e-01\n",
            "  5.5482e-01 -1.5730e-01  1.0282e-01 -1.1142e-01 -8.3167e-02 -1.1425e+00\n",
            "  4.8521e-02 -3.0276e-03  3.6737e-01  1.9888e-01  4.6231e-02  4.9800e-01\n",
            " -9.2291e-02 -2.4074e-02]\n",
            "dog:  [-1.3791e-01 -4.7601e-01 -5.6369e-02 -3.9082e-01 -1.7544e-01 -6.2244e-01\n",
            " -3.9816e-01  2.9620e-01 -6.0647e-02 -6.7017e-02  1.1466e-01 -3.3015e-01\n",
            " -2.0318e-02  6.0616e-01 -1.3920e-01  1.3896e-01 -5.4781e-01  3.0864e-01\n",
            "  1.7354e-01  3.9927e-01  2.1137e-01  1.3004e+00  8.8030e-01  2.3946e-01\n",
            "  2.8838e-01 -4.6336e-01  2.5745e-01 -3.1755e-01 -3.2877e-01 -5.9534e-01\n",
            "  2.3983e-01  3.4159e-01  1.2754e-01 -8.8208e-01  1.4258e-01 -1.8857e-01\n",
            " -1.6961e-01  2.7808e-01 -2.4600e-01  1.9122e-01  5.0244e-01  5.3660e-01\n",
            " -5.3568e-01  2.4827e-01  3.2561e-01  6.7882e-01  9.6401e-01 -2.8892e-01\n",
            "  5.1206e-01  5.8496e-01 -3.1934e-02 -2.4849e-02  8.8564e-02  1.7360e-01\n",
            "  5.4166e-01 -8.6743e-02 -3.8412e-01  1.3974e-01 -7.4122e-03  9.2210e-01\n",
            " -2.5799e-01 -4.7018e-01 -5.5742e-01 -2.1213e-02 -7.1072e-01  8.0995e-02\n",
            " -4.7254e-01 -3.2925e-01  6.8052e-01  1.7242e-01  8.7783e-02 -2.6560e-01\n",
            " -6.0070e-01 -8.5217e-02 -3.6977e-02 -3.6593e-01 -6.2576e-01 -3.4162e-01\n",
            "  5.4672e-02 -1.1734e-01  1.9686e-01  8.3758e-02  4.3157e-01 -8.2195e-01\n",
            " -5.7756e-01  6.7821e-02 -4.9520e-01  1.4769e-01  3.2863e-01 -1.0649e+00\n",
            " -3.9756e-01 -3.4890e-01 -6.1548e-02  7.5400e-01  5.2457e-01  1.3657e-01\n",
            " -4.1904e-02 -4.4660e-01  8.4754e-02  3.7516e-01 -6.2374e-02 -8.1762e-02\n",
            " -4.1776e-01  3.0157e-02 -7.7967e-01  8.7627e-02  9.0542e-02  7.5266e-01\n",
            "  2.9235e-02 -1.8324e-01  5.5433e-01 -3.4632e-01 -7.8019e-02 -1.2078e-01\n",
            " -6.8377e-01  2.8826e-02 -4.1618e-01  2.2341e-01 -8.0811e-01 -5.9707e-01\n",
            "  4.6835e-01 -3.8246e-01 -2.3549e-01 -6.2565e-01  6.1201e-01  1.4221e-01\n",
            "  2.4076e-02  4.2106e-01  5.1978e-01  2.7811e-01  1.6885e-01 -4.9293e-01\n",
            "  3.1563e-01 -5.9663e-01  1.5091e-01 -6.3300e-01  1.1324e-03 -6.2031e-02\n",
            " -3.8694e-02 -2.6038e-01  2.1907e-01  2.3103e-01  8.2427e-01  1.4963e-01\n",
            "  8.5767e-01  1.5706e-01 -2.9116e-01 -4.2033e-01  4.5080e-01  3.9614e-01\n",
            " -2.0271e-01  1.0702e+00 -4.1153e-01  2.2282e-01  1.3287e-01  9.3896e-01\n",
            "  1.6088e-01 -2.2247e-01 -1.1443e+00 -5.0556e-01 -1.9619e-01  3.4685e-01\n",
            "  5.9883e-01  2.7666e-02 -2.1223e-01 -5.4970e-01 -1.6784e-01  5.2375e-01\n",
            " -9.8196e-02  1.5559e-01 -2.3997e-01 -1.1526e-01  1.6577e-02  6.1643e-02\n",
            "  9.2234e-02  2.5817e-02  2.9163e-01 -5.0848e-01 -1.5164e-01  1.8360e-01\n",
            "  1.6434e+00 -3.1567e-01 -5.3165e-01 -4.4914e-01 -7.2425e-01 -4.1122e-01\n",
            "  1.8799e-01  1.6130e-01  7.4169e-01 -2.1597e-02 -3.7775e-01 -5.5265e-01\n",
            "  7.3535e-02  8.5621e-02 -6.6452e-02  5.4982e-02 -4.0218e-01  4.3235e-01\n",
            " -7.5727e-02  7.0530e-02]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5FQP_HC2O9wm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------------- \n",
        "# Define vocabulary size for the language model    \n",
        "# To reduce the size of the vocabulary to the n most frequently used words\n",
        "\n",
        "def default_factory():\n",
        "    return EVOCABSIZE  # last/unknown-word row in limited_index_to_embedding\n",
        "# dictionary has the items() function, returns list of (key, value) tuples\n",
        "limited_word_to_index = defaultdict(default_factory, \\\n",
        "    {k: v for k, v in word_to_index.items() if v < EVOCABSIZE})\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iXEhEciFO9wn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Select the first EVOCABSIZE rows to the index_to_embedding\n",
        "limited_index_to_embedding = index_to_embedding[0:EVOCABSIZE,:]\n",
        "# Set the unknown-word row to be all zeros as previously\n",
        "limited_index_to_embedding = np.append(limited_index_to_embedding, \n",
        "    index_to_embedding[index_to_embedding.shape[0] - 1, :].\\\n",
        "        reshape(1,embedding_dim), \n",
        "    axis = 0)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kvVW3ZuaO9wq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Delete large numpy array to clear some CPU RAM\n",
        "del index_to_embedding"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KSyun8w4O9wu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5270
        },
        "outputId": "0b9d3f7d-b4d5-4c59-bac8-1d0fd9ee4bba"
      },
      "cell_type": "code",
      "source": [
        "# Verify the new vocabulary: should get same embeddings for test sentence\n",
        "# Note that a small EVOCABSIZE may yield some zero vectors for embeddings\n",
        "print('\\nTest sentence embeddings from vocabulary of', EVOCABSIZE, 'words:\\n')\n",
        "for word in words_in_test_sentence:\n",
        "    word_ = word.lower()\n",
        "    embedding = limited_index_to_embedding[limited_word_to_index[word_]]\n",
        "    print(word_ + \": \", embedding)\n",
        "\n"
      ],
      "execution_count": 259,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test sentence embeddings from vocabulary of 100000 words:\n",
            "\n",
            "the:  [-7.1549e-02  9.3459e-02  2.3738e-02 -9.0339e-02  5.6123e-02  3.2547e-01\n",
            " -3.9796e-01 -9.2139e-02  6.1181e-02 -1.8950e-01  1.3061e-01  1.4349e-01\n",
            "  1.1479e-02  3.8158e-01  5.4030e-01 -1.4088e-01  2.4315e-01  2.3036e-01\n",
            " -5.5339e-01  4.8154e-02  4.5662e-01  3.2338e+00  2.0199e-02  4.9019e-02\n",
            " -1.4132e-02  7.6017e-02 -1.1527e-01  2.0060e-01 -7.7657e-02  2.4328e-01\n",
            "  1.6368e-01 -3.4118e-01 -6.6070e-02  1.0152e-01  3.8232e-02 -1.7668e-01\n",
            " -8.8153e-01 -3.3895e-01 -3.5481e-02 -5.5095e-01 -1.6899e-02 -4.3982e-01\n",
            "  3.9004e-02  4.0447e-01 -2.5880e-01  6.4594e-01  2.6641e-01  2.8009e-01\n",
            " -2.4625e-02  6.3302e-01 -3.1700e-01  1.0271e-01  3.0886e-01  9.7792e-02\n",
            " -3.8227e-01  8.6552e-02  4.7075e-02  2.3511e-01 -3.2127e-01 -2.8538e-01\n",
            "  1.6670e-01 -4.9707e-03 -6.2714e-01 -2.4904e-01  2.9713e-01  1.4379e-01\n",
            " -1.2325e-01 -5.8178e-02 -1.0290e-03 -8.2126e-02  3.6935e-01 -5.8442e-04\n",
            "  3.4286e-01  2.8426e-01 -6.8599e-02  6.5747e-01 -2.9087e-02  1.6184e-01\n",
            "  7.3672e-02 -3.0343e-01  9.5733e-02 -5.2860e-01 -2.2898e-01  6.4079e-02\n",
            "  1.5218e-02  3.4921e-01 -4.3960e-01 -4.3983e-01  7.7515e-01 -8.7767e-01\n",
            " -8.7504e-02  3.9598e-01  6.2362e-01 -2.6211e-01 -3.0539e-01 -2.2964e-02\n",
            "  3.0567e-01  6.7660e-02  1.5383e-01 -1.1211e-01 -9.1540e-02  8.2562e-02\n",
            "  1.6897e-01 -3.2952e-02 -2.8775e-01 -2.2320e-01 -9.0426e-02  1.2407e+00\n",
            " -1.8244e-01 -7.5219e-03 -4.1388e-02 -1.1083e-02  7.8186e-02  3.8511e-01\n",
            "  2.3334e-01  1.4414e-01 -9.1070e-04 -2.6388e-01 -2.0481e-01  1.0099e-01\n",
            "  1.4076e-01  2.8834e-01 -4.5429e-02  3.7247e-01  1.3645e-01 -6.7457e-01\n",
            "  2.2786e-01  1.2599e-01  2.9091e-02  3.0428e-02 -1.3028e-01  1.9408e-01\n",
            "  4.9014e-01 -3.9121e-01 -7.5952e-02  7.4731e-02  1.8902e-01 -1.6922e-01\n",
            " -2.6019e-01 -3.9771e-02 -2.4153e-01  1.0875e-01  3.0434e-01  3.6009e-02\n",
            "  1.4264e+00  1.2759e-01 -7.3811e-02 -2.0418e-01  8.0016e-03  1.5381e-01\n",
            "  2.0223e-01  2.8274e-01  9.6206e-02 -3.3634e-01  5.0983e-01  3.2625e-01\n",
            " -2.6535e-01  3.7400e-01 -3.0388e-01 -4.0033e-01 -4.2910e-02 -6.7897e-02\n",
            " -2.9332e-01  1.0978e-01 -4.5365e-02  2.3222e-01 -3.1134e-01 -2.8983e-01\n",
            " -6.6687e-01  5.3097e-01  1.9461e-01  3.6670e-01  2.6185e-01 -6.5187e-01\n",
            "  1.0266e-01  1.1363e-01 -1.2953e-01 -6.8246e-01 -1.8751e-01  1.4760e-01\n",
            "  1.0765e+00 -2.2908e-01 -9.3435e-03 -2.0651e-01 -3.5225e-01 -2.6720e-01\n",
            " -3.4307e-03  2.5906e-01  2.1759e-01  6.6158e-01  1.2180e-01  1.9957e-01\n",
            " -2.0303e-01  3.4474e-01 -2.4328e-01  1.3139e-01 -8.8767e-03  3.3617e-01\n",
            "  3.0591e-02  2.5577e-01]\n",
            "quick:  [ 0.53142    0.49769   -0.46218   -0.35345   -0.50683   -0.34447\n",
            " -0.81172    0.39306    0.27834    0.62198    0.06481    0.14899\n",
            "  0.43073    0.19718   -0.28265   -0.11391   -0.59028   -0.42922\n",
            " -0.13215   -0.25498   -0.25008    1.7146    -0.1231     0.38283\n",
            "  0.70766    0.16659   -0.32859    0.53923    0.33162   -0.70424\n",
            "  0.0098712  0.24168   -0.078985  -0.2846     0.33886   -0.15721\n",
            " -0.20075   -0.28108   -0.64413    0.26945    0.34069    0.26855\n",
            " -0.084457   0.36366    0.056946   0.025703   0.33665    0.18138\n",
            " -0.17127    0.12221   -0.15343   -0.45456   -0.35414    0.36805\n",
            "  0.35846    0.035429  -0.2158    -0.027401   0.38349   -0.062931\n",
            "  0.38079   -0.48306   -0.35183   -0.36452    0.90971    0.013474\n",
            "  0.17566    0.16205    0.55181    0.38169    0.069562   0.25644\n",
            " -0.10965   -0.41509   -0.147      0.41523   -0.61416    0.26874\n",
            " -0.15854    0.087907   0.50174    0.033713   0.033137  -0.011\n",
            "  0.29725   -0.16901    0.52102   -0.37652    0.33479   -0.73266\n",
            "  0.094927  -0.10657   -0.060679   0.30579    0.10662   -0.44642\n",
            " -0.24021    0.36714    0.26208    0.18919    0.40989   -0.060084\n",
            "  0.60582   -0.27333   -0.39621    0.83985    0.24053    1.1565\n",
            " -0.36838   -0.11829    0.0029027 -0.21962    0.023316  -0.43563\n",
            " -0.090644   0.086079   0.89851   -0.35074   -0.50602    0.51644\n",
            "  0.29897   -0.25667    0.17148    0.21049   -0.59112   -0.035632\n",
            " -0.024478   0.17495    0.57612    0.1432    -0.0088392  0.52941\n",
            "  0.042058  -0.088134   0.23524    0.017935  -0.42959   -0.18663\n",
            "  0.26853   -0.78495    0.12948    0.26742    0.043747   0.59647\n",
            "  1.4267     0.090168  -0.40832   -1.1234    -0.38028    0.27162\n",
            "  0.14629    0.014124   0.43367   -0.10298   -0.05676   -0.2036\n",
            " -0.43859   -0.07543    0.06907   -0.5567     0.012278   0.39384\n",
            " -0.086888   0.49387    0.44799   -0.75701   -0.36998    0.26186\n",
            " -0.20048    0.043936  -0.60191   -0.52284    0.065432   0.27434\n",
            "  0.70803    0.22079   -0.095421  -0.23274   -0.076995  -0.073985\n",
            "  0.65561    0.22114    0.28689    0.079496  -0.63842   -0.0055558\n",
            "  0.028695  -0.16838    0.32392   -0.039101  -0.38377    0.22408\n",
            "  0.057295  -0.48569    0.073566   0.48297   -0.31941    0.36239\n",
            " -0.32581    0.54421  ]\n",
            "brown:  [-3.1442e-01 -4.6750e-01 -8.4407e-01 -1.8710e-01 -1.2632e-01  1.2006e-01\n",
            "  5.6806e-02 -3.4877e-01 -6.7246e-02 -1.0056e-01  2.0937e-01 -2.3444e-01\n",
            " -3.0512e-01  1.2300e-01  6.8152e-01  3.3224e-01 -1.5463e-01 -6.3934e-02\n",
            "  2.9683e-01  2.1666e-01  1.8406e-01  1.6954e+00 -2.8552e-01  3.1491e-01\n",
            "  3.9817e-03 -3.4663e-01 -3.1703e-01 -3.8260e-01 -3.0899e-01 -7.5480e-02\n",
            "  1.9759e-01 -4.5206e-01  2.2378e-02  7.8655e-02  2.3765e-01 -1.2059e-01\n",
            "  3.2246e-01 -6.3736e-01 -3.3414e-01 -4.6758e-01 -5.2968e-01  8.0329e-02\n",
            " -4.2745e-02  4.5676e-01 -3.0410e-02 -4.2255e-02  1.1239e+00 -4.0934e-01\n",
            "  6.2239e-01  1.8442e-01  3.4795e-01  3.9478e-01 -4.1470e-01  3.3069e-01\n",
            "  5.3098e-01 -5.7980e-01 -4.0896e-01 -3.7668e-03  1.6660e-01 -3.7549e-01\n",
            "  5.9203e-01  4.2156e-01 -3.5300e-01 -2.3176e-01 -1.7937e-01 -6.2997e-01\n",
            "  2.4777e-01  1.1116e+00  3.1387e-01  5.1727e-01 -4.7342e-02  7.8297e-02\n",
            " -2.4665e-02  4.7672e-01 -3.4149e-02 -5.8473e-01 -3.0589e-01 -3.0829e-01\n",
            "  1.7088e-02  4.3659e-02  2.5648e-01 -1.7711e-01 -1.8196e-01  6.3430e-02\n",
            "  7.5974e-01  1.9292e-01 -1.3693e-01  4.4574e-01  1.2233e-01 -8.3919e-01\n",
            " -4.2436e-01 -1.0580e-02 -1.5049e-01  7.5005e-01 -8.6058e-02 -4.8509e-01\n",
            " -3.3137e-01 -1.1382e-01  1.0846e-02  3.3227e-01 -1.3328e-01  1.2578e-01\n",
            "  4.8929e-01 -6.9796e-02 -7.2486e-01  2.5884e-01 -2.1520e-01  2.0662e+00\n",
            " -4.7097e-01 -2.5758e-01 -5.4581e-01 -2.4952e-01  8.5905e-02 -1.8132e-01\n",
            "  2.8409e-01 -5.1668e-01 -1.1887e-01  9.5748e-04 -1.9196e-01  3.8302e-03\n",
            "  2.3549e-02  7.0700e-01 -1.4062e-01 -1.6726e-01  4.8694e-01 -6.4805e-01\n",
            "  5.0976e-01  2.7775e-01  5.4095e-01  6.4624e-01 -2.7274e-01 -2.2282e-01\n",
            "  3.0647e-01 -3.3406e-01  4.7047e-01 -5.2202e-01  2.1262e-01 -3.8874e-01\n",
            " -2.4386e-01  3.3994e-01  6.1815e-02 -1.1480e-01  8.0026e-01 -6.2362e-02\n",
            "  1.1721e+00  4.0950e-01  5.1069e-01 -4.6940e-01 -4.5256e-01  4.0319e-01\n",
            "  1.4089e-01  1.5493e-01 -1.0399e-01  2.9251e-01  4.3181e-02 -1.9882e-02\n",
            "  2.2089e-01 -1.6188e-01  2.3468e-02 -3.8417e-01  5.1735e-01  2.1983e-01\n",
            " -2.4505e-01  9.4134e-01 -2.1018e-01  8.2230e-02 -6.3052e-01  9.1849e-02\n",
            "  7.6589e-01  6.2383e-01  5.2695e-01  5.5622e-01 -7.1670e-02  3.5850e-01\n",
            " -5.9761e-01  1.4845e-01 -9.3695e-02 -4.8003e-01  1.6384e-01  2.0724e-01\n",
            "  1.7857e-02 -2.2912e-01 -2.1439e-01  2.9748e-01 -7.7659e-01 -8.4560e-01\n",
            " -3.3909e-01  2.8332e-01  1.1776e-01  1.6187e-01  5.3056e-01  1.0725e-01\n",
            "  2.5875e-01 -2.6135e-01 -2.6731e-01  5.6855e-01 -4.0107e-01  5.1542e-01\n",
            " -1.4011e-01 -2.0160e-01]\n",
            "fox:  [-7.9079e-01  2.5975e-01 -2.2002e-01 -8.2449e-01 -5.5317e-01 -2.2102e-01\n",
            " -1.9796e-01 -3.3495e-02 -5.1390e-01  2.4442e-01  1.4431e-01 -2.7593e-01\n",
            " -9.0063e-02 -6.7059e-01  1.1376e-01  2.1882e-01 -1.7679e-01  1.1152e+00\n",
            "  2.9556e-01  3.8743e-01  7.3205e-01  9.5070e-01 -4.2076e-01  2.6629e-01\n",
            " -1.0438e-01 -1.4570e-01  4.8923e-04 -1.5552e-01 -1.6013e-01 -2.8630e-02\n",
            " -2.8804e-01 -7.8735e-02  4.6452e-01 -7.3855e-01  8.8646e-02 -5.3238e-01\n",
            "  9.8893e-02 -8.1580e-01  1.2033e-01 -3.1299e-01 -3.7875e-01 -3.5234e-01\n",
            "  4.6905e-01  1.3722e-01 -2.5760e-01  4.7174e-02  4.6980e-01 -5.6979e-01\n",
            "  1.0979e-01  4.0741e-02  1.2481e-01 -3.0212e-01  2.6864e-01  4.1942e-01\n",
            "  2.5819e-02  1.6389e-01 -2.5527e-01 -5.5753e-01  4.0756e-01  7.5333e-01\n",
            "  3.7972e-01  2.5484e-02 -4.4053e-01  1.1309e-02  7.3067e-01 -3.0212e-01\n",
            "  1.0309e-01  1.5677e-01  1.1449e+00 -1.7550e-01  2.0491e-01 -4.5462e-01\n",
            " -1.5605e-01  2.3622e-01  3.0375e-01 -1.6485e-01  4.0938e-01  7.0845e-02\n",
            "  2.0811e-01  6.7304e-01  1.1978e-01 -3.1846e-02  3.0240e-01 -7.4044e-01\n",
            " -1.4869e-01  4.9821e-01 -1.3478e+00  5.0864e-02  1.5759e-01 -8.8998e-01\n",
            " -3.4145e-01  4.2453e-01  1.4006e-01  7.3076e-02 -5.8784e-01  4.6956e-01\n",
            " -3.5577e-01 -4.9402e-01  4.0573e-01 -3.6992e-01  5.1449e-01  2.9251e-01\n",
            " -4.9839e-03  3.4426e-02 -6.6246e-01 -3.6776e-01  5.1810e-03  1.2211e+00\n",
            " -4.3950e-01  2.3250e-01  1.4206e-01 -8.7137e-01  2.9829e-01 -4.9394e-01\n",
            " -3.5215e-01 -8.2592e-01 -3.5974e-01 -8.5425e-02 -4.3717e-01 -4.0122e-03\n",
            "  6.2199e-03 -5.6365e-01 -2.0579e-01  1.6423e-01  5.3871e-01 -7.3100e-01\n",
            " -5.0571e-01  1.8094e-01  3.6548e-01  4.0391e-01  9.8332e-02 -8.5674e-01\n",
            " -3.4216e-01  2.3133e-01  2.0996e-01  1.0736e-01  6.4859e-02  3.8116e-01\n",
            " -1.9745e-01 -3.4992e-01 -1.4024e-01 -1.1248e-01  4.0959e-01  3.9222e-02\n",
            "  9.7528e-01  2.8322e-01  6.9825e-02 -7.8264e-01 -3.5349e-01  1.0494e-01\n",
            " -1.3360e-01  6.3501e-01 -2.6349e-01  7.4375e-02  6.5436e-01 -5.5198e-01\n",
            " -2.3475e-02 -1.7402e-01 -3.6973e-01 -3.9919e-01  5.6042e-01 -2.8520e-01\n",
            " -5.5555e-01  9.7751e-02 -2.6530e-03 -4.9564e-01  4.1002e-01 -4.5144e-01\n",
            "  6.1865e-01  6.5570e-01  3.2134e-01 -2.5954e-01 -5.8615e-01  1.8095e-01\n",
            "  5.5883e-01 -2.8088e-01  2.1256e-01 -3.4411e-01 -5.3353e-01  4.3147e-01\n",
            "  8.9076e-01  2.0905e-01 -1.7124e-02  3.5795e-01 -6.2475e-01 -1.6113e-01\n",
            "  1.3008e-01 -3.1728e-01  6.8400e-02  1.1515e-02  4.5719e-01 -4.8818e-01\n",
            " -1.7250e-01 -3.6966e-01 -4.2067e-01  2.6162e-01 -3.1324e-01  5.1430e-01\n",
            "  5.3977e-01  5.3872e-01]\n",
            "jumps:  [ 0.34995    0.095074  -0.64607   -0.25639    0.17252   -0.47998\n",
            " -0.25145   -0.35619   -0.40904   -0.26207   -0.075424  -0.3558\n",
            "  0.36008    0.032778  -0.20451    0.2594    -0.28017   -0.19266\n",
            "  0.7978    -0.061735   0.43611    0.75488    0.71363   -0.16999\n",
            " -0.018755   0.33876   -0.18916    0.054519  -0.70085   -0.50739\n",
            " -0.17996   -0.21199    0.16489   -0.90909    0.72333    0.096124\n",
            " -0.75995   -0.43706   -0.92707    0.15698    0.25441    0.25025\n",
            " -0.28406   -0.37363    0.67765    0.3535    -0.29235   -0.56071\n",
            " -0.58276    0.10647   -0.42709   -0.14311    0.31088    0.26132\n",
            "  0.53098    0.45227   -0.72973   -0.30574   -0.10712    0.20832\n",
            " -0.63229    0.12498    0.42135   -0.094986   0.18732    0.17619\n",
            "  0.10098   -0.077144   0.22384   -0.23426   -0.016561   0.11803\n",
            " -0.3579     0.083167   0.39282    0.20462   -0.10714   -0.2095\n",
            "  0.047883   0.87777    0.46483    0.12565   -0.59199   -0.1897\n",
            " -0.76223   -0.92849    0.14238    0.10795    0.13299   -1.0459\n",
            " -0.40652   -0.0069797 -0.62038    0.68819    0.32092   -0.034481\n",
            "  0.41429    0.080192   0.36452   -0.51149   -0.17595    0.32034\n",
            " -0.33943    0.48514    0.30028   -0.29691    0.19679    0.042799\n",
            "  0.062819   0.32962    0.0024064 -0.55518    0.52484   -0.37855\n",
            " -0.16013    0.18067    0.75056   -0.37793    0.4326     0.25855\n",
            " -0.14586   -0.25838   -0.26509   -0.090255  -0.84919    0.038647\n",
            " -0.56398    0.26634    0.37917    0.15311   -0.23534   -0.1771\n",
            "  0.043473  -0.87943   -0.017739  -0.035083   0.18803   -0.063969\n",
            "  0.060301  -0.54672    0.25768    0.403     -0.18811   -0.34368\n",
            "  0.91118    0.12708    0.10243   -0.2001    -0.050847   0.45843\n",
            " -0.022441  -0.13771    0.31546    0.31103   -0.18422   -0.11485\n",
            "  0.065411   0.046909   0.079907  -0.24623    0.42528   -0.01039\n",
            "  0.38679    0.31717   -0.042185  -0.31751   -0.13434   -0.12508\n",
            " -0.96391    0.033417  -0.48754   -0.3622    -0.45879    0.5039\n",
            "  0.22754   -0.060078  -0.34449   -0.57871    0.036803  -0.15362\n",
            "  0.70166    0.078452   0.15758    1.3976    -0.33147   -0.20967\n",
            "  0.52176    0.3201     0.42299   -0.27026    0.40665   -0.63597\n",
            " -0.65809    0.22565   -0.087924  -0.18587   -0.083251   0.33991\n",
            " -0.28863   -0.30402  ]\n",
            "over:  [-1.8136e-01  5.4384e-02 -1.6638e-02 -1.0872e-01  8.8530e-03  6.2523e-01\n",
            " -5.8821e-02  3.3254e-02  2.9468e-02  2.5949e-01  1.6012e-01  4.5017e-01\n",
            " -2.6535e-02  2.5883e-01  6.2274e-01  3.9724e-01 -4.2582e-01  7.8368e-02\n",
            " -1.7108e-01  9.3726e-02  5.5443e-01  2.9364e+00 -2.9856e-01 -2.6919e-01\n",
            "  2.6883e-01 -4.5052e-02 -2.6976e-01  1.2477e-01  3.2422e-02  2.1593e-01\n",
            "  1.2230e-01 -7.0885e-01 -1.3760e-01  3.8023e-01 -2.8467e-01 -4.2573e-01\n",
            " -1.2020e+00 -4.2784e-01 -1.8816e-01 -1.9955e-01  2.3510e-01 -3.0719e-01\n",
            "  1.1255e-01 -1.1238e-01 -3.0902e-01  2.9093e-01  5.8251e-01 -6.7236e-02\n",
            "  1.1878e-01  4.7467e-01 -9.4490e-03  2.6370e-01  6.7172e-02  8.6137e-01\n",
            "  7.9867e-01 -1.1649e-01 -3.4968e-01  3.5306e-01 -1.6667e-01  9.5608e-02\n",
            " -5.3847e-02  2.6636e-01 -8.5124e-02 -3.4143e-01  5.2658e-01 -2.5312e-01\n",
            "  3.6868e-01  1.4808e-01  5.0457e-01 -5.4672e-01 -5.7193e-03 -2.8087e-01\n",
            "  2.7664e-01 -1.1067e-01 -5.6938e-02  8.4386e-01 -1.5502e-02 -1.4716e-01\n",
            " -2.4319e-01  1.5463e-01 -1.4038e-01 -2.8210e-02  4.0895e-02 -6.9638e-02\n",
            "  7.3984e-03 -2.0785e-02 -1.9026e-01 -3.4531e-01  1.3974e-01 -1.7654e-01\n",
            "  3.6421e-01  3.5446e-02  8.9064e-03  3.2949e-01  7.8715e-02  1.4295e-01\n",
            "  3.4110e-01 -3.0039e-01  1.5895e-01 -4.4791e-01  2.8847e-01 -4.9616e-01\n",
            " -1.2712e-01  5.9769e-01 -1.7956e-01  1.2578e-01  2.2449e-01  1.3406e+00\n",
            "  3.1268e-01 -5.1212e-01 -2.8810e-01 -1.2124e-01 -3.6539e-01  2.3938e-01\n",
            "  6.0236e-01  5.5483e-01 -1.6237e-01 -9.7217e-03 -3.9532e-01 -8.0846e-02\n",
            " -2.0053e-01  4.3234e-01  3.7900e-01  2.1911e-01  5.0871e-01 -9.6578e-01\n",
            "  4.0803e-01 -8.7810e-02  4.1018e-01  3.3282e-01 -2.6322e-01 -3.1882e-01\n",
            " -4.6560e-01 -4.9194e-01  1.3927e-01  6.7400e-01  3.7538e-01 -4.7802e-01\n",
            " -9.5047e-02  4.3122e-01 -3.3629e-01 -2.8367e-01  2.4260e-01 -1.6096e-01\n",
            "  1.3671e+00  2.6446e-01  1.8355e-01 -2.1050e-01  2.1915e-03  3.2011e-01\n",
            " -2.2010e-01  2.1907e-01 -1.6236e-01 -1.5451e-01  1.8775e-01 -1.5687e-01\n",
            " -1.1443e-01  7.4750e-02 -8.2473e-03 -4.4988e-01 -1.2534e-01  1.0274e-03\n",
            " -2.6980e-01  3.8504e-01  1.2015e-01  1.8465e-01 -5.7222e-01  2.6703e-01\n",
            " -3.6049e-02  1.0055e+00 -1.2874e-01  3.1265e-02  1.3461e-01  1.4928e-01\n",
            " -2.8600e-01  2.3514e-01 -2.9374e-01 -2.9196e-01 -2.9380e-01 -3.0427e-01\n",
            "  1.2025e+00  5.6008e-02  8.0408e-02  2.3824e-01  4.0132e-02  5.8081e-02\n",
            " -1.9350e-01  3.4714e-01  1.3727e-02 -1.3661e-02 -2.9019e-01  1.1244e-01\n",
            " -2.6544e-01  9.0986e-02  9.8320e-02 -2.1316e-01  6.3920e-01  1.6703e-01\n",
            " -3.7282e-01  9.7439e-02]\n",
            "the:  [-7.1549e-02  9.3459e-02  2.3738e-02 -9.0339e-02  5.6123e-02  3.2547e-01\n",
            " -3.9796e-01 -9.2139e-02  6.1181e-02 -1.8950e-01  1.3061e-01  1.4349e-01\n",
            "  1.1479e-02  3.8158e-01  5.4030e-01 -1.4088e-01  2.4315e-01  2.3036e-01\n",
            " -5.5339e-01  4.8154e-02  4.5662e-01  3.2338e+00  2.0199e-02  4.9019e-02\n",
            " -1.4132e-02  7.6017e-02 -1.1527e-01  2.0060e-01 -7.7657e-02  2.4328e-01\n",
            "  1.6368e-01 -3.4118e-01 -6.6070e-02  1.0152e-01  3.8232e-02 -1.7668e-01\n",
            " -8.8153e-01 -3.3895e-01 -3.5481e-02 -5.5095e-01 -1.6899e-02 -4.3982e-01\n",
            "  3.9004e-02  4.0447e-01 -2.5880e-01  6.4594e-01  2.6641e-01  2.8009e-01\n",
            " -2.4625e-02  6.3302e-01 -3.1700e-01  1.0271e-01  3.0886e-01  9.7792e-02\n",
            " -3.8227e-01  8.6552e-02  4.7075e-02  2.3511e-01 -3.2127e-01 -2.8538e-01\n",
            "  1.6670e-01 -4.9707e-03 -6.2714e-01 -2.4904e-01  2.9713e-01  1.4379e-01\n",
            " -1.2325e-01 -5.8178e-02 -1.0290e-03 -8.2126e-02  3.6935e-01 -5.8442e-04\n",
            "  3.4286e-01  2.8426e-01 -6.8599e-02  6.5747e-01 -2.9087e-02  1.6184e-01\n",
            "  7.3672e-02 -3.0343e-01  9.5733e-02 -5.2860e-01 -2.2898e-01  6.4079e-02\n",
            "  1.5218e-02  3.4921e-01 -4.3960e-01 -4.3983e-01  7.7515e-01 -8.7767e-01\n",
            " -8.7504e-02  3.9598e-01  6.2362e-01 -2.6211e-01 -3.0539e-01 -2.2964e-02\n",
            "  3.0567e-01  6.7660e-02  1.5383e-01 -1.1211e-01 -9.1540e-02  8.2562e-02\n",
            "  1.6897e-01 -3.2952e-02 -2.8775e-01 -2.2320e-01 -9.0426e-02  1.2407e+00\n",
            " -1.8244e-01 -7.5219e-03 -4.1388e-02 -1.1083e-02  7.8186e-02  3.8511e-01\n",
            "  2.3334e-01  1.4414e-01 -9.1070e-04 -2.6388e-01 -2.0481e-01  1.0099e-01\n",
            "  1.4076e-01  2.8834e-01 -4.5429e-02  3.7247e-01  1.3645e-01 -6.7457e-01\n",
            "  2.2786e-01  1.2599e-01  2.9091e-02  3.0428e-02 -1.3028e-01  1.9408e-01\n",
            "  4.9014e-01 -3.9121e-01 -7.5952e-02  7.4731e-02  1.8902e-01 -1.6922e-01\n",
            " -2.6019e-01 -3.9771e-02 -2.4153e-01  1.0875e-01  3.0434e-01  3.6009e-02\n",
            "  1.4264e+00  1.2759e-01 -7.3811e-02 -2.0418e-01  8.0016e-03  1.5381e-01\n",
            "  2.0223e-01  2.8274e-01  9.6206e-02 -3.3634e-01  5.0983e-01  3.2625e-01\n",
            " -2.6535e-01  3.7400e-01 -3.0388e-01 -4.0033e-01 -4.2910e-02 -6.7897e-02\n",
            " -2.9332e-01  1.0978e-01 -4.5365e-02  2.3222e-01 -3.1134e-01 -2.8983e-01\n",
            " -6.6687e-01  5.3097e-01  1.9461e-01  3.6670e-01  2.6185e-01 -6.5187e-01\n",
            "  1.0266e-01  1.1363e-01 -1.2953e-01 -6.8246e-01 -1.8751e-01  1.4760e-01\n",
            "  1.0765e+00 -2.2908e-01 -9.3435e-03 -2.0651e-01 -3.5225e-01 -2.6720e-01\n",
            " -3.4307e-03  2.5906e-01  2.1759e-01  6.6158e-01  1.2180e-01  1.9957e-01\n",
            " -2.0303e-01  3.4474e-01 -2.4328e-01  1.3139e-01 -8.8767e-03  3.3617e-01\n",
            "  3.0591e-02  2.5577e-01]\n",
            "lazy:  [ 2.4360e-01 -3.4947e-01 -3.6866e-01 -1.2049e-01  1.5666e-01 -2.8681e-01\n",
            " -2.7496e-01 -1.7186e-01  4.1094e-02  2.7805e-01  4.7063e-01 -7.6601e-01\n",
            "  9.3984e-01  3.1374e-01 -1.1558e-01  3.7066e-01 -7.3339e-01 -1.4001e-01\n",
            "  3.8372e-01 -1.1413e-01  2.2680e-01  1.8525e-01 -4.8286e-01  6.8405e-01\n",
            "  4.2411e-01 -5.3271e-01  2.1549e-01 -5.1370e-01 -2.0095e-01  3.6552e-01\n",
            "  7.3175e-02  4.7971e-01 -7.4379e-01  2.5743e-02  5.1195e-01 -9.6689e-02\n",
            " -3.1828e-01 -5.0514e-01 -7.7848e-02 -5.9908e-02  4.1613e-01  5.6312e-01\n",
            "  6.2281e-01 -1.1399e-01  5.7956e-01  5.4217e-01 -1.5364e-02 -1.8366e-01\n",
            " -9.4557e-02  9.7040e-01 -6.9829e-01 -2.8877e-01 -1.0291e-01  1.6857e-01\n",
            "  3.9540e-01 -2.6136e-01 -2.0364e-01 -4.3896e-01  1.5321e-01  2.0340e-01\n",
            "  2.3708e-01 -4.4019e-01 -2.5953e-01 -2.9465e-01 -7.0392e-01  4.5277e-01\n",
            "  4.8609e-01  2.4371e-01  1.0621e+00 -1.1893e-02  1.6272e-01  4.7658e-02\n",
            " -6.0591e-01  1.7743e-01 -2.3131e-01 -5.2727e-01 -4.4549e-01  2.2854e-01\n",
            "  4.5121e-02  1.9055e-01  4.8879e-02 -1.6769e-01 -1.5795e-01 -7.7439e-01\n",
            "  4.8220e-01 -2.1924e-01  1.5959e-01 -2.3288e-01 -2.1956e-01 -5.6318e-01\n",
            " -3.0414e-01 -4.9283e-01 -1.0453e-01  5.2286e-01  1.6010e-01  1.5194e-01\n",
            "  1.4254e-01 -2.3295e-01  4.2537e-01  5.8918e-01 -3.2303e-03 -2.0670e-01\n",
            "  1.1983e-01 -1.1128e-01  2.1679e-01  2.0885e-02  2.3287e-02  5.7264e-04\n",
            " -3.7682e-01  3.8070e-01  1.7784e-01 -3.3404e-01  4.4448e-01 -2.1758e-01\n",
            " -5.8585e-02  2.9275e-01  6.3896e-01  4.5773e-02 -5.1163e-01  2.3018e-01\n",
            "  3.5605e-02 -3.1697e-01 -1.7654e-02 -7.9892e-01  1.0807e-01  6.0575e-01\n",
            "  6.1157e-01  2.0286e-01  5.6747e-01 -3.7504e-02  8.8656e-02 -2.2181e-01\n",
            " -8.2162e-02  2.4194e-01  9.2096e-02 -5.4754e-01 -4.4629e-01  6.1179e-01\n",
            "  2.5916e-01 -7.8731e-01  8.8334e-02 -5.4586e-01 -1.6790e-01 -2.9476e-01\n",
            "  1.2879e+00 -3.0823e-02 -2.1768e-01 -4.3510e-01  2.8531e-01  1.6418e-02\n",
            "  4.0111e-01  1.0992e+00 -2.2871e-01 -9.6040e-02  2.4002e-01 -2.2189e-03\n",
            "  2.2825e-01 -8.8586e-02 -6.9752e-01  4.5946e-02  3.6988e-01 -1.2353e-01\n",
            " -8.9403e-02  8.0708e-01  4.2137e-02 -6.1002e-01 -2.7454e-01  7.8604e-01\n",
            " -1.8031e-01 -1.2763e-01 -5.1040e-01  2.1468e-01 -1.2239e-01  4.3400e-01\n",
            "  4.1641e-01 -4.0750e-01 -4.5797e-01 -3.3990e-02  7.3162e-02  7.9555e-01\n",
            "  3.5424e-01  3.9136e-01 -7.2358e-01  4.7071e-01 -7.6863e-01  5.8818e-01\n",
            "  5.5482e-01 -1.5730e-01  1.0282e-01 -1.1142e-01 -8.3167e-02 -1.1425e+00\n",
            "  4.8521e-02 -3.0276e-03  3.6737e-01  1.9888e-01  4.6231e-02  4.9800e-01\n",
            " -9.2291e-02 -2.4074e-02]\n",
            "dog:  [-1.3791e-01 -4.7601e-01 -5.6369e-02 -3.9082e-01 -1.7544e-01 -6.2244e-01\n",
            " -3.9816e-01  2.9620e-01 -6.0647e-02 -6.7017e-02  1.1466e-01 -3.3015e-01\n",
            " -2.0318e-02  6.0616e-01 -1.3920e-01  1.3896e-01 -5.4781e-01  3.0864e-01\n",
            "  1.7354e-01  3.9927e-01  2.1137e-01  1.3004e+00  8.8030e-01  2.3946e-01\n",
            "  2.8838e-01 -4.6336e-01  2.5745e-01 -3.1755e-01 -3.2877e-01 -5.9534e-01\n",
            "  2.3983e-01  3.4159e-01  1.2754e-01 -8.8208e-01  1.4258e-01 -1.8857e-01\n",
            " -1.6961e-01  2.7808e-01 -2.4600e-01  1.9122e-01  5.0244e-01  5.3660e-01\n",
            " -5.3568e-01  2.4827e-01  3.2561e-01  6.7882e-01  9.6401e-01 -2.8892e-01\n",
            "  5.1206e-01  5.8496e-01 -3.1934e-02 -2.4849e-02  8.8564e-02  1.7360e-01\n",
            "  5.4166e-01 -8.6743e-02 -3.8412e-01  1.3974e-01 -7.4122e-03  9.2210e-01\n",
            " -2.5799e-01 -4.7018e-01 -5.5742e-01 -2.1213e-02 -7.1072e-01  8.0995e-02\n",
            " -4.7254e-01 -3.2925e-01  6.8052e-01  1.7242e-01  8.7783e-02 -2.6560e-01\n",
            " -6.0070e-01 -8.5217e-02 -3.6977e-02 -3.6593e-01 -6.2576e-01 -3.4162e-01\n",
            "  5.4672e-02 -1.1734e-01  1.9686e-01  8.3758e-02  4.3157e-01 -8.2195e-01\n",
            " -5.7756e-01  6.7821e-02 -4.9520e-01  1.4769e-01  3.2863e-01 -1.0649e+00\n",
            " -3.9756e-01 -3.4890e-01 -6.1548e-02  7.5400e-01  5.2457e-01  1.3657e-01\n",
            " -4.1904e-02 -4.4660e-01  8.4754e-02  3.7516e-01 -6.2374e-02 -8.1762e-02\n",
            " -4.1776e-01  3.0157e-02 -7.7967e-01  8.7627e-02  9.0542e-02  7.5266e-01\n",
            "  2.9235e-02 -1.8324e-01  5.5433e-01 -3.4632e-01 -7.8019e-02 -1.2078e-01\n",
            " -6.8377e-01  2.8826e-02 -4.1618e-01  2.2341e-01 -8.0811e-01 -5.9707e-01\n",
            "  4.6835e-01 -3.8246e-01 -2.3549e-01 -6.2565e-01  6.1201e-01  1.4221e-01\n",
            "  2.4076e-02  4.2106e-01  5.1978e-01  2.7811e-01  1.6885e-01 -4.9293e-01\n",
            "  3.1563e-01 -5.9663e-01  1.5091e-01 -6.3300e-01  1.1324e-03 -6.2031e-02\n",
            " -3.8694e-02 -2.6038e-01  2.1907e-01  2.3103e-01  8.2427e-01  1.4963e-01\n",
            "  8.5767e-01  1.5706e-01 -2.9116e-01 -4.2033e-01  4.5080e-01  3.9614e-01\n",
            " -2.0271e-01  1.0702e+00 -4.1153e-01  2.2282e-01  1.3287e-01  9.3896e-01\n",
            "  1.6088e-01 -2.2247e-01 -1.1443e+00 -5.0556e-01 -1.9619e-01  3.4685e-01\n",
            "  5.9883e-01  2.7666e-02 -2.1223e-01 -5.4970e-01 -1.6784e-01  5.2375e-01\n",
            " -9.8196e-02  1.5559e-01 -2.3997e-01 -1.1526e-01  1.6577e-02  6.1643e-02\n",
            "  9.2234e-02  2.5817e-02  2.9163e-01 -5.0848e-01 -1.5164e-01  1.8360e-01\n",
            "  1.6434e+00 -3.1567e-01 -5.3165e-01 -4.4914e-01 -7.2425e-01 -4.1122e-01\n",
            "  1.8799e-01  1.6130e-01  7.4169e-01 -2.1597e-02 -3.7775e-01 -5.5265e-01\n",
            "  7.3535e-02  8.5621e-02 -6.6452e-02  5.4982e-02 -4.0218e-01  4.3235e-01\n",
            " -7.5727e-02  7.0530e-02]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "YN0sAZkEO9w0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------------\n",
        "# code for working with movie reviews data \n",
        "# Source: Miller, T. W. (2016). Web and Network Data Science.\n",
        "#    Upper Saddle River, N.J.: Pearson Education.\n",
        "#    ISBN-13: 978-0-13-388644-3\n",
        "# This original study used a simple bag-of-words approach\n",
        "# to sentiment analysis, along with pre-defined lists of\n",
        "# negative and positive words.        \n",
        "# Code available at:  https://github.com/mtpa/wnds       \n",
        "# ------------------------------------------------------------"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PT2lfN98O9w4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Utility function to get file names within a directory\n",
        "def listdir_no_hidden(path):\n",
        "    start_list = os.listdir(path)\n",
        "    end_list = []\n",
        "    for file in start_list:\n",
        "        if (not file.startswith('.')):\n",
        "            end_list.append(file)\n",
        "    return(end_list)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "clA0JpmBO9w6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# define list of codes to be dropped from document\n",
        "# carriage-returns, line-feeds, tabs\n",
        "codelist = ['\\r', '\\n', '\\t']   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gnzNIeyaO9w7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# We will not remove stopwords in this exercise because they are\n",
        "# important to keeping sentences intact\n",
        "if REMOVE_STOPWORDS:\n",
        "    print(nltk.corpus.stopwords.words('english'))\n",
        "\n",
        "# previous analysis of a list of top terms showed a number of words, along \n",
        "# with contractions and other word strings to drop from further analysis, add\n",
        "# these to the usual English stopwords to be dropped from a document collection\n",
        "    more_stop_words = ['cant','didnt','doesnt','dont','goes','isnt','hes',\\\n",
        "        'shes','thats','theres','theyre','wont','youll','youre','youve', 'br'\\\n",
        "        've', 're', 'vs'] \n",
        "\n",
        "    some_proper_nouns_to_remove = ['dick','ginger','hollywood','jack',\\\n",
        "        'jill','john','karloff','kudrow','orson','peter','tcm','tom',\\\n",
        "        'toni','welles','william','wolheim','nikita']\n",
        "\n",
        "    # start with the initial list and add to it for movie text work \n",
        "    stoplist = nltk.corpus.stopwords.words('english') + more_stop_words +\\\n",
        "        some_proper_nouns_to_remove"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JSQqTE4aO9w8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# text parsing function for creating text documents \n",
        "# there is more we could do for data preparation \n",
        "# stemming... looking for contractions... possessives... \n",
        "# but we will work with what we have in this parsing function\n",
        "# if we want to do stemming at a later time, we can use\n",
        "#     porter = nltk.PorterStemmer()  \n",
        "# in a construction like this\n",
        "#     words_stemmed =  [porter.stem(word) for word in initial_words]  \n",
        "def text_parse(string):\n",
        "    # replace non-alphanumeric with space \n",
        "    temp_string = re.sub('[^a-zA-Z]', '  ', string)    \n",
        "    # replace codes with space\n",
        "    for i in range(len(codelist)):\n",
        "        stopstring = ' ' + codelist[i] + '  '\n",
        "        temp_string = re.sub(stopstring, '  ', temp_string)      \n",
        "    # replace single-character words with space\n",
        "    temp_string = re.sub('\\s.\\s', ' ', temp_string)   \n",
        "    # convert uppercase to lowercase\n",
        "    temp_string = temp_string.lower()    \n",
        "    if REMOVE_STOPWORDS:\n",
        "        # replace selected character strings/stop-words with space\n",
        "        for i in range(len(stoplist)):\n",
        "            stopstring = ' ' + str(stoplist[i]) + ' '\n",
        "            temp_string = re.sub(stopstring, ' ', temp_string)        \n",
        "    # replace multiple blank characters with one blank character\n",
        "    temp_string = re.sub('\\s+', ' ', temp_string)    \n",
        "    return(temp_string)    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KN3SrgKFO9xC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Manually import these"
      ]
    },
    {
      "metadata": {
        "id": "tEtQyJz9O9xE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "4b2ce8e2-b649-407c-edef-083dbd427e47"
      },
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------\n",
        "# gather data for 500 negative movie reviews\n",
        "# -----------------------------------------------\n",
        "dir_name = 'movie-reviews-negative'\n",
        "    \n",
        "filenames = listdir_no_hidden(path=dir_name)\n",
        "num_files = len(filenames)\n",
        "\n",
        "for i in range(len(filenames)):\n",
        "    file_exists = os.path.isfile(os.path.join(dir_name, filenames[i]))\n",
        "    assert file_exists\n",
        "print('\\nDirectory:',dir_name)    \n",
        "print('%d files found' % len(filenames))"
      ],
      "execution_count": 265,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Directory: movie-reviews-negative\n",
            "500 files found\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "RQfUiyUgO9xI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "e0305c9a-8339-4943-f4ad-84ba31fb6af2"
      },
      "cell_type": "code",
      "source": [
        "# Read data for negative movie reviews\n",
        "# Data will be stored in a list of lists where the each list represents \n",
        "# a document and document is a list of words.\n",
        "# We then break the text into words.\n",
        "\n",
        "def read_data(filename):\n",
        "\n",
        "  #with open(filename, encoding='utf-8') as f:\n",
        "  with open(filename) as f:\n",
        "    data = tf.compat.as_str(f.read())\n",
        "    data = data.lower()\n",
        "    data = text_parse(data)\n",
        "    data = TreebankWordTokenizer().tokenize(data)  # The Penn Treebank\n",
        "\n",
        "  return data\n",
        "\n",
        "negative_documents = []\n",
        "\n",
        "print('\\nProcessing document files under', dir_name)\n",
        "for i in range(num_files):\n",
        "    ## print(' ', filenames[i])\n",
        "\n",
        "    words = read_data(os.path.join(dir_name, filenames[i]))\n",
        "\n",
        "    negative_documents.append(words)\n",
        "    # print('Data size (Characters) (Document %d) %d' %(i,len(words)))\n",
        "    # print('Sample string (Document %d) %s'%(i,words[:50]))"
      ],
      "execution_count": 266,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Processing document files under movie-reviews-negative\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5bR_uHo3O9xK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "9cd6aacd-cf7d-4c4a-9034-eb83edc50681"
      },
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------\n",
        "# gather data for 500 positive movie reviews\n",
        "# -----------------------------------------------\n",
        "dir_name = 'movie-reviews-positive'  \n",
        "filenames = listdir_no_hidden(path=dir_name)\n",
        "num_files = len(filenames)\n",
        "\n",
        "for i in range(len(filenames)):\n",
        "    file_exists = os.path.isfile(os.path.join(dir_name, filenames[i]))\n",
        "    assert file_exists\n",
        "print('\\nDirectory:',dir_name)    \n",
        "print('%d files found' % len(filenames))"
      ],
      "execution_count": 267,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Directory: movie-reviews-positive\n",
            "500 files found\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DopRsdLUO9xS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "a3c2f5fc-f68c-4cc3-fa22-a06be0c33485"
      },
      "cell_type": "code",
      "source": [
        "# Read data for positive movie reviews\n",
        "# Data will be stored in a list of lists where the each list \n",
        "# represents a document and document is a list of words.\n",
        "# We then break the text into words.\n",
        "\n",
        "def read_data(filename):\n",
        "\n",
        "  with open(filename) as f:\n",
        "    data = tf.compat.as_str(f.read())\n",
        "    data = data.lower()\n",
        "    data = text_parse(data)\n",
        "    data = TreebankWordTokenizer().tokenize(data)  # The Penn Treebank\n",
        "\n",
        "  return data\n",
        "\n",
        "positive_documents = []\n",
        "\n",
        "print('\\nProcessing document files under', dir_name)\n",
        "for i in range(num_files):\n",
        "    ## print(' ', filenames[i])\n",
        "\n",
        "    words = read_data(os.path.join(dir_name, filenames[i]))\n",
        "\n",
        "    positive_documents.append(words)\n",
        "    # print('Data size (Characters) (Document %d) %d' %(i,len(words)))\n",
        "    # print('Sample string (Document %d) %s'%(i,words[:50]))\n",
        "\n"
      ],
      "execution_count": 268,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Processing document files under movie-reviews-positive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bnf_ewWhO9xU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "371d5914-68fe-4878-d659-ac44b53a14be"
      },
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------------\n",
        "# convert positive/negative documents into numpy array\n",
        "# note that reviews vary from 22 to 1052 words   \n",
        "# so we use the first 20 and last 20 words of each review \n",
        "# as our word sequences for analysis\n",
        "# -----------------------------------------------------\n",
        "max_review_length = 0  # initialize\n",
        "for doc in negative_documents:\n",
        "    max_review_length = max(max_review_length, len(doc))    \n",
        "for doc in positive_documents:\n",
        "    max_review_length = max(max_review_length, len(doc)) \n",
        "print('max_review_length:', max_review_length) \n",
        "\n",
        "min_review_length = max_review_length  # initialize\n",
        "for doc in negative_documents:\n",
        "    min_review_length = min(min_review_length, len(doc))    \n",
        "for doc in positive_documents:\n",
        "    min_review_length = min(min_review_length, len(doc)) \n",
        "print('min_review_length:', min_review_length) \n",
        "\n"
      ],
      "execution_count": 269,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "max_review_length: 1052\n",
            "min_review_length: 22\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "j_iaUVcPO9xX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# construct list of 1000 lists with 40 words in each list\n",
        "from itertools import chain\n",
        "documents = []\n",
        "for doc in negative_documents:\n",
        "    doc_begin = doc[0:20]\n",
        "    doc_end = doc[len(doc) - 20: len(doc)]\n",
        "    documents.append(list(chain(*[doc_begin, doc_end])))    \n",
        "for doc in positive_documents:\n",
        "    doc_begin = doc[0:20]\n",
        "    doc_end = doc[len(doc) - 20: len(doc)]\n",
        "    documents.append(list(chain(*[doc_begin, doc_end])))    \n",
        "\n",
        "# create list of lists of lists for embeddings\n",
        "embeddings = []    \n",
        "for doc in documents:\n",
        "    embedding = []\n",
        "    for word in doc:\n",
        "       embedding.append(limited_index_to_embedding[limited_word_to_index[word]]) \n",
        "    embeddings.append(embedding)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lWzmWNnUO9xY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1224
        },
        "outputId": "22265f56-4e46-4c65-e031-ce03c1797832"
      },
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------------    \n",
        "# Check on the embeddings list of list of lists \n",
        "# -----------------------------------------------------\n",
        "# Show the first word in the first document\n",
        "test_word = documents[0][0]    \n",
        "print('First word in first document:', test_word)    \n",
        "print('Embedding for this word:\\n', \n",
        "      limited_index_to_embedding[limited_word_to_index[test_word]])\n",
        "print('Corresponding embedding from embeddings list of list of lists\\n',\n",
        "      embeddings[0][0][:])\n",
        "\n"
      ],
      "execution_count": 272,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First word in first document: strained\n",
            "Embedding for this word:\n",
            " [-9.6104e-02 -4.9919e-01  3.1536e-01  1.9711e-01 -4.3080e-01  3.0870e-01\n",
            " -1.4987e-01  3.0923e-01 -8.2258e-01  3.7316e-01 -1.3906e-01  5.4338e-01\n",
            " -4.5029e-01  8.3690e-02  6.2485e-01  4.3483e-01 -8.6542e-01  7.1274e-01\n",
            " -4.5097e-01 -1.7131e-01  6.3485e-01  1.5939e+00 -6.0457e-01  2.0648e-01\n",
            " -1.5717e-01  1.5143e-01  6.6534e-01  1.0821e-01 -3.4151e-01 -1.0388e-01\n",
            " -4.2171e-01 -4.7619e-01 -5.4476e-01  2.7599e-01 -1.5306e-01 -2.3153e-02\n",
            " -2.1823e-01  1.1273e-01 -8.9172e-01 -7.4871e-01 -3.4198e-01  1.4960e-01\n",
            " -5.1809e-01  3.5296e-01  7.0884e-01 -1.5524e-01 -2.3003e-01 -5.0443e-01\n",
            " -1.5025e-01  1.6477e-01  1.5172e-01  5.2337e-01 -2.8257e-01  3.6122e-01\n",
            "  3.9183e-01 -4.0019e-01  6.3946e-01  3.7599e-02 -1.6185e-01 -3.4232e-01\n",
            "  3.8010e-01  3.7329e-01 -5.5415e-01 -2.0802e-01  3.4501e-01 -3.9860e-01\n",
            "  3.5179e-01 -4.5948e-01  4.3508e-01 -2.3781e-02  2.5629e-01  2.3058e-01\n",
            " -3.6185e-01  1.9051e-01 -7.2532e-01  4.0383e-01 -4.6919e-01 -4.6535e-01\n",
            "  4.9748e-02  2.8736e-02 -7.5741e-01 -7.2535e-01  2.3552e-01 -6.3291e-01\n",
            "  3.4161e-01 -3.1012e-01  9.8412e-01 -3.9369e-01 -2.3284e-02  3.5040e-01\n",
            " -1.1184e-01 -3.1817e-01  1.2922e-01 -3.3598e-01 -5.8844e-01  2.1254e-01\n",
            " -4.9977e-01  3.4264e-02  4.0339e-02  5.1240e-01  1.5904e-01 -2.2130e-01\n",
            " -2.6743e-01  8.4299e-04 -4.2637e-01  4.2900e-01 -6.2602e-01 -5.0934e-01\n",
            " -6.4174e-03 -5.4732e-01 -4.7422e-01  9.5203e-02 -5.0472e-01 -1.5143e-02\n",
            "  1.6225e-01 -7.0379e-02 -1.1214e+00  4.3672e-02  1.3457e-01 -2.2615e-02\n",
            " -1.5405e-01  3.1841e-01 -9.4757e-02  1.1208e+00  2.0000e-01  1.8864e-01\n",
            "  1.1271e-01  4.2490e-02 -3.4035e-01  1.0283e+00  7.2587e-01 -7.5562e-02\n",
            " -2.1494e-01 -5.4325e-01 -2.0568e-01 -1.2925e-01 -6.1079e-02 -1.3631e-02\n",
            " -7.1170e-01 -3.0614e-01 -6.6467e-01 -2.0466e-01 -3.0216e-01  1.7461e-01\n",
            "  1.1361e+00  4.9711e-01 -2.8688e-01 -2.7638e-01  6.3191e-01 -4.7838e-01\n",
            " -7.9403e-02 -6.8140e-01 -1.4690e-01 -2.4596e-01 -5.8379e-01 -1.7419e-01\n",
            " -3.3996e-01  2.6822e-01  5.5732e-01 -3.3832e-02  5.0178e-01 -7.2444e-01\n",
            " -5.8312e-01  7.2225e-01 -5.6705e-01  1.8360e-01  5.9092e-01  1.0928e+00\n",
            " -9.9976e-02  1.4157e-01 -3.7656e-02 -3.7221e-01 -3.9105e-01  4.6243e-01\n",
            "  2.1704e-01  2.6170e-01  5.3678e-02 -6.6062e-01  1.1492e+00 -3.8682e-02\n",
            "  1.3699e-01 -7.9457e-02 -3.0951e-01  8.0134e-01 -7.5186e-01 -1.4870e-01\n",
            "  9.8889e-02 -2.3245e-01  1.8529e-01  2.4994e-01  1.9563e-01 -7.7170e-01\n",
            " -5.0174e-01  2.7482e-01 -3.9986e-01 -2.1185e-01 -2.1390e-01  2.9093e-01\n",
            " -3.3584e-01  8.6414e-02]\n",
            "Corresponding embedding from embeddings list of list of lists\n",
            " [-9.6104e-02 -4.9919e-01  3.1536e-01  1.9711e-01 -4.3080e-01  3.0870e-01\n",
            " -1.4987e-01  3.0923e-01 -8.2258e-01  3.7316e-01 -1.3906e-01  5.4338e-01\n",
            " -4.5029e-01  8.3690e-02  6.2485e-01  4.3483e-01 -8.6542e-01  7.1274e-01\n",
            " -4.5097e-01 -1.7131e-01  6.3485e-01  1.5939e+00 -6.0457e-01  2.0648e-01\n",
            " -1.5717e-01  1.5143e-01  6.6534e-01  1.0821e-01 -3.4151e-01 -1.0388e-01\n",
            " -4.2171e-01 -4.7619e-01 -5.4476e-01  2.7599e-01 -1.5306e-01 -2.3153e-02\n",
            " -2.1823e-01  1.1273e-01 -8.9172e-01 -7.4871e-01 -3.4198e-01  1.4960e-01\n",
            " -5.1809e-01  3.5296e-01  7.0884e-01 -1.5524e-01 -2.3003e-01 -5.0443e-01\n",
            " -1.5025e-01  1.6477e-01  1.5172e-01  5.2337e-01 -2.8257e-01  3.6122e-01\n",
            "  3.9183e-01 -4.0019e-01  6.3946e-01  3.7599e-02 -1.6185e-01 -3.4232e-01\n",
            "  3.8010e-01  3.7329e-01 -5.5415e-01 -2.0802e-01  3.4501e-01 -3.9860e-01\n",
            "  3.5179e-01 -4.5948e-01  4.3508e-01 -2.3781e-02  2.5629e-01  2.3058e-01\n",
            " -3.6185e-01  1.9051e-01 -7.2532e-01  4.0383e-01 -4.6919e-01 -4.6535e-01\n",
            "  4.9748e-02  2.8736e-02 -7.5741e-01 -7.2535e-01  2.3552e-01 -6.3291e-01\n",
            "  3.4161e-01 -3.1012e-01  9.8412e-01 -3.9369e-01 -2.3284e-02  3.5040e-01\n",
            " -1.1184e-01 -3.1817e-01  1.2922e-01 -3.3598e-01 -5.8844e-01  2.1254e-01\n",
            " -4.9977e-01  3.4264e-02  4.0339e-02  5.1240e-01  1.5904e-01 -2.2130e-01\n",
            " -2.6743e-01  8.4299e-04 -4.2637e-01  4.2900e-01 -6.2602e-01 -5.0934e-01\n",
            " -6.4174e-03 -5.4732e-01 -4.7422e-01  9.5203e-02 -5.0472e-01 -1.5143e-02\n",
            "  1.6225e-01 -7.0379e-02 -1.1214e+00  4.3672e-02  1.3457e-01 -2.2615e-02\n",
            " -1.5405e-01  3.1841e-01 -9.4757e-02  1.1208e+00  2.0000e-01  1.8864e-01\n",
            "  1.1271e-01  4.2490e-02 -3.4035e-01  1.0283e+00  7.2587e-01 -7.5562e-02\n",
            " -2.1494e-01 -5.4325e-01 -2.0568e-01 -1.2925e-01 -6.1079e-02 -1.3631e-02\n",
            " -7.1170e-01 -3.0614e-01 -6.6467e-01 -2.0466e-01 -3.0216e-01  1.7461e-01\n",
            "  1.1361e+00  4.9711e-01 -2.8688e-01 -2.7638e-01  6.3191e-01 -4.7838e-01\n",
            " -7.9403e-02 -6.8140e-01 -1.4690e-01 -2.4596e-01 -5.8379e-01 -1.7419e-01\n",
            " -3.3996e-01  2.6822e-01  5.5732e-01 -3.3832e-02  5.0178e-01 -7.2444e-01\n",
            " -5.8312e-01  7.2225e-01 -5.6705e-01  1.8360e-01  5.9092e-01  1.0928e+00\n",
            " -9.9976e-02  1.4157e-01 -3.7656e-02 -3.7221e-01 -3.9105e-01  4.6243e-01\n",
            "  2.1704e-01  2.6170e-01  5.3678e-02 -6.6062e-01  1.1492e+00 -3.8682e-02\n",
            "  1.3699e-01 -7.9457e-02 -3.0951e-01  8.0134e-01 -7.5186e-01 -1.4870e-01\n",
            "  9.8889e-02 -2.3245e-01  1.8529e-01  2.4994e-01  1.9563e-01 -7.7170e-01\n",
            " -5.0174e-01  2.7482e-01 -3.9986e-01 -2.1185e-01 -2.1390e-01  2.9093e-01\n",
            " -3.3584e-01  8.6414e-02]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tvbFdwc0O9xa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1224
        },
        "outputId": "313fea53-8ee9-4e3f-973a-aece321cb844"
      },
      "cell_type": "code",
      "source": [
        "# Show the seventh word in the tenth document\n",
        "test_word = documents[6][9]    \n",
        "print('First word in first document:', test_word)    \n",
        "print('Embedding for this word:\\n', \n",
        "      limited_index_to_embedding[limited_word_to_index[test_word]])\n",
        "print('Corresponding embedding from embeddings list of list of lists\\n',\n",
        "      embeddings[6][9][:])\n",
        "\n"
      ],
      "execution_count": 273,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First word in first document: ever\n",
            "Embedding for this word:\n",
            " [ 2.1330e-01  1.0159e-01 -7.9846e-01 -3.1258e-01 -5.2301e-01  8.5879e-02\n",
            " -3.9847e-01  1.2413e-01  5.3686e-01 -2.8413e-01 -2.2199e-01  8.3012e-02\n",
            " -3.2399e-01  7.8373e-02  4.1602e-01  3.5626e-01  2.7240e-02  6.1429e-01\n",
            "  2.5173e-01  1.5019e-01  1.7121e-01  2.8553e+00  2.0633e-01  2.4040e-02\n",
            " -4.4565e-02 -2.0433e-01  2.5073e-01  2.6887e-01  2.3178e-02 -4.4504e-01\n",
            " -2.2790e-01 -2.6710e-01  5.3275e-01  6.8353e-02 -2.2983e-01  2.5972e-01\n",
            " -5.9040e-01 -3.0783e-02  1.3726e-01 -9.6350e-02 -4.2854e-01 -3.7986e-01\n",
            " -1.6216e-01  4.2504e-01 -4.1500e-01  2.3998e-03  1.0266e-01  1.8879e-01\n",
            "  4.0432e-01 -2.7399e-01 -2.6548e-01 -1.6831e-01 -1.1130e-01  4.1575e-01\n",
            "  5.1421e-01  1.4271e-01 -4.9686e-02 -8.0578e-02 -7.9020e-01  1.2625e-01\n",
            " -2.5764e-01  2.1986e-01 -3.8605e-01 -1.0071e-01  3.4754e-01  1.5648e-02\n",
            " -5.2139e-02  1.4517e-01  1.3641e-01  1.5839e-03  7.0667e-01 -2.1576e-02\n",
            "  3.8876e-01  2.2115e-01 -1.9469e-01  2.6829e-01 -2.8725e-01 -1.1961e-01\n",
            " -8.8836e-02 -1.0590e-01 -5.6784e-02 -1.3934e-01 -7.1522e-02 -5.7851e-02\n",
            " -1.2869e-01 -2.5081e-01 -2.2837e-02 -9.1676e-03  4.9658e-01 -9.0234e-01\n",
            " -2.5347e-01 -1.6432e-01  4.0843e-01 -2.3894e-01 -1.2900e-01 -1.5628e-01\n",
            "  3.2472e-01 -5.7837e-04 -1.8881e-01 -1.5174e-01  2.1750e-01 -3.0693e-01\n",
            " -3.1859e-01 -3.0137e-01  2.5286e-01 -1.0822e-01 -3.0438e-01  9.4749e-01\n",
            " -4.9148e-01 -3.5039e-02  3.8469e-01 -2.9068e-01 -3.8954e-02 -2.8539e-01\n",
            "  1.1991e-01  1.2586e-01 -2.1127e-01 -1.6648e-01 -6.2480e-01 -1.2853e-01\n",
            "  2.3134e-01 -2.9600e-01  2.8460e-01 -1.1783e-01 -3.2090e-01 -6.4879e-01\n",
            " -1.2510e-01 -1.0019e-02  5.2994e-01 -4.7399e-01 -2.5558e-01 -1.4615e-01\n",
            "  2.5803e-01 -2.4356e-01 -1.1342e-01  9.3975e-02 -8.4328e-02  3.9791e-01\n",
            "  2.1943e-02  1.3591e-01 -2.7038e-01 -3.7911e-01  1.3448e-01 -6.9800e-02\n",
            "  1.1493e+00  2.1612e-01  2.6659e-01 -4.3342e-01  1.0608e-01  1.8793e-01\n",
            "  2.0271e-01 -3.0976e-01 -3.3623e-01  2.4832e-01 -4.6154e-01  1.9787e-01\n",
            "  9.5651e-02  1.3149e-01 -1.0407e-01  4.4642e-01 -2.8652e-01  1.8765e-01\n",
            "  9.6509e-02 -1.7640e-01 -4.0949e-02 -3.2538e-02 -2.4234e-01 -1.9571e-01\n",
            " -5.8448e-01 -6.2755e-02 -1.0115e-01  2.0660e-02  1.0226e-01 -3.2458e-03\n",
            "  6.6059e-02 -7.9512e-02 -5.2104e-01 -2.2644e-01  2.0442e-01  2.7293e-01\n",
            "  1.5559e+00  6.3341e-02 -4.9536e-01  1.5236e-02 -6.1479e-01 -3.6319e-01\n",
            "  3.7308e-01  8.4875e-01  2.1099e-01 -1.1354e-01 -5.5955e-01 -8.2692e-02\n",
            " -2.0785e-01 -6.1039e-02  2.7334e-01  1.2062e-01 -6.6524e-02  2.5887e-01\n",
            "  2.4723e-01 -7.0231e-02]\n",
            "Corresponding embedding from embeddings list of list of lists\n",
            " [ 2.1330e-01  1.0159e-01 -7.9846e-01 -3.1258e-01 -5.2301e-01  8.5879e-02\n",
            " -3.9847e-01  1.2413e-01  5.3686e-01 -2.8413e-01 -2.2199e-01  8.3012e-02\n",
            " -3.2399e-01  7.8373e-02  4.1602e-01  3.5626e-01  2.7240e-02  6.1429e-01\n",
            "  2.5173e-01  1.5019e-01  1.7121e-01  2.8553e+00  2.0633e-01  2.4040e-02\n",
            " -4.4565e-02 -2.0433e-01  2.5073e-01  2.6887e-01  2.3178e-02 -4.4504e-01\n",
            " -2.2790e-01 -2.6710e-01  5.3275e-01  6.8353e-02 -2.2983e-01  2.5972e-01\n",
            " -5.9040e-01 -3.0783e-02  1.3726e-01 -9.6350e-02 -4.2854e-01 -3.7986e-01\n",
            " -1.6216e-01  4.2504e-01 -4.1500e-01  2.3998e-03  1.0266e-01  1.8879e-01\n",
            "  4.0432e-01 -2.7399e-01 -2.6548e-01 -1.6831e-01 -1.1130e-01  4.1575e-01\n",
            "  5.1421e-01  1.4271e-01 -4.9686e-02 -8.0578e-02 -7.9020e-01  1.2625e-01\n",
            " -2.5764e-01  2.1986e-01 -3.8605e-01 -1.0071e-01  3.4754e-01  1.5648e-02\n",
            " -5.2139e-02  1.4517e-01  1.3641e-01  1.5839e-03  7.0667e-01 -2.1576e-02\n",
            "  3.8876e-01  2.2115e-01 -1.9469e-01  2.6829e-01 -2.8725e-01 -1.1961e-01\n",
            " -8.8836e-02 -1.0590e-01 -5.6784e-02 -1.3934e-01 -7.1522e-02 -5.7851e-02\n",
            " -1.2869e-01 -2.5081e-01 -2.2837e-02 -9.1676e-03  4.9658e-01 -9.0234e-01\n",
            " -2.5347e-01 -1.6432e-01  4.0843e-01 -2.3894e-01 -1.2900e-01 -1.5628e-01\n",
            "  3.2472e-01 -5.7837e-04 -1.8881e-01 -1.5174e-01  2.1750e-01 -3.0693e-01\n",
            " -3.1859e-01 -3.0137e-01  2.5286e-01 -1.0822e-01 -3.0438e-01  9.4749e-01\n",
            " -4.9148e-01 -3.5039e-02  3.8469e-01 -2.9068e-01 -3.8954e-02 -2.8539e-01\n",
            "  1.1991e-01  1.2586e-01 -2.1127e-01 -1.6648e-01 -6.2480e-01 -1.2853e-01\n",
            "  2.3134e-01 -2.9600e-01  2.8460e-01 -1.1783e-01 -3.2090e-01 -6.4879e-01\n",
            " -1.2510e-01 -1.0019e-02  5.2994e-01 -4.7399e-01 -2.5558e-01 -1.4615e-01\n",
            "  2.5803e-01 -2.4356e-01 -1.1342e-01  9.3975e-02 -8.4328e-02  3.9791e-01\n",
            "  2.1943e-02  1.3591e-01 -2.7038e-01 -3.7911e-01  1.3448e-01 -6.9800e-02\n",
            "  1.1493e+00  2.1612e-01  2.6659e-01 -4.3342e-01  1.0608e-01  1.8793e-01\n",
            "  2.0271e-01 -3.0976e-01 -3.3623e-01  2.4832e-01 -4.6154e-01  1.9787e-01\n",
            "  9.5651e-02  1.3149e-01 -1.0407e-01  4.4642e-01 -2.8652e-01  1.8765e-01\n",
            "  9.6509e-02 -1.7640e-01 -4.0949e-02 -3.2538e-02 -2.4234e-01 -1.9571e-01\n",
            " -5.8448e-01 -6.2755e-02 -1.0115e-01  2.0660e-02  1.0226e-01 -3.2458e-03\n",
            "  6.6059e-02 -7.9512e-02 -5.2104e-01 -2.2644e-01  2.0442e-01  2.7293e-01\n",
            "  1.5559e+00  6.3341e-02 -4.9536e-01  1.5236e-02 -6.1479e-01 -3.6319e-01\n",
            "  3.7308e-01  8.4875e-01  2.1099e-01 -1.1354e-01 -5.5955e-01 -8.2692e-02\n",
            " -2.0785e-01 -6.1039e-02  2.7334e-01  1.2062e-01 -6.6524e-02  2.5887e-01\n",
            "  2.4723e-01 -7.0231e-02]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UGCdV6lzO9xh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1224
        },
        "outputId": "cfd2ca4d-43e9-4e7f-b26f-3ae51edbd850"
      },
      "cell_type": "code",
      "source": [
        "# Show the last word in the last document\n",
        "test_word = documents[999][39]    \n",
        "print('First word in first document:', test_word)    \n",
        "print('Embedding for this word:\\n', \n",
        "      limited_index_to_embedding[limited_word_to_index[test_word]])\n",
        "print('Corresponding embedding from embeddings list of list of lists\\n',\n",
        "      embeddings[999][39][:])        \n",
        "\n"
      ],
      "execution_count": 274,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First word in first document: out\n",
            "Embedding for this word:\n",
            " [ 0.08278    0.047335  -0.38854   -0.030124  -0.25428    0.351\n",
            " -0.26002   -0.46838   -0.048807   0.99838   -0.11526    0.45842\n",
            "  0.29279   -0.058347   0.11      -0.11706   -0.28233    0.1506\n",
            " -0.31632   -0.0068872  0.38673    3.1392     0.13653   -0.03454\n",
            "  0.35863   -0.12377   -0.45192   -0.38827   -0.25126   -0.12714\n",
            " -0.16456   -0.0213    -0.20211    0.082716   0.024684  -0.029757\n",
            " -0.64346   -0.3954     0.1412     0.3469     0.35425   -0.22096\n",
            "  0.06306    0.26536   -0.02977    0.27786    0.50287    0.12727\n",
            "  0.014117   0.25642   -0.045204   0.06792   -0.087254   0.44804\n",
            "  0.15815   -0.023931  -0.56487   -0.069249   0.095865   0.34341\n",
            " -0.017929  -0.2697    -0.42827   -0.21422    0.7592    -0.30571\n",
            " -0.37242   -0.032854  -0.20689   -0.15047    0.40052    0.087469\n",
            " -0.17631   -0.061099  -0.22828    0.13397   -0.14535   -0.24022\n",
            " -0.34951   -0.17367    0.41097   -0.34589   -0.1284     0.069622\n",
            "  0.15335   -0.21237   -0.35279   -0.5199     0.20454   -0.8524\n",
            " -0.43651   -0.20606    0.41662   -0.032318  -0.21709    0.42287\n",
            " -0.10071   -0.5374     0.014013  -0.31488    0.36917   -0.092383\n",
            " -0.0064015  0.45894    0.10606   -0.040475   0.15918    1.3422\n",
            " -0.47419   -0.12446   -0.10035   -0.17992    0.35506   -0.11075\n",
            "  0.33538    0.22004    0.12629    0.16089   -0.29405   -0.62881\n",
            "  0.54672    0.19233    0.21447    0.14141   -0.097707  -0.80096\n",
            "  0.40245    0.024641   0.3603    -0.32832   -0.10543    0.087132\n",
            "  0.25838   -0.93438   -0.071264   0.29401   -0.088105  -0.26681\n",
            " -0.19677    0.12828    0.35249   -0.05541    0.067679  -0.10851\n",
            "  1.4911     0.40962    0.03795   -0.45417    0.081169  -0.040032\n",
            "  0.31238    0.41232   -0.22689   -0.16883   -0.16087   -0.27297\n",
            " -0.75234   -0.14401   -0.12639   -0.34394   -0.39058    0.072552\n",
            "  0.25473    0.24631    0.35457    0.21095   -0.17613    0.051294\n",
            " -0.30058    0.078074  -0.097081   0.2568     0.33171    0.37339\n",
            "  0.1818    -0.16684   -0.76578   -0.092136   0.10767   -0.12707\n",
            "  1.0557    -0.67527   -0.33764    0.27431   -0.39686    0.016443\n",
            " -0.31431   -0.10314   -0.056415   0.23018   -0.073648  -0.30886\n",
            " -0.090317  -0.11952    0.29949   -0.087195   0.20275    0.21454\n",
            " -0.24295    0.048424 ]\n",
            "Corresponding embedding from embeddings list of list of lists\n",
            " [ 0.08278    0.047335  -0.38854   -0.030124  -0.25428    0.351\n",
            " -0.26002   -0.46838   -0.048807   0.99838   -0.11526    0.45842\n",
            "  0.29279   -0.058347   0.11      -0.11706   -0.28233    0.1506\n",
            " -0.31632   -0.0068872  0.38673    3.1392     0.13653   -0.03454\n",
            "  0.35863   -0.12377   -0.45192   -0.38827   -0.25126   -0.12714\n",
            " -0.16456   -0.0213    -0.20211    0.082716   0.024684  -0.029757\n",
            " -0.64346   -0.3954     0.1412     0.3469     0.35425   -0.22096\n",
            "  0.06306    0.26536   -0.02977    0.27786    0.50287    0.12727\n",
            "  0.014117   0.25642   -0.045204   0.06792   -0.087254   0.44804\n",
            "  0.15815   -0.023931  -0.56487   -0.069249   0.095865   0.34341\n",
            " -0.017929  -0.2697    -0.42827   -0.21422    0.7592    -0.30571\n",
            " -0.37242   -0.032854  -0.20689   -0.15047    0.40052    0.087469\n",
            " -0.17631   -0.061099  -0.22828    0.13397   -0.14535   -0.24022\n",
            " -0.34951   -0.17367    0.41097   -0.34589   -0.1284     0.069622\n",
            "  0.15335   -0.21237   -0.35279   -0.5199     0.20454   -0.8524\n",
            " -0.43651   -0.20606    0.41662   -0.032318  -0.21709    0.42287\n",
            " -0.10071   -0.5374     0.014013  -0.31488    0.36917   -0.092383\n",
            " -0.0064015  0.45894    0.10606   -0.040475   0.15918    1.3422\n",
            " -0.47419   -0.12446   -0.10035   -0.17992    0.35506   -0.11075\n",
            "  0.33538    0.22004    0.12629    0.16089   -0.29405   -0.62881\n",
            "  0.54672    0.19233    0.21447    0.14141   -0.097707  -0.80096\n",
            "  0.40245    0.024641   0.3603    -0.32832   -0.10543    0.087132\n",
            "  0.25838   -0.93438   -0.071264   0.29401   -0.088105  -0.26681\n",
            " -0.19677    0.12828    0.35249   -0.05541    0.067679  -0.10851\n",
            "  1.4911     0.40962    0.03795   -0.45417    0.081169  -0.040032\n",
            "  0.31238    0.41232   -0.22689   -0.16883   -0.16087   -0.27297\n",
            " -0.75234   -0.14401   -0.12639   -0.34394   -0.39058    0.072552\n",
            "  0.25473    0.24631    0.35457    0.21095   -0.17613    0.051294\n",
            " -0.30058    0.078074  -0.097081   0.2568     0.33171    0.37339\n",
            "  0.1818    -0.16684   -0.76578   -0.092136   0.10767   -0.12707\n",
            "  1.0557    -0.67527   -0.33764    0.27431   -0.39686    0.016443\n",
            " -0.31431   -0.10314   -0.056415   0.23018   -0.073648  -0.30886\n",
            " -0.090317  -0.11952    0.29949   -0.087195   0.20275    0.21454\n",
            " -0.24295    0.048424 ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wKvtW-kvO9xm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------------    \n",
        "# Make embeddings a numpy array for use in an RNN \n",
        "# Create training and test sets with Scikit Learn\n",
        "# -----------------------------------------------------\n",
        "embeddings_array = np.array(embeddings)\n",
        "\n",
        "# Define the labels to be used 500 negative (0) and 500 positive (1)\n",
        "thumbs_down_up = np.concatenate((np.zeros((500), dtype = np.int32), \n",
        "                      np.ones((500), dtype = np.int32)), axis = 0)\n",
        "\n",
        "# Scikit Learn for random splitting of the data  \n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Random splitting of the data in to training (80%) and test (20%)  \n",
        "X_train, X_test, y_train, y_test = \\\n",
        "    train_test_split(embeddings_array, thumbs_down_up, test_size=0.20, \n",
        "                     random_state = RANDOM_SEED)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bKSuqAZeUjIP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "130865a9-c11a-4fd0-f974-c7b0417bc1c0"
      },
      "cell_type": "code",
      "source": [
        "embeddings_array.shape"
      ],
      "execution_count": 276,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1000, 40, 200)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 276
        }
      ]
    },
    {
      "metadata": {
        "id": "uaOj6s83O9xn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# --------------------------------------------------------------------------      \n",
        "# We use a very simple Recurrent Neural Network for this assignment\n",
        "# Gron, A. 2017. Hands-On Machine Learning with Scikit-Learn & TensorFlow: \n",
        "#    Concepts, Tools, and Techniques to Build Intelligent Systems. \n",
        "#    Sebastopol, Calif.: O'Reilly. [ISBN-13 978-1-491-96229-9] \n",
        "#    Chapter 14 Recurrent Neural Networks, pages 390-391\n",
        "#    Source code available at https://github.com/ageron/handson-ml\n",
        "#    Jupyter notebook file 14_recurrent_neural_networks.ipynb\n",
        "#    See section on Training an sequence Classifier, # In [34]:\n",
        "#    which uses the MNIST case data...  we revise to accommodate\n",
        "#    the movie review data in this assignment    \n",
        "# --------------------------------------------------------------------------  \n",
        "reset_graph()\n",
        "\n",
        "n_steps = embeddings_array.shape[1]  # number of words per document \n",
        "n_inputs = embeddings_array.shape[2]  # dimension of  pre-trained embeddings\n",
        "n_neurons = 20  # analyst specified number of neurons\n",
        "n_outputs = 2  # thumbs-down or thumbs-up\n",
        "\n",
        "learning_rate = 0.001"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fbPV6QyBO9xo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
        "y = tf.placeholder(tf.int32, [None])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZeJEViJXO9xq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "basic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)\n",
        "outputs, states = tf.nn.dynamic_rnn(basic_cell, X, dtype=tf.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G84oelz9O9xs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "logits = tf.layers.dense(states, n_outputs)\n",
        "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,\n",
        "                                                          logits=logits)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LelNCOuSO9xt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "loss = tf.reduce_mean(xentropy)\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "training_op = optimizer.minimize(loss)\n",
        "correct = tf.nn.in_top_k(logits, y, 1)\n",
        "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cBj6juctO9xv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "init = tf.global_variables_initializer()\n",
        "\n",
        "n_epochs = 50\n",
        "batch_size = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fbNXUx6RO9x5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 11067
        },
        "outputId": "f3c316bd-18c2-4961-a071-751cf302b268"
      },
      "cell_type": "code",
      "source": [
        "with tf.Session() as sess:\n",
        "    init.run()\n",
        "    for epoch in range(n_epochs):\n",
        "        print('\\n  ---- Epoch ', epoch, ' ----\\n')\n",
        "        for iteration in range(y_train.shape[0] // batch_size):          \n",
        "            X_batch = X_train[iteration*batch_size:(iteration + 1)*batch_size,:]\n",
        "            y_batch = y_train[iteration*batch_size:(iteration + 1)*batch_size]\n",
        "            print('  Batch ', iteration, ' training observations from ',  \n",
        "                  iteration*batch_size, ' to ', (iteration + 1)*batch_size-1,)\n",
        "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
        "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
        "        acc_test = accuracy.eval(feed_dict={X: X_test, y: y_test})\n",
        "        print('\\n  Train accuracy:', acc_train, 'Test accuracy:', acc_test)\n",
        "\n",
        "\n"
      ],
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  ---- Epoch  0  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.57 Test accuracy: 0.53\n",
            "\n",
            "  ---- Epoch  1  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.61 Test accuracy: 0.565\n",
            "\n",
            "  ---- Epoch  2  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.65 Test accuracy: 0.53\n",
            "\n",
            "  ---- Epoch  3  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.65 Test accuracy: 0.54\n",
            "\n",
            "  ---- Epoch  4  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.64 Test accuracy: 0.545\n",
            "\n",
            "  ---- Epoch  5  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.69 Test accuracy: 0.54\n",
            "\n",
            "  ---- Epoch  6  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.7 Test accuracy: 0.545\n",
            "\n",
            "  ---- Epoch  7  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.72 Test accuracy: 0.575\n",
            "\n",
            "  ---- Epoch  8  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.74 Test accuracy: 0.59\n",
            "\n",
            "  ---- Epoch  9  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.77 Test accuracy: 0.585\n",
            "\n",
            "  ---- Epoch  10  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.79 Test accuracy: 0.59\n",
            "\n",
            "  ---- Epoch  11  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.79 Test accuracy: 0.6\n",
            "\n",
            "  ---- Epoch  12  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.79 Test accuracy: 0.595\n",
            "\n",
            "  ---- Epoch  13  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.81 Test accuracy: 0.6\n",
            "\n",
            "  ---- Epoch  14  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.81 Test accuracy: 0.615\n",
            "\n",
            "  ---- Epoch  15  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.81 Test accuracy: 0.6\n",
            "\n",
            "  ---- Epoch  16  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.81 Test accuracy: 0.595\n",
            "\n",
            "  ---- Epoch  17  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.82 Test accuracy: 0.59\n",
            "\n",
            "  ---- Epoch  18  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.82 Test accuracy: 0.585\n",
            "\n",
            "  ---- Epoch  19  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.82 Test accuracy: 0.6\n",
            "\n",
            "  ---- Epoch  20  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.84 Test accuracy: 0.595\n",
            "\n",
            "  ---- Epoch  21  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.85 Test accuracy: 0.6\n",
            "\n",
            "  ---- Epoch  22  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.84 Test accuracy: 0.61\n",
            "\n",
            "  ---- Epoch  23  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.85 Test accuracy: 0.605\n",
            "\n",
            "  ---- Epoch  24  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.85 Test accuracy: 0.605\n",
            "\n",
            "  ---- Epoch  25  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.86 Test accuracy: 0.61\n",
            "\n",
            "  ---- Epoch  26  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.87 Test accuracy: 0.615\n",
            "\n",
            "  ---- Epoch  27  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.87 Test accuracy: 0.615\n",
            "\n",
            "  ---- Epoch  28  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.87 Test accuracy: 0.6\n",
            "\n",
            "  ---- Epoch  29  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.88 Test accuracy: 0.6\n",
            "\n",
            "  ---- Epoch  30  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.88 Test accuracy: 0.595\n",
            "\n",
            "  ---- Epoch  31  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.88 Test accuracy: 0.585\n",
            "\n",
            "  ---- Epoch  32  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.89 Test accuracy: 0.6\n",
            "\n",
            "  ---- Epoch  33  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.89 Test accuracy: 0.6\n",
            "\n",
            "  ---- Epoch  34  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.89 Test accuracy: 0.615\n",
            "\n",
            "  ---- Epoch  35  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.91 Test accuracy: 0.605\n",
            "\n",
            "  ---- Epoch  36  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.91 Test accuracy: 0.605\n",
            "\n",
            "  ---- Epoch  37  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.88 Test accuracy: 0.615\n",
            "\n",
            "  ---- Epoch  38  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.86 Test accuracy: 0.6\n",
            "\n",
            "  ---- Epoch  39  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.88 Test accuracy: 0.58\n",
            "\n",
            "  ---- Epoch  40  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.88 Test accuracy: 0.57\n",
            "\n",
            "  ---- Epoch  41  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.92 Test accuracy: 0.59\n",
            "\n",
            "  ---- Epoch  42  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.91 Test accuracy: 0.605\n",
            "\n",
            "  ---- Epoch  43  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.92 Test accuracy: 0.615\n",
            "\n",
            "  ---- Epoch  44  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.92 Test accuracy: 0.59\n",
            "\n",
            "  ---- Epoch  45  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.93 Test accuracy: 0.59\n",
            "\n",
            "  ---- Epoch  46  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.93 Test accuracy: 0.575\n",
            "\n",
            "  ---- Epoch  47  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.93 Test accuracy: 0.58\n",
            "\n",
            "  ---- Epoch  48  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.94 Test accuracy: 0.59\n",
            "\n",
            "  ---- Epoch  49  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.94 Test accuracy: 0.59\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "eRJiuiMJQDf0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Adjust learning rate on most accuracte trial"
      ]
    },
    {
      "metadata": {
        "id": "hHNpHtpVPnyG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hlIkhKzBQZB5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# To make output stable across runs\n",
        "def reset_graph(seed= RANDOM_SEED):\n",
        "    tf.reset_default_graph()\n",
        "    tf.set_random_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "REMOVE_STOPWORDS = False  # no stopword removal \n",
        "\n",
        "EVOCABSIZE = 10000  # specify desired size of pre-defined embedding vocabulary "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CBVz9r51QZCC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------------- \n",
        "# Select the pre-defined embeddings source        \n",
        "# Define vocabulary size for the language model    \n",
        "# Create a word_to_embedding_dict for GloVe.6B.50d\n",
        "embeddings_directory = 'embeddings/glove.6B'\n",
        "filename = 'glove.6B.50d.txt'\n",
        "embeddings_filename = os.path.join(embeddings_directory, filename)\n",
        "# ------------------------------------------------------------- "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TLtmfbIAQZCI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Utility function for loading embeddings follows methods described in\n",
        "# https://github.com/guillaume-chevalier/GloVe-as-a-TensorFlow-Embedding-Layer\n",
        "# Creates the Python defaultdict dictionary word_to_embedding_dict\n",
        "# for the requested pre-trained word embeddings\n",
        "# \n",
        "# Note the use of defaultdict data structure from the Python Standard Library\n",
        "# collections_defaultdict.py lets the caller specify a default value up front\n",
        "# The default value will be retuned if the key is not a known dictionary key\n",
        "# That is, unknown words are represented by a vector of zeros\n",
        "# For word embeddings, this default value is a vector of zeros\n",
        "# Documentation for the Python standard library:\n",
        "#   Hellmann, D. 2017. The Python 3 Standard Library by Example. Boston: \n",
        "#     Addison-Wesley. [ISBN-13: 978-0-13-429105-5]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "D7u7m_EmQZCO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "ee23228c-9fcd-4285-bb56-e6a6e412188e"
      },
      "cell_type": "code",
      "source": [
        "# Utility function for loading embeddings follows methods described in\n",
        "# https://github.com/guillaume-chevalier/GloVe-as-a-TensorFlow-Embedding-Layer\n",
        "# Creates the Python defaultdict dictionary word_to_embedding_dict\n",
        "# for the requested pre-trained word embeddings\n",
        "# \n",
        "# Note the use of defaultdict data structure from the Python Standard Library\n",
        "# collections_defaultdict.py lets the caller specify a default value up front\n",
        "# The default value will be retuned if the key is not a known dictionary key\n",
        "# That is, unknown words are represented by a vector of zeros\n",
        "# For word embeddings, this default value is a vector of zeros\n",
        "# Documentation for the Python standard library:\n",
        "#   Hellmann, D. 2017. The Python 3 Standard Library by Example. Boston: \n",
        "#     Addison-Wesley. [ISBN-13: 978-0-13-429105-5]\n",
        "def load_embedding_from_disks(embeddings_filename, with_indexes=True):\n",
        "    \"\"\"\n",
        "    Read a embeddings txt file. If `with_indexes=True`, \n",
        "    we return a tuple of two dictionnaries\n",
        "    `(word_to_index_dict, index_to_embedding_array)`, \n",
        "    otherwise we return only a direct \n",
        "    `word_to_embedding_dict` dictionnary mapping \n",
        "    from a string to a numpy array.\n",
        "    \"\"\"\n",
        "    if with_indexes:\n",
        "        word_to_index_dict = dict()\n",
        "        index_to_embedding_array = []\n",
        "  \n",
        "    else:\n",
        "        word_to_embedding_dict = dict()\n",
        "\n",
        "    with open(embeddings_filename, 'r') as embeddings_file:\n",
        "        for (i, line) in enumerate(embeddings_file):\n",
        "\n",
        "            split = line.split(' ')\n",
        "\n",
        "            word = split[0]\n",
        "\n",
        "            representation = split[1:]\n",
        "            representation = np.array(\n",
        "                [float(val) for val in representation]\n",
        "            )\n",
        "\n",
        "            if with_indexes:\n",
        "                word_to_index_dict[word] = i\n",
        "                index_to_embedding_array.append(representation)\n",
        "            else:\n",
        "                word_to_embedding_dict[word] = representation\n",
        "\n",
        "    # Empty representation for unknown words.\n",
        "    _WORD_NOT_FOUND = [0.0] * len(representation)\n",
        "    if with_indexes:\n",
        "        _LAST_INDEX = i + 1\n",
        "        word_to_index_dict = defaultdict(\n",
        "            lambda: _LAST_INDEX, word_to_index_dict)\n",
        "        index_to_embedding_array = np.array(\n",
        "            index_to_embedding_array + [_WORD_NOT_FOUND])\n",
        "        return word_to_index_dict, index_to_embedding_array\n",
        "    else:\n",
        "        word_to_embedding_dict = defaultdict(lambda: _WORD_NOT_FOUND)\n",
        "        return word_to_embedding_dict\n",
        "\n",
        "print('\\nLoading embeddings from', embeddings_filename)\n",
        "word_to_index, index_to_embedding = \\\n",
        "    load_embedding_from_disks(embeddings_filename, with_indexes=True)\n",
        "print(\"Embedding loaded from disks.\")"
      ],
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Loading embeddings from embeddings/glove.6B/glove.6B.50d.txt\n",
            "Embedding loaded from disks.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "yUoUXPuaQZCV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "outputId": "7957d15a-21f5-43f0-e62e-92183c7e6040"
      },
      "cell_type": "code",
      "source": [
        "# Note: unknown words have representations with values [0, 0, ..., 0]\n",
        "\n",
        "# Additional background code from\n",
        "# https://github.com/guillaume-chevalier/GloVe-as-a-TensorFlow-Embedding-Layer\n",
        "# shows the general structure of the data structures for word embeddings\n",
        "# This code is modified for our purposes in language modeling \n",
        "vocab_size, embedding_dim = index_to_embedding.shape\n",
        "print(\"Embedding is of shape: {}\".format(index_to_embedding.shape))\n",
        "print(\"This means (number of words, number of dimensions per word)\\n\")\n",
        "print(\"The first words are words that tend occur more often.\")\n",
        "\n",
        "print(\"Note: for unknown words, the representation is an empty vector,\\n\"\n",
        "      \"and the index is the last one. The dictionnary has a limit:\")\n",
        "print(\"    {} --> {} --> {}\".format(\"A word\", \"Index in embedding\", \n",
        "      \"Representation\"))\n",
        "word = \"worsdfkljsdf\"  # a word obviously not in the vocabulary\n",
        "idx = word_to_index[word] # index for word obviously not in the vocabulary\n",
        "complete_vocabulary_size = idx \n",
        "embd = list(np.array(index_to_embedding[idx], dtype=int)) # \"int\" compact print\n",
        "print(\"    {} --> {} --> {}\".format(word, idx, embd))\n",
        "word = \"the\"\n",
        "idx = word_to_index[word]\n",
        "embd = list(index_to_embedding[idx])  # \"int\" for compact print only.\n",
        "print(\"    {} --> {} --> {}\".format(word, idx, embd))"
      ],
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Embedding is of shape: (400001, 50)\n",
            "This means (number of words, number of dimensions per word)\n",
            "\n",
            "The first words are words that tend occur more often.\n",
            "Note: for unknown words, the representation is an empty vector,\n",
            "and the index is the last one. The dictionnary has a limit:\n",
            "    A word --> Index in embedding --> Representation\n",
            "    worsdfkljsdf --> 400000 --> [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "    the --> 0 --> [0.418, 0.24968, -0.41242, 0.1217, 0.34527, -0.044457, -0.49688, -0.17862, -0.00066023, -0.6566, 0.27843, -0.14767, -0.55677, 0.14658, -0.0095095, 0.011658, 0.10204, -0.12792, -0.8443, -0.12181, -0.016801, -0.33279, -0.1552, -0.23131, -0.19181, -1.8823, -0.76746, 0.099051, -0.42125, -0.19526, 4.0071, -0.18594, -0.52287, -0.31681, 0.00059213, 0.0074449, 0.17778, -0.15897, 0.012041, -0.054223, -0.29871, -0.15749, -0.34758, -0.045637, -0.44251, 0.18785, 0.0027849, -0.18411, -0.11514, -0.78581]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "I5brMkTcQZCd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1428
        },
        "outputId": "e8870b22-eeb6-460b-e8cf-5fc5cc63c4c4"
      },
      "cell_type": "code",
      "source": [
        "# Show how to use embeddings dictionaries with a test sentence\n",
        "# This is a famous typing exercise with all letters of the alphabet\n",
        "# https://en.wikipedia.org/wiki/The_quick_brown_fox_jumps_over_the_lazy_dog\n",
        "a_typing_test_sentence = 'The quick brown fox jumps over the lazy dog'\n",
        "print('\\nTest sentence: ', a_typing_test_sentence, '\\n')\n",
        "words_in_test_sentence = a_typing_test_sentence.split()\n",
        "\n",
        "print('Test sentence embeddings from complete vocabulary of', \n",
        "      complete_vocabulary_size, 'words:\\n')\n",
        "for word in words_in_test_sentence:\n",
        "    word_ = word.lower()\n",
        "    embedding = index_to_embedding[word_to_index[word_]]\n",
        "    print(word_ + \": \", embedding)"
      ],
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test sentence:  The quick brown fox jumps over the lazy dog \n",
            "\n",
            "Test sentence embeddings from complete vocabulary of 400000 words:\n",
            "\n",
            "the:  [ 4.1800e-01  2.4968e-01 -4.1242e-01  1.2170e-01  3.4527e-01 -4.4457e-02\n",
            " -4.9688e-01 -1.7862e-01 -6.6023e-04 -6.5660e-01  2.7843e-01 -1.4767e-01\n",
            " -5.5677e-01  1.4658e-01 -9.5095e-03  1.1658e-02  1.0204e-01 -1.2792e-01\n",
            " -8.4430e-01 -1.2181e-01 -1.6801e-02 -3.3279e-01 -1.5520e-01 -2.3131e-01\n",
            " -1.9181e-01 -1.8823e+00 -7.6746e-01  9.9051e-02 -4.2125e-01 -1.9526e-01\n",
            "  4.0071e+00 -1.8594e-01 -5.2287e-01 -3.1681e-01  5.9213e-04  7.4449e-03\n",
            "  1.7778e-01 -1.5897e-01  1.2041e-02 -5.4223e-02 -2.9871e-01 -1.5749e-01\n",
            " -3.4758e-01 -4.5637e-02 -4.4251e-01  1.8785e-01  2.7849e-03 -1.8411e-01\n",
            " -1.1514e-01 -7.8581e-01]\n",
            "quick:  [ 0.13967   -0.53798   -0.18047   -0.25142    0.16203   -0.13868\n",
            " -0.24637    0.75111    0.27264    0.61035   -0.82548    0.038647\n",
            " -0.32361    0.30373   -0.14598   -0.23551    0.39267   -1.1287\n",
            " -0.23636   -1.0629     0.046277   0.29143   -0.25819   -0.094902\n",
            "  0.79478   -1.2095    -0.01039   -0.092086   0.84322   -0.11061\n",
            "  3.0096     0.51652   -0.76986    0.51074    0.37508    0.12156\n",
            "  0.082794   0.43605   -0.1584    -0.61048    0.35006    0.52465\n",
            " -0.51747    0.0034705  0.73625    0.16252    0.85279    0.85268\n",
            "  0.57892    0.64483  ]\n",
            "brown:  [-0.88497   0.71685  -0.40379  -0.10698   0.81457   1.0258   -1.2698\n",
            " -0.49382  -0.27839  -0.92251  -0.49409   0.78942  -0.20066  -0.057371\n",
            "  0.060682  0.30746   0.13441  -0.49376  -0.54788  -0.81912  -0.45394\n",
            "  0.52098   1.0325   -0.8584   -0.65848  -1.2736    0.23616   1.0486\n",
            "  0.18442  -0.3901    2.1385   -0.45301  -0.16911  -0.46737   0.15938\n",
            " -0.095071 -0.26512  -0.056479  0.63849  -1.0494    0.037507  0.76434\n",
            " -0.6412   -0.59594   0.46589   0.31494  -0.34072  -0.59167  -0.31057\n",
            "  0.73274 ]\n",
            "fox:  [ 0.44206   0.059552  0.15861   0.92777   0.1876    0.24256  -1.593\n",
            " -0.79847  -0.34099  -0.24021  -0.32756   0.43639  -0.11057   0.50472\n",
            "  0.43853   0.19738  -0.1498   -0.046979 -0.83286   0.39878   0.062174\n",
            "  0.28803   0.79134   0.31798  -0.21933  -1.1015   -0.080309  0.39122\n",
            "  0.19503  -0.5936    1.7921    0.3826   -0.30509  -0.58686  -0.76935\n",
            " -0.61914  -0.61771  -0.68484  -0.67919  -0.74626  -0.036646  0.78251\n",
            " -1.0072   -0.59057  -0.7849   -0.39113  -0.49727  -0.4283   -0.15204\n",
            "  1.5064  ]\n",
            "jumps:  [-0.46105   -0.34219    0.71473   -0.29778    0.28839    0.6248\n",
            "  0.36807   -0.072746   0.60476    0.31463   -0.052247  -0.62302\n",
            " -0.56332    0.7855     0.18116   -0.31698    0.38298   -0.081953\n",
            " -1.3658    -0.78263    0.39804   -0.17001   -0.11926   -0.40146\n",
            "  1.1057    -0.51142   -0.36614    0.22177    0.34626   -0.30648\n",
            "  1.3869     0.77328    0.5946     1.2577     0.23472   -0.46087\n",
            " -0.009223   0.44534    0.012732  -0.24749   -0.7142     0.02422\n",
            "  0.083527   0.25088   -0.24259   -1.354      1.5481    -0.31728\n",
            "  0.55305   -0.0028062]\n",
            "over:  [ 0.12972    0.088073   0.24375    0.078102  -0.12783    0.27831\n",
            " -0.48693    0.19649   -0.39558   -0.28362   -0.47425   -0.59317\n",
            " -0.58804   -0.31702    0.49593    0.0087594  0.039613  -0.42495\n",
            " -0.97641   -0.46534    0.020675   0.086042   0.39317   -0.51255\n",
            " -0.17913   -1.8333     0.5622     0.41626    0.075127   0.02189\n",
            "  3.784      0.71067   -0.073943   0.15373   -0.3853    -0.070163\n",
            " -0.35374    0.074501  -0.084228  -0.45548   -0.081068   0.39157\n",
            "  0.173      0.2254    -0.12836    0.40951   -0.26079    0.090912\n",
            " -0.60515   -0.9827   ]\n",
            "the:  [ 4.1800e-01  2.4968e-01 -4.1242e-01  1.2170e-01  3.4527e-01 -4.4457e-02\n",
            " -4.9688e-01 -1.7862e-01 -6.6023e-04 -6.5660e-01  2.7843e-01 -1.4767e-01\n",
            " -5.5677e-01  1.4658e-01 -9.5095e-03  1.1658e-02  1.0204e-01 -1.2792e-01\n",
            " -8.4430e-01 -1.2181e-01 -1.6801e-02 -3.3279e-01 -1.5520e-01 -2.3131e-01\n",
            " -1.9181e-01 -1.8823e+00 -7.6746e-01  9.9051e-02 -4.2125e-01 -1.9526e-01\n",
            "  4.0071e+00 -1.8594e-01 -5.2287e-01 -3.1681e-01  5.9213e-04  7.4449e-03\n",
            "  1.7778e-01 -1.5897e-01  1.2041e-02 -5.4223e-02 -2.9871e-01 -1.5749e-01\n",
            " -3.4758e-01 -4.5637e-02 -4.4251e-01  1.8785e-01  2.7849e-03 -1.8411e-01\n",
            " -1.1514e-01 -7.8581e-01]\n",
            "lazy:  [-0.27611  -0.59712  -0.49227  -1.0372   -0.35878  -0.097425 -0.21014\n",
            " -0.092836 -0.054118  0.4542   -0.53296   0.37602   0.77087   0.79669\n",
            " -0.076608 -0.42515   0.42576   0.32791  -0.21996  -0.20261  -0.85139\n",
            "  0.80547   0.97621   0.9792    1.1118   -0.36062  -0.2588    0.8596\n",
            "  0.73631  -0.18601   1.2376   -0.038938  0.19246   0.52473  -0.04842\n",
            " -0.044149  0.064432  0.087822  0.42232  -0.55991  -0.44096   0.097736\n",
            " -0.17589   1.1799    0.13152  -1.0795    0.45685  -0.63312   1.2752\n",
            "  1.1672  ]\n",
            "dog:  [ 0.11008   -0.38781   -0.57615   -0.27714    0.70521    0.53994\n",
            " -1.0786    -0.40146    1.1504    -0.5678     0.0038977  0.52878\n",
            "  0.64561    0.47262    0.48549   -0.18407    0.1801     0.91397\n",
            " -1.1979    -0.5778    -0.37985    0.33606    0.772      0.75555\n",
            "  0.45506   -1.7671    -1.0503     0.42566    0.41893   -0.68327\n",
            "  1.5673     0.27685   -0.61708    0.64638   -0.076996   0.37118\n",
            "  0.1308    -0.45137    0.25398   -0.74392   -0.086199   0.24068\n",
            " -0.64819    0.83549    1.2502    -0.51379    0.04224   -0.88118\n",
            "  0.7158     0.38519  ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "iSjp7enhQZCx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------------- \n",
        "# Define vocabulary size for the language model    \n",
        "# To reduce the size of the vocabulary to the n most frequently used words\n",
        "\n",
        "def default_factory():\n",
        "    return EVOCABSIZE  # last/unknown-word row in limited_index_to_embedding\n",
        "# dictionary has the items() function, returns list of (key, value) tuples\n",
        "limited_word_to_index = defaultdict(default_factory, \\\n",
        "    {k: v for k, v in word_to_index.items() if v < EVOCABSIZE})\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EL8YkTzXQZCy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Select the first EVOCABSIZE rows to the index_to_embedding\n",
        "limited_index_to_embedding = index_to_embedding[0:EVOCABSIZE,:]\n",
        "# Set the unknown-word row to be all zeros as previously\n",
        "limited_index_to_embedding = np.append(limited_index_to_embedding, \n",
        "    index_to_embedding[index_to_embedding.shape[0] - 1, :].\\\n",
        "        reshape(1,embedding_dim), \n",
        "    axis = 0)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1FLMl_5lQZC0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Delete large numpy array to clear some CPU RAM\n",
        "del index_to_embedding"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "COitCDAOQZC1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1207
        },
        "outputId": "9bdb493c-b1ff-4bff-8367-64667b83d8c9"
      },
      "cell_type": "code",
      "source": [
        "# Verify the new vocabulary: should get same embeddings for test sentence\n",
        "# Note that a small EVOCABSIZE may yield some zero vectors for embeddings\n",
        "print('\\nTest sentence embeddings from vocabulary of', EVOCABSIZE, 'words:\\n')\n",
        "for word in words_in_test_sentence:\n",
        "    word_ = word.lower()\n",
        "    embedding = limited_index_to_embedding[limited_word_to_index[word_]]\n",
        "    print(word_ + \": \", embedding)\n",
        "\n"
      ],
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test sentence embeddings from vocabulary of 10000 words:\n",
            "\n",
            "the:  [ 4.1800e-01  2.4968e-01 -4.1242e-01  1.2170e-01  3.4527e-01 -4.4457e-02\n",
            " -4.9688e-01 -1.7862e-01 -6.6023e-04 -6.5660e-01  2.7843e-01 -1.4767e-01\n",
            " -5.5677e-01  1.4658e-01 -9.5095e-03  1.1658e-02  1.0204e-01 -1.2792e-01\n",
            " -8.4430e-01 -1.2181e-01 -1.6801e-02 -3.3279e-01 -1.5520e-01 -2.3131e-01\n",
            " -1.9181e-01 -1.8823e+00 -7.6746e-01  9.9051e-02 -4.2125e-01 -1.9526e-01\n",
            "  4.0071e+00 -1.8594e-01 -5.2287e-01 -3.1681e-01  5.9213e-04  7.4449e-03\n",
            "  1.7778e-01 -1.5897e-01  1.2041e-02 -5.4223e-02 -2.9871e-01 -1.5749e-01\n",
            " -3.4758e-01 -4.5637e-02 -4.4251e-01  1.8785e-01  2.7849e-03 -1.8411e-01\n",
            " -1.1514e-01 -7.8581e-01]\n",
            "quick:  [ 0.13967   -0.53798   -0.18047   -0.25142    0.16203   -0.13868\n",
            " -0.24637    0.75111    0.27264    0.61035   -0.82548    0.038647\n",
            " -0.32361    0.30373   -0.14598   -0.23551    0.39267   -1.1287\n",
            " -0.23636   -1.0629     0.046277   0.29143   -0.25819   -0.094902\n",
            "  0.79478   -1.2095    -0.01039   -0.092086   0.84322   -0.11061\n",
            "  3.0096     0.51652   -0.76986    0.51074    0.37508    0.12156\n",
            "  0.082794   0.43605   -0.1584    -0.61048    0.35006    0.52465\n",
            " -0.51747    0.0034705  0.73625    0.16252    0.85279    0.85268\n",
            "  0.57892    0.64483  ]\n",
            "brown:  [-0.88497   0.71685  -0.40379  -0.10698   0.81457   1.0258   -1.2698\n",
            " -0.49382  -0.27839  -0.92251  -0.49409   0.78942  -0.20066  -0.057371\n",
            "  0.060682  0.30746   0.13441  -0.49376  -0.54788  -0.81912  -0.45394\n",
            "  0.52098   1.0325   -0.8584   -0.65848  -1.2736    0.23616   1.0486\n",
            "  0.18442  -0.3901    2.1385   -0.45301  -0.16911  -0.46737   0.15938\n",
            " -0.095071 -0.26512  -0.056479  0.63849  -1.0494    0.037507  0.76434\n",
            " -0.6412   -0.59594   0.46589   0.31494  -0.34072  -0.59167  -0.31057\n",
            "  0.73274 ]\n",
            "fox:  [ 0.44206   0.059552  0.15861   0.92777   0.1876    0.24256  -1.593\n",
            " -0.79847  -0.34099  -0.24021  -0.32756   0.43639  -0.11057   0.50472\n",
            "  0.43853   0.19738  -0.1498   -0.046979 -0.83286   0.39878   0.062174\n",
            "  0.28803   0.79134   0.31798  -0.21933  -1.1015   -0.080309  0.39122\n",
            "  0.19503  -0.5936    1.7921    0.3826   -0.30509  -0.58686  -0.76935\n",
            " -0.61914  -0.61771  -0.68484  -0.67919  -0.74626  -0.036646  0.78251\n",
            " -1.0072   -0.59057  -0.7849   -0.39113  -0.49727  -0.4283   -0.15204\n",
            "  1.5064  ]\n",
            "jumps:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0.]\n",
            "over:  [ 0.12972    0.088073   0.24375    0.078102  -0.12783    0.27831\n",
            " -0.48693    0.19649   -0.39558   -0.28362   -0.47425   -0.59317\n",
            " -0.58804   -0.31702    0.49593    0.0087594  0.039613  -0.42495\n",
            " -0.97641   -0.46534    0.020675   0.086042   0.39317   -0.51255\n",
            " -0.17913   -1.8333     0.5622     0.41626    0.075127   0.02189\n",
            "  3.784      0.71067   -0.073943   0.15373   -0.3853    -0.070163\n",
            " -0.35374    0.074501  -0.084228  -0.45548   -0.081068   0.39157\n",
            "  0.173      0.2254    -0.12836    0.40951   -0.26079    0.090912\n",
            " -0.60515   -0.9827   ]\n",
            "the:  [ 4.1800e-01  2.4968e-01 -4.1242e-01  1.2170e-01  3.4527e-01 -4.4457e-02\n",
            " -4.9688e-01 -1.7862e-01 -6.6023e-04 -6.5660e-01  2.7843e-01 -1.4767e-01\n",
            " -5.5677e-01  1.4658e-01 -9.5095e-03  1.1658e-02  1.0204e-01 -1.2792e-01\n",
            " -8.4430e-01 -1.2181e-01 -1.6801e-02 -3.3279e-01 -1.5520e-01 -2.3131e-01\n",
            " -1.9181e-01 -1.8823e+00 -7.6746e-01  9.9051e-02 -4.2125e-01 -1.9526e-01\n",
            "  4.0071e+00 -1.8594e-01 -5.2287e-01 -3.1681e-01  5.9213e-04  7.4449e-03\n",
            "  1.7778e-01 -1.5897e-01  1.2041e-02 -5.4223e-02 -2.9871e-01 -1.5749e-01\n",
            " -3.4758e-01 -4.5637e-02 -4.4251e-01  1.8785e-01  2.7849e-03 -1.8411e-01\n",
            " -1.1514e-01 -7.8581e-01]\n",
            "lazy:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0.]\n",
            "dog:  [ 0.11008   -0.38781   -0.57615   -0.27714    0.70521    0.53994\n",
            " -1.0786    -0.40146    1.1504    -0.5678     0.0038977  0.52878\n",
            "  0.64561    0.47262    0.48549   -0.18407    0.1801     0.91397\n",
            " -1.1979    -0.5778    -0.37985    0.33606    0.772      0.75555\n",
            "  0.45506   -1.7671    -1.0503     0.42566    0.41893   -0.68327\n",
            "  1.5673     0.27685   -0.61708    0.64638   -0.076996   0.37118\n",
            "  0.1308    -0.45137    0.25398   -0.74392   -0.086199   0.24068\n",
            " -0.64819    0.83549    1.2502    -0.51379    0.04224   -0.88118\n",
            "  0.7158     0.38519  ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wZewOVO6QZC9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------------\n",
        "# code for working with movie reviews data \n",
        "# Source: Miller, T. W. (2016). Web and Network Data Science.\n",
        "#    Upper Saddle River, N.J.: Pearson Education.\n",
        "#    ISBN-13: 978-0-13-388644-3\n",
        "# This original study used a simple bag-of-words approach\n",
        "# to sentiment analysis, along with pre-defined lists of\n",
        "# negative and positive words.        \n",
        "# Code available at:  https://github.com/mtpa/wnds       \n",
        "# ------------------------------------------------------------"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "684PqBNgQZC_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Utility function to get file names within a directory\n",
        "def listdir_no_hidden(path):\n",
        "    start_list = os.listdir(path)\n",
        "    end_list = []\n",
        "    for file in start_list:\n",
        "        if (not file.startswith('.')):\n",
        "            end_list.append(file)\n",
        "    return(end_list)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UilCr0IJQZDD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# define list of codes to be dropped from document\n",
        "# carriage-returns, line-feeds, tabs\n",
        "codelist = ['\\r', '\\n', '\\t']   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "s6Iu3qCEQZDG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# We will not remove stopwords in this exercise because they are\n",
        "# important to keeping sentences intact\n",
        "if REMOVE_STOPWORDS:\n",
        "    print(nltk.corpus.stopwords.words('english'))\n",
        "\n",
        "# previous analysis of a list of top terms showed a number of words, along \n",
        "# with contractions and other word strings to drop from further analysis, add\n",
        "# these to the usual English stopwords to be dropped from a document collection\n",
        "    more_stop_words = ['cant','didnt','doesnt','dont','goes','isnt','hes',\\\n",
        "        'shes','thats','theres','theyre','wont','youll','youre','youve', 'br'\\\n",
        "        've', 're', 'vs'] \n",
        "\n",
        "    some_proper_nouns_to_remove = ['dick','ginger','hollywood','jack',\\\n",
        "        'jill','john','karloff','kudrow','orson','peter','tcm','tom',\\\n",
        "        'toni','welles','william','wolheim','nikita']\n",
        "\n",
        "    # start with the initial list and add to it for movie text work \n",
        "    stoplist = nltk.corpus.stopwords.words('english') + more_stop_words +\\\n",
        "        some_proper_nouns_to_remove"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3DrEOOENQZDI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# text parsing function for creating text documents \n",
        "# there is more we could do for data preparation \n",
        "# stemming... looking for contractions... possessives... \n",
        "# but we will work with what we have in this parsing function\n",
        "# if we want to do stemming at a later time, we can use\n",
        "#     porter = nltk.PorterStemmer()  \n",
        "# in a construction like this\n",
        "#     words_stemmed =  [porter.stem(word) for word in initial_words]  \n",
        "def text_parse(string):\n",
        "    # replace non-alphanumeric with space \n",
        "    temp_string = re.sub('[^a-zA-Z]', '  ', string)    \n",
        "    # replace codes with space\n",
        "    for i in range(len(codelist)):\n",
        "        stopstring = ' ' + codelist[i] + '  '\n",
        "        temp_string = re.sub(stopstring, '  ', temp_string)      \n",
        "    # replace single-character words with space\n",
        "    temp_string = re.sub('\\s.\\s', ' ', temp_string)   \n",
        "    # convert uppercase to lowercase\n",
        "    temp_string = temp_string.lower()    \n",
        "    if REMOVE_STOPWORDS:\n",
        "        # replace selected character strings/stop-words with space\n",
        "        for i in range(len(stoplist)):\n",
        "            stopstring = ' ' + str(stoplist[i]) + ' '\n",
        "            temp_string = re.sub(stopstring, ' ', temp_string)        \n",
        "    # replace multiple blank characters with one blank character\n",
        "    temp_string = re.sub('\\s+', ' ', temp_string)    \n",
        "    return(temp_string)    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NRaT0cwmQZDL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Manually import these"
      ]
    },
    {
      "metadata": {
        "id": "NieD9kM5QZDi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "fb15e964-648e-4d9f-c0b2-4dc14790ff8e"
      },
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------\n",
        "# gather data for 500 negative movie reviews\n",
        "# -----------------------------------------------\n",
        "dir_name = 'movie-reviews-negative'\n",
        "    \n",
        "filenames = listdir_no_hidden(path=dir_name)\n",
        "num_files = len(filenames)\n",
        "\n",
        "for i in range(len(filenames)):\n",
        "    file_exists = os.path.isfile(os.path.join(dir_name, filenames[i]))\n",
        "    assert file_exists\n",
        "print('\\nDirectory:',dir_name)    \n",
        "print('%d files found' % len(filenames))"
      ],
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Directory: movie-reviews-negative\n",
            "500 files found\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "WgYTdbJLQZDr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "3badc723-dec5-4ea0-d53a-ffb56c934d21"
      },
      "cell_type": "code",
      "source": [
        "# Read data for negative movie reviews\n",
        "# Data will be stored in a list of lists where the each list represents \n",
        "# a document and document is a list of words.\n",
        "# We then break the text into words.\n",
        "\n",
        "def read_data(filename):\n",
        "\n",
        "  #with open(filename, encoding='utf-8') as f:\n",
        "  with open(filename) as f:\n",
        "    data = tf.compat.as_str(f.read())\n",
        "    data = data.lower()\n",
        "    data = text_parse(data)\n",
        "    data = TreebankWordTokenizer().tokenize(data)  # The Penn Treebank\n",
        "\n",
        "  return data\n",
        "\n",
        "negative_documents = []\n",
        "\n",
        "print('\\nProcessing document files under', dir_name)\n",
        "for i in range(num_files):\n",
        "    ## print(' ', filenames[i])\n",
        "\n",
        "    words = read_data(os.path.join(dir_name, filenames[i]))\n",
        "\n",
        "    negative_documents.append(words)\n",
        "    # print('Data size (Characters) (Document %d) %d' %(i,len(words)))\n",
        "    # print('Sample string (Document %d) %s'%(i,words[:50]))"
      ],
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Processing document files under movie-reviews-negative\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cyVYiEFXQZD7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "a8197f14-7474-41d8-e117-81e989239b26"
      },
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------\n",
        "# gather data for 500 positive movie reviews\n",
        "# -----------------------------------------------\n",
        "dir_name = 'movie-reviews-positive'  \n",
        "filenames = listdir_no_hidden(path=dir_name)\n",
        "num_files = len(filenames)\n",
        "\n",
        "for i in range(len(filenames)):\n",
        "    file_exists = os.path.isfile(os.path.join(dir_name, filenames[i]))\n",
        "    assert file_exists\n",
        "print('\\nDirectory:',dir_name)    \n",
        "print('%d files found' % len(filenames))"
      ],
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Directory: movie-reviews-positive\n",
            "500 files found\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "l1jW5imQQZD9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "7b1d8426-f554-4dc0-fad7-441173295645"
      },
      "cell_type": "code",
      "source": [
        "# Read data for positive movie reviews\n",
        "# Data will be stored in a list of lists where the each list \n",
        "# represents a document and document is a list of words.\n",
        "# We then break the text into words.\n",
        "\n",
        "def read_data(filename):\n",
        "\n",
        "  with open(filename) as f:\n",
        "    data = tf.compat.as_str(f.read())\n",
        "    data = data.lower()\n",
        "    data = text_parse(data)\n",
        "    data = TreebankWordTokenizer().tokenize(data)  # The Penn Treebank\n",
        "\n",
        "  return data\n",
        "\n",
        "positive_documents = []\n",
        "\n",
        "print('\\nProcessing document files under', dir_name)\n",
        "for i in range(num_files):\n",
        "    ## print(' ', filenames[i])\n",
        "\n",
        "    words = read_data(os.path.join(dir_name, filenames[i]))\n",
        "\n",
        "    positive_documents.append(words)\n",
        "    # print('Data size (Characters) (Document %d) %d' %(i,len(words)))\n",
        "    # print('Sample string (Document %d) %s'%(i,words[:50]))\n",
        "\n"
      ],
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Processing document files under movie-reviews-positive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hE75m5h7QZEA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "55eafbb4-552f-4ab4-d472-69b344a8a3d9"
      },
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------------\n",
        "# convert positive/negative documents into numpy array\n",
        "# note that reviews vary from 22 to 1052 words   \n",
        "# so we use the first 20 and last 20 words of each review \n",
        "# as our word sequences for analysis\n",
        "# -----------------------------------------------------\n",
        "max_review_length = 0  # initialize\n",
        "for doc in negative_documents:\n",
        "    max_review_length = max(max_review_length, len(doc))    \n",
        "for doc in positive_documents:\n",
        "    max_review_length = max(max_review_length, len(doc)) \n",
        "print('max_review_length:', max_review_length) \n",
        "\n",
        "min_review_length = max_review_length  # initialize\n",
        "for doc in negative_documents:\n",
        "    min_review_length = min(min_review_length, len(doc))    \n",
        "for doc in positive_documents:\n",
        "    min_review_length = min(min_review_length, len(doc)) \n",
        "print('min_review_length:', min_review_length) \n",
        "\n"
      ],
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "max_review_length: 1052\n",
            "min_review_length: 22\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "iJbgHqbwQZEF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# construct list of 1000 lists with 40 words in each list\n",
        "from itertools import chain\n",
        "documents = []\n",
        "for doc in negative_documents:\n",
        "    doc_begin = doc[0:20]\n",
        "    doc_end = doc[len(doc) - 20: len(doc)]\n",
        "    documents.append(list(chain(*[doc_begin, doc_end])))    \n",
        "for doc in positive_documents:\n",
        "    doc_begin = doc[0:20]\n",
        "    doc_end = doc[len(doc) - 20: len(doc)]\n",
        "    documents.append(list(chain(*[doc_begin, doc_end])))    \n",
        "\n",
        "# create list of lists of lists for embeddings\n",
        "embeddings = []    \n",
        "for doc in documents:\n",
        "    embedding = []\n",
        "    for word in doc:\n",
        "       embedding.append(limited_index_to_embedding[limited_word_to_index[word]]) \n",
        "    embeddings.append(embedding)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NfdtGkKtQZEK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "0a1dbaa6-cc43-4a1e-8e79-d67049cac9eb"
      },
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------------    \n",
        "# Check on the embeddings list of list of lists \n",
        "# -----------------------------------------------------\n",
        "# Show the first word in the first document\n",
        "test_word = documents[0][0]    \n",
        "print('First word in first document:', test_word)    \n",
        "print('Embedding for this word:\\n', \n",
        "      limited_index_to_embedding[limited_word_to_index[test_word]])\n",
        "print('Corresponding embedding from embeddings list of list of lists\\n',\n",
        "      embeddings[0][0][:])\n",
        "\n"
      ],
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First word in first document: strained\n",
            "Embedding for this word:\n",
            " [ 1.8565e-01  4.5892e-02 -7.9469e-01  4.5509e-02 -3.4420e-01  1.3177e+00\n",
            " -1.8251e-02  9.0133e-01 -2.4033e-01 -2.2524e-01  6.2932e-01 -9.8314e-04\n",
            " -1.0314e+00  3.3354e-01 -3.5601e-02  7.4361e-01 -1.3273e+00 -6.8377e-01\n",
            "  1.0453e+00  9.3062e-01 -9.2818e-01  5.0075e-01  5.6506e-02 -4.6347e-01\n",
            " -1.8777e-01 -1.1040e+00  1.6604e+00  1.1720e+00  8.9319e-01  3.9953e-01\n",
            "  2.0753e+00  7.9695e-01  1.2480e+00 -1.4327e-02 -5.8202e-01  5.8175e-01\n",
            " -5.3769e-01  4.2895e-01 -3.3331e-01 -1.1000e-01 -6.3102e-01 -2.1928e-02\n",
            " -1.9526e-01 -1.3278e-01  4.0494e-01  3.5085e-01 -1.3195e-01  3.3285e-01\n",
            " -1.0968e+00 -3.1978e-01]\n",
            "Corresponding embedding from embeddings list of list of lists\n",
            " [ 1.8565e-01  4.5892e-02 -7.9469e-01  4.5509e-02 -3.4420e-01  1.3177e+00\n",
            " -1.8251e-02  9.0133e-01 -2.4033e-01 -2.2524e-01  6.2932e-01 -9.8314e-04\n",
            " -1.0314e+00  3.3354e-01 -3.5601e-02  7.4361e-01 -1.3273e+00 -6.8377e-01\n",
            "  1.0453e+00  9.3062e-01 -9.2818e-01  5.0075e-01  5.6506e-02 -4.6347e-01\n",
            " -1.8777e-01 -1.1040e+00  1.6604e+00  1.1720e+00  8.9319e-01  3.9953e-01\n",
            "  2.0753e+00  7.9695e-01  1.2480e+00 -1.4327e-02 -5.8202e-01  5.8175e-01\n",
            " -5.3769e-01  4.2895e-01 -3.3331e-01 -1.1000e-01 -6.3102e-01 -2.1928e-02\n",
            " -1.9526e-01 -1.3278e-01  4.0494e-01  3.5085e-01 -1.3195e-01  3.3285e-01\n",
            " -1.0968e+00 -3.1978e-01]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5jUofdUFQZEP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "9d480a27-3d4a-47b0-9013-7544630009e3"
      },
      "cell_type": "code",
      "source": [
        "# Show the seventh word in the tenth document\n",
        "test_word = documents[6][9]    \n",
        "print('First word in first document:', test_word)    \n",
        "print('Embedding for this word:\\n', \n",
        "      limited_index_to_embedding[limited_word_to_index[test_word]])\n",
        "print('Corresponding embedding from embeddings list of list of lists\\n',\n",
        "      embeddings[6][9][:])\n",
        "\n"
      ],
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First word in first document: ever\n",
            "Embedding for this word:\n",
            " [-9.1840e-02  4.5829e-02 -1.4578e-02 -1.5912e-01  3.6966e-01 -5.5325e-02\n",
            " -6.4645e-01  3.7028e-01  4.4005e-02  3.1908e-01 -2.2221e-03  5.4468e-02\n",
            " -5.5971e-01  3.2805e-01  1.2631e+00 -2.4974e-02  4.1510e-01  4.1472e-01\n",
            " -8.0000e-01 -3.6899e-01 -2.6027e-01 -1.6870e-03  1.9953e-01  7.3913e-02\n",
            "  5.8280e-01 -2.0068e+00 -1.0067e+00  3.4075e-02  4.1367e-01 -7.2996e-02\n",
            "  2.9454e+00  2.7576e-01  1.0770e-01 -4.0572e-01  1.1923e-03  6.1022e-02\n",
            "  6.9987e-02  5.2403e-01 -7.6733e-01 -9.3638e-01 -8.1252e-01 -1.1230e-01\n",
            "  1.0376e-01  2.9453e-01 -2.7063e-01 -1.2739e-02 -3.1710e-01 -3.3740e-02\n",
            " -3.9644e-01 -1.9175e-02]\n",
            "Corresponding embedding from embeddings list of list of lists\n",
            " [-9.1840e-02  4.5829e-02 -1.4578e-02 -1.5912e-01  3.6966e-01 -5.5325e-02\n",
            " -6.4645e-01  3.7028e-01  4.4005e-02  3.1908e-01 -2.2221e-03  5.4468e-02\n",
            " -5.5971e-01  3.2805e-01  1.2631e+00 -2.4974e-02  4.1510e-01  4.1472e-01\n",
            " -8.0000e-01 -3.6899e-01 -2.6027e-01 -1.6870e-03  1.9953e-01  7.3913e-02\n",
            "  5.8280e-01 -2.0068e+00 -1.0067e+00  3.4075e-02  4.1367e-01 -7.2996e-02\n",
            "  2.9454e+00  2.7576e-01  1.0770e-01 -4.0572e-01  1.1923e-03  6.1022e-02\n",
            "  6.9987e-02  5.2403e-01 -7.6733e-01 -9.3638e-01 -8.1252e-01 -1.1230e-01\n",
            "  1.0376e-01  2.9453e-01 -2.7063e-01 -1.2739e-02 -3.1710e-01 -3.3740e-02\n",
            " -3.9644e-01 -1.9175e-02]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "RSHKJFIzQZEb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "a84dbe1b-b4a3-4c84-d611-f6b51d964003"
      },
      "cell_type": "code",
      "source": [
        "# Show the last word in the last document\n",
        "test_word = documents[999][39]    \n",
        "print('First word in first document:', test_word)    \n",
        "print('Embedding for this word:\\n', \n",
        "      limited_index_to_embedding[limited_word_to_index[test_word]])\n",
        "print('Corresponding embedding from embeddings list of list of lists\\n',\n",
        "      embeddings[999][39][:])        \n",
        "\n"
      ],
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First word in first document: out\n",
            "Embedding for this word:\n",
            " [ 3.2112e-01 -6.9306e-01  4.7922e-01 -5.4602e-01  2.8352e-01  2.0346e-01\n",
            " -9.8445e-01 -1.4103e-01 -1.3147e-01 -8.5975e-02 -4.9509e-01  2.7600e-03\n",
            " -1.1173e+00  3.3729e-01  6.1312e-01 -6.7110e-02  3.5380e-01 -3.5183e-01\n",
            " -5.8191e-01 -6.9525e-01 -2.5032e-02  6.1675e-01  7.8522e-01 -1.9594e-01\n",
            "  2.6324e-01 -1.8976e+00  1.4645e-01  4.8885e-01  6.1818e-01 -1.0120e+00\n",
            "  3.7285e+00  6.6615e-01 -3.3364e-01  3.1896e-01 -1.5174e-01  3.0980e-01\n",
            "  4.9670e-02  2.7144e-01  3.4595e-01 -8.1850e-02 -3.7469e-01  3.9981e-01\n",
            "  8.4925e-02  3.1237e-01 -1.2677e-01  3.6322e-02 -6.9533e-02 -4.3547e-01\n",
            " -1.1080e-01 -5.8500e-01]\n",
            "Corresponding embedding from embeddings list of list of lists\n",
            " [ 3.2112e-01 -6.9306e-01  4.7922e-01 -5.4602e-01  2.8352e-01  2.0346e-01\n",
            " -9.8445e-01 -1.4103e-01 -1.3147e-01 -8.5975e-02 -4.9509e-01  2.7600e-03\n",
            " -1.1173e+00  3.3729e-01  6.1312e-01 -6.7110e-02  3.5380e-01 -3.5183e-01\n",
            " -5.8191e-01 -6.9525e-01 -2.5032e-02  6.1675e-01  7.8522e-01 -1.9594e-01\n",
            "  2.6324e-01 -1.8976e+00  1.4645e-01  4.8885e-01  6.1818e-01 -1.0120e+00\n",
            "  3.7285e+00  6.6615e-01 -3.3364e-01  3.1896e-01 -1.5174e-01  3.0980e-01\n",
            "  4.9670e-02  2.7144e-01  3.4595e-01 -8.1850e-02 -3.7469e-01  3.9981e-01\n",
            "  8.4925e-02  3.1237e-01 -1.2677e-01  3.6322e-02 -6.9533e-02 -4.3547e-01\n",
            " -1.1080e-01 -5.8500e-01]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "b_0DEDmFQZEj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------------    \n",
        "# Make embeddings a numpy array for use in an RNN \n",
        "# Create training and test sets with Scikit Learn\n",
        "# -----------------------------------------------------\n",
        "embeddings_array = np.array(embeddings)\n",
        "\n",
        "# Define the labels to be used 500 negative (0) and 500 positive (1)\n",
        "thumbs_down_up = np.concatenate((np.zeros((500), dtype = np.int32), \n",
        "                      np.ones((500), dtype = np.int32)), axis = 0)\n",
        "\n",
        "# Scikit Learn for random splitting of the data  \n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Random splitting of the data in to training (80%) and test (20%)  \n",
        "X_train, X_test, y_train, y_test = \\\n",
        "    train_test_split(embeddings_array, thumbs_down_up, test_size=0.20, \n",
        "                     random_state = RANDOM_SEED)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Pc1jOEguQZEl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# --------------------------------------------------------------------------      \n",
        "# We use a very simple Recurrent Neural Network for this assignment\n",
        "# Gron, A. 2017. Hands-On Machine Learning with Scikit-Learn & TensorFlow: \n",
        "#    Concepts, Tools, and Techniques to Build Intelligent Systems. \n",
        "#    Sebastopol, Calif.: O'Reilly. [ISBN-13 978-1-491-96229-9] \n",
        "#    Chapter 14 Recurrent Neural Networks, pages 390-391\n",
        "#    Source code available at https://github.com/ageron/handson-ml\n",
        "#    Jupyter notebook file 14_recurrent_neural_networks.ipynb\n",
        "#    See section on Training an sequence Classifier, # In [34]:\n",
        "#    which uses the MNIST case data...  we revise to accommodate\n",
        "#    the movie review data in this assignment    \n",
        "# --------------------------------------------------------------------------  \n",
        "reset_graph()\n",
        "\n",
        "n_steps = embeddings_array.shape[1]  # number of words per document \n",
        "n_inputs = embeddings_array.shape[2]  # dimension of  pre-trained embeddings\n",
        "n_neurons = 20  # analyst specified number of neurons\n",
        "n_outputs = 2  # thumbs-down or thumbs-up\n",
        "\n",
        "learning_rate = 1.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "p7Ao50tMQZEo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
        "y = tf.placeholder(tf.int32, [None])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mggfKadzQZEp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "basic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)\n",
        "outputs, states = tf.nn.dynamic_rnn(basic_cell, X, dtype=tf.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ReRhik7-QZEr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "logits = tf.layers.dense(states, n_outputs)\n",
        "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,\n",
        "                                                          logits=logits)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5O95GJEtQZEr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "loss = tf.reduce_mean(xentropy)\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "training_op = optimizer.minimize(loss)\n",
        "correct = tf.nn.in_top_k(logits, y, 1)\n",
        "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BysDQhDTQZEt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "init = tf.global_variables_initializer()\n",
        "\n",
        "n_epochs = 50\n",
        "batch_size = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SE0BOFJhQZEu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 11067
        },
        "outputId": "b71e4704-2176-4fa5-f4bf-b075da10e17d"
      },
      "cell_type": "code",
      "source": [
        "with tf.Session() as sess:\n",
        "    init.run()\n",
        "    for epoch in range(n_epochs):\n",
        "        print('\\n  ---- Epoch ', epoch, ' ----\\n')\n",
        "        for iteration in range(y_train.shape[0] // batch_size):          \n",
        "            X_batch = X_train[iteration*batch_size:(iteration + 1)*batch_size,:]\n",
        "            y_batch = y_train[iteration*batch_size:(iteration + 1)*batch_size]\n",
        "            print('  Batch ', iteration, ' training observations from ',  \n",
        "                  iteration*batch_size, ' to ', (iteration + 1)*batch_size-1,)\n",
        "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
        "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
        "        acc_test = accuracy.eval(feed_dict={X: X_test, y: y_test})\n",
        "        print('\\n  Train accuracy:', acc_train, 'Test accuracy:', acc_test)\n",
        "\n",
        "\n"
      ],
      "execution_count": 189,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  ---- Epoch  0  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.52 Test accuracy: 0.48\n",
            "\n",
            "  ---- Epoch  1  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.58 Test accuracy: 0.51\n",
            "\n",
            "  ---- Epoch  2  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.55 Test accuracy: 0.51\n",
            "\n",
            "  ---- Epoch  3  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.5 Test accuracy: 0.5\n",
            "\n",
            "  ---- Epoch  4  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.53 Test accuracy: 0.5\n",
            "\n",
            "  ---- Epoch  5  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.49 Test accuracy: 0.475\n",
            "\n",
            "  ---- Epoch  6  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.52 Test accuracy: 0.535\n",
            "\n",
            "  ---- Epoch  7  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.5 Test accuracy: 0.545\n",
            "\n",
            "  ---- Epoch  8  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.53 Test accuracy: 0.5\n",
            "\n",
            "  ---- Epoch  9  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.58 Test accuracy: 0.48\n",
            "\n",
            "  ---- Epoch  10  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.51 Test accuracy: 0.515\n",
            "\n",
            "  ---- Epoch  11  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.6 Test accuracy: 0.545\n",
            "\n",
            "  ---- Epoch  12  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.49 Test accuracy: 0.465\n",
            "\n",
            "  ---- Epoch  13  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.57 Test accuracy: 0.5\n",
            "\n",
            "  ---- Epoch  14  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.53 Test accuracy: 0.52\n",
            "\n",
            "  ---- Epoch  15  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.52 Test accuracy: 0.54\n",
            "\n",
            "  ---- Epoch  16  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.49 Test accuracy: 0.465\n",
            "\n",
            "  ---- Epoch  17  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.52 Test accuracy: 0.51\n",
            "\n",
            "  ---- Epoch  18  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.5 Test accuracy: 0.465\n",
            "\n",
            "  ---- Epoch  19  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.52 Test accuracy: 0.49\n",
            "\n",
            "  ---- Epoch  20  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.51 Test accuracy: 0.48\n",
            "\n",
            "  ---- Epoch  21  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.54 Test accuracy: 0.505\n",
            "\n",
            "  ---- Epoch  22  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.55 Test accuracy: 0.485\n",
            "\n",
            "  ---- Epoch  23  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.5 Test accuracy: 0.54\n",
            "\n",
            "  ---- Epoch  24  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.5 Test accuracy: 0.475\n",
            "\n",
            "  ---- Epoch  25  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.5 Test accuracy: 0.53\n",
            "\n",
            "  ---- Epoch  26  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.55 Test accuracy: 0.485\n",
            "\n",
            "  ---- Epoch  27  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.51 Test accuracy: 0.535\n",
            "\n",
            "  ---- Epoch  28  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.5 Test accuracy: 0.485\n",
            "\n",
            "  ---- Epoch  29  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.49 Test accuracy: 0.53\n",
            "\n",
            "  ---- Epoch  30  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.56 Test accuracy: 0.51\n",
            "\n",
            "  ---- Epoch  31  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.54 Test accuracy: 0.54\n",
            "\n",
            "  ---- Epoch  32  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.52 Test accuracy: 0.545\n",
            "\n",
            "  ---- Epoch  33  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.49 Test accuracy: 0.535\n",
            "\n",
            "  ---- Epoch  34  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.49 Test accuracy: 0.54\n",
            "\n",
            "  ---- Epoch  35  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.51 Test accuracy: 0.515\n",
            "\n",
            "  ---- Epoch  36  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.51 Test accuracy: 0.485\n",
            "\n",
            "  ---- Epoch  37  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.5 Test accuracy: 0.53\n",
            "\n",
            "  ---- Epoch  38  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.56 Test accuracy: 0.515\n",
            "\n",
            "  ---- Epoch  39  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.53 Test accuracy: 0.55\n",
            "\n",
            "  ---- Epoch  40  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.56 Test accuracy: 0.535\n",
            "\n",
            "  ---- Epoch  41  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.53 Test accuracy: 0.515\n",
            "\n",
            "  ---- Epoch  42  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.52 Test accuracy: 0.48\n",
            "\n",
            "  ---- Epoch  43  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.59 Test accuracy: 0.54\n",
            "\n",
            "  ---- Epoch  44  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.6 Test accuracy: 0.55\n",
            "\n",
            "  ---- Epoch  45  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.54 Test accuracy: 0.52\n",
            "\n",
            "  ---- Epoch  46  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.51 Test accuracy: 0.465\n",
            "\n",
            "  ---- Epoch  47  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.54 Test accuracy: 0.525\n",
            "\n",
            "  ---- Epoch  48  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.51 Test accuracy: 0.48\n",
            "\n",
            "  ---- Epoch  49  ----\n",
            "\n",
            "  Batch  0  training observations from  0  to  99\n",
            "  Batch  1  training observations from  100  to  199\n",
            "  Batch  2  training observations from  200  to  299\n",
            "  Batch  3  training observations from  300  to  399\n",
            "  Batch  4  training observations from  400  to  499\n",
            "  Batch  5  training observations from  500  to  599\n",
            "  Batch  6  training observations from  600  to  699\n",
            "  Batch  7  training observations from  700  to  799\n",
            "\n",
            "  Train accuracy: 0.52 Test accuracy: 0.505\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "LVn9ve6URWK3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Remove Stop Words"
      ]
    },
    {
      "metadata": {
        "id": "swC-QJqmROVY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# To make output stable across runs\n",
        "def reset_graph(seed= RANDOM_SEED):\n",
        "    tf.reset_default_graph()\n",
        "    tf.set_random_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "REMOVE_STOPWORDS = True  # stopword removal \n",
        "\n",
        "EVOCABSIZE = 10000  # specify desired size of pre-defined embedding vocabulary "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9hYW6C-CROVg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------------- \n",
        "# Select the pre-defined embeddings source        \n",
        "# Define vocabulary size for the language model    \n",
        "# Create a word_to_embedding_dict for GloVe.6B.50d\n",
        "embeddings_directory = 'embeddings/glove.6B'\n",
        "filename = 'glove.6B.50d.txt'\n",
        "embeddings_filename = os.path.join(embeddings_directory, filename)\n",
        "# ------------------------------------------------------------- "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KGzwIioNROVj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Utility function for loading embeddings follows methods described in\n",
        "# https://github.com/guillaume-chevalier/GloVe-as-a-TensorFlow-Embedding-Layer\n",
        "# Creates the Python defaultdict dictionary word_to_embedding_dict\n",
        "# for the requested pre-trained word embeddings\n",
        "# \n",
        "# Note the use of defaultdict data structure from the Python Standard Library\n",
        "# collections_defaultdict.py lets the caller specify a default value up front\n",
        "# The default value will be retuned if the key is not a known dictionary key\n",
        "# That is, unknown words are represented by a vector of zeros\n",
        "# For word embeddings, this default value is a vector of zeros\n",
        "# Documentation for the Python standard library:\n",
        "#   Hellmann, D. 2017. The Python 3 Standard Library by Example. Boston: \n",
        "#     Addison-Wesley. [ISBN-13: 978-0-13-429105-5]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "khSHLcPdROVm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "01907cca-0c7b-45a6-a42d-69d7a914d09f"
      },
      "cell_type": "code",
      "source": [
        "# Utility function for loading embeddings follows methods described in\n",
        "# https://github.com/guillaume-chevalier/GloVe-as-a-TensorFlow-Embedding-Layer\n",
        "# Creates the Python defaultdict dictionary word_to_embedding_dict\n",
        "# for the requested pre-trained word embeddings\n",
        "# \n",
        "# Note the use of defaultdict data structure from the Python Standard Library\n",
        "# collections_defaultdict.py lets the caller specify a default value up front\n",
        "# The default value will be retuned if the key is not a known dictionary key\n",
        "# That is, unknown words are represented by a vector of zeros\n",
        "# For word embeddings, this default value is a vector of zeros\n",
        "# Documentation for the Python standard library:\n",
        "#   Hellmann, D. 2017. The Python 3 Standard Library by Example. Boston: \n",
        "#     Addison-Wesley. [ISBN-13: 978-0-13-429105-5]\n",
        "def load_embedding_from_disks(embeddings_filename, with_indexes=True):\n",
        "    \"\"\"\n",
        "    Read a embeddings txt file. If `with_indexes=True`, \n",
        "    we return a tuple of two dictionnaries\n",
        "    `(word_to_index_dict, index_to_embedding_array)`, \n",
        "    otherwise we return only a direct \n",
        "    `word_to_embedding_dict` dictionnary mapping \n",
        "    from a string to a numpy array.\n",
        "    \"\"\"\n",
        "    if with_indexes:\n",
        "        word_to_index_dict = dict()\n",
        "        index_to_embedding_array = []\n",
        "  \n",
        "    else:\n",
        "        word_to_embedding_dict = dict()\n",
        "\n",
        "    with open(embeddings_filename, 'r') as embeddings_file:\n",
        "        for (i, line) in enumerate(embeddings_file):\n",
        "\n",
        "            split = line.split(' ')\n",
        "\n",
        "            word = split[0]\n",
        "\n",
        "            representation = split[1:]\n",
        "            representation = np.array(\n",
        "                [float(val) for val in representation]\n",
        "            )\n",
        "\n",
        "            if with_indexes:\n",
        "                word_to_index_dict[word] = i\n",
        "                index_to_embedding_array.append(representation)\n",
        "            else:\n",
        "                word_to_embedding_dict[word] = representation\n",
        "\n",
        "    # Empty representation for unknown words.\n",
        "    _WORD_NOT_FOUND = [0.0] * len(representation)\n",
        "    if with_indexes:\n",
        "        _LAST_INDEX = i + 1\n",
        "        word_to_index_dict = defaultdict(\n",
        "            lambda: _LAST_INDEX, word_to_index_dict)\n",
        "        index_to_embedding_array = np.array(\n",
        "            index_to_embedding_array + [_WORD_NOT_FOUND])\n",
        "        return word_to_index_dict, index_to_embedding_array\n",
        "    else:\n",
        "        word_to_embedding_dict = defaultdict(lambda: _WORD_NOT_FOUND)\n",
        "        return word_to_embedding_dict\n",
        "\n",
        "print('\\nLoading embeddings from', embeddings_filename)\n",
        "word_to_index, index_to_embedding = \\\n",
        "    load_embedding_from_disks(embeddings_filename, with_indexes=True)\n",
        "print(\"Embedding loaded from disks.\")"
      ],
      "execution_count": 319,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Loading embeddings from embeddings/glove.6B/glove.6B.50d.txt\n",
            "Embedding loaded from disks.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dv8s0_nTROVq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "outputId": "d55c4c44-57ef-4b9d-8a84-54294f0b20a0"
      },
      "cell_type": "code",
      "source": [
        "# Note: unknown words have representations with values [0, 0, ..., 0]\n",
        "\n",
        "# Additional background code from\n",
        "# https://github.com/guillaume-chevalier/GloVe-as-a-TensorFlow-Embedding-Layer\n",
        "# shows the general structure of the data structures for word embeddings\n",
        "# This code is modified for our purposes in language modeling \n",
        "vocab_size, embedding_dim = index_to_embedding.shape\n",
        "print(\"Embedding is of shape: {}\".format(index_to_embedding.shape))\n",
        "print(\"This means (number of words, number of dimensions per word)\\n\")\n",
        "print(\"The first words are words that tend occur more often.\")\n",
        "\n",
        "print(\"Note: for unknown words, the representation is an empty vector,\\n\"\n",
        "      \"and the index is the last one. The dictionnary has a limit:\")\n",
        "print(\"    {} --> {} --> {}\".format(\"A word\", \"Index in embedding\", \n",
        "      \"Representation\"))\n",
        "word = \"worsdfkljsdf\"  # a word obviously not in the vocabulary\n",
        "idx = word_to_index[word] # index for word obviously not in the vocabulary\n",
        "complete_vocabulary_size = idx \n",
        "embd = list(np.array(index_to_embedding[idx], dtype=int)) # \"int\" compact print\n",
        "print(\"    {} --> {} --> {}\".format(word, idx, embd))\n",
        "word = \"the\"\n",
        "idx = word_to_index[word]\n",
        "embd = list(index_to_embedding[idx])  # \"int\" for compact print only.\n",
        "print(\"    {} --> {} --> {}\".format(word, idx, embd))"
      ],
      "execution_count": 320,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Embedding is of shape: (400001, 50)\n",
            "This means (number of words, number of dimensions per word)\n",
            "\n",
            "The first words are words that tend occur more often.\n",
            "Note: for unknown words, the representation is an empty vector,\n",
            "and the index is the last one. The dictionnary has a limit:\n",
            "    A word --> Index in embedding --> Representation\n",
            "    worsdfkljsdf --> 400000 --> [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "    the --> 0 --> [0.418, 0.24968, -0.41242, 0.1217, 0.34527, -0.044457, -0.49688, -0.17862, -0.00066023, -0.6566, 0.27843, -0.14767, -0.55677, 0.14658, -0.0095095, 0.011658, 0.10204, -0.12792, -0.8443, -0.12181, -0.016801, -0.33279, -0.1552, -0.23131, -0.19181, -1.8823, -0.76746, 0.099051, -0.42125, -0.19526, 4.0071, -0.18594, -0.52287, -0.31681, 0.00059213, 0.0074449, 0.17778, -0.15897, 0.012041, -0.054223, -0.29871, -0.15749, -0.34758, -0.045637, -0.44251, 0.18785, 0.0027849, -0.18411, -0.11514, -0.78581]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hf2QsN_lROV1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Show how to use embeddings dictionaries with a test sentence\n",
        "# This is a famous typing exercise with all letters of the alphabet\n",
        "# https://en.wikipedia.org/wiki/The_quick_brown_fox_jumps_over_the_lazy_dog\n",
        "a_typing_test_sentence = 'The quick brown fox jumps over the lazy dog'\n",
        "print('\\nTest sentence: ', a_typing_test_sentence, '\\n')\n",
        "words_in_test_sentence = a_typing_test_sentence.split()\n",
        "\n",
        "print('Test sentence embeddings from complete vocabulary of', \n",
        "      complete_vocabulary_size, 'words:\\n')\n",
        "for word in words_in_test_sentence:\n",
        "    word_ = word.lower()\n",
        "    embedding = index_to_embedding[word_to_index[word_]]\n",
        "    print(word_ + \": \", embedding)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cleLITM9ROV5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------------- \n",
        "# Define vocabulary size for the language model    \n",
        "# To reduce the size of the vocabulary to the n most frequently used words\n",
        "\n",
        "def default_factory():\n",
        "    return EVOCABSIZE  # last/unknown-word row in limited_index_to_embedding\n",
        "# dictionary has the items() function, returns list of (key, value) tuples\n",
        "limited_word_to_index = defaultdict(default_factory, \\\n",
        "    {k: v for k, v in word_to_index.items() if v < EVOCABSIZE})\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "e6vA69oYROV8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Select the first EVOCABSIZE rows to the index_to_embedding\n",
        "limited_index_to_embedding = index_to_embedding[0:EVOCABSIZE,:]\n",
        "# Set the unknown-word row to be all zeros as previously\n",
        "limited_index_to_embedding = np.append(limited_index_to_embedding, \n",
        "    index_to_embedding[index_to_embedding.shape[0] - 1, :].\\\n",
        "        reshape(1,embedding_dim), \n",
        "    axis = 0)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "A-2tCtuuROWD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Delete large numpy array to clear some CPU RAM\n",
        "del index_to_embedding"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yoJ7Gk2SROWE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1227
        },
        "outputId": "1b74171c-2532-487b-8d71-b5c3aef34abb"
      },
      "cell_type": "code",
      "source": [
        "# Verify the new vocabulary: should get same embeddings for test sentence\n",
        "# Note that a small EVOCABSIZE may yield some zero vectors for embeddings\n",
        "print('\\nTest sentence embeddings from vocabulary of', EVOCABSIZE, 'words:\\n')\n",
        "for word in words_in_test_sentence:\n",
        "    word_ = word.lower()\n",
        "    embedding = limited_index_to_embedding[limited_word_to_index[word_]]\n",
        "    print(word_ + \": \", embedding)\n",
        "\n"
      ],
      "execution_count": 325,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test sentence embeddings from vocabulary of 10000 words:\n",
            "\n",
            "the:  [ 4.1800e-01  2.4968e-01 -4.1242e-01  1.2170e-01  3.4527e-01 -4.4457e-02\n",
            " -4.9688e-01 -1.7862e-01 -6.6023e-04 -6.5660e-01  2.7843e-01 -1.4767e-01\n",
            " -5.5677e-01  1.4658e-01 -9.5095e-03  1.1658e-02  1.0204e-01 -1.2792e-01\n",
            " -8.4430e-01 -1.2181e-01 -1.6801e-02 -3.3279e-01 -1.5520e-01 -2.3131e-01\n",
            " -1.9181e-01 -1.8823e+00 -7.6746e-01  9.9051e-02 -4.2125e-01 -1.9526e-01\n",
            "  4.0071e+00 -1.8594e-01 -5.2287e-01 -3.1681e-01  5.9213e-04  7.4449e-03\n",
            "  1.7778e-01 -1.5897e-01  1.2041e-02 -5.4223e-02 -2.9871e-01 -1.5749e-01\n",
            " -3.4758e-01 -4.5637e-02 -4.4251e-01  1.8785e-01  2.7849e-03 -1.8411e-01\n",
            " -1.1514e-01 -7.8581e-01]\n",
            "quick:  [ 0.13967   -0.53798   -0.18047   -0.25142    0.16203   -0.13868\n",
            " -0.24637    0.75111    0.27264    0.61035   -0.82548    0.038647\n",
            " -0.32361    0.30373   -0.14598   -0.23551    0.39267   -1.1287\n",
            " -0.23636   -1.0629     0.046277   0.29143   -0.25819   -0.094902\n",
            "  0.79478   -1.2095    -0.01039   -0.092086   0.84322   -0.11061\n",
            "  3.0096     0.51652   -0.76986    0.51074    0.37508    0.12156\n",
            "  0.082794   0.43605   -0.1584    -0.61048    0.35006    0.52465\n",
            " -0.51747    0.0034705  0.73625    0.16252    0.85279    0.85268\n",
            "  0.57892    0.64483  ]\n",
            "brown:  [-0.88497   0.71685  -0.40379  -0.10698   0.81457   1.0258   -1.2698\n",
            " -0.49382  -0.27839  -0.92251  -0.49409   0.78942  -0.20066  -0.057371\n",
            "  0.060682  0.30746   0.13441  -0.49376  -0.54788  -0.81912  -0.45394\n",
            "  0.52098   1.0325   -0.8584   -0.65848  -1.2736    0.23616   1.0486\n",
            "  0.18442  -0.3901    2.1385   -0.45301  -0.16911  -0.46737   0.15938\n",
            " -0.095071 -0.26512  -0.056479  0.63849  -1.0494    0.037507  0.76434\n",
            " -0.6412   -0.59594   0.46589   0.31494  -0.34072  -0.59167  -0.31057\n",
            "  0.73274 ]\n",
            "fox:  [ 0.44206   0.059552  0.15861   0.92777   0.1876    0.24256  -1.593\n",
            " -0.79847  -0.34099  -0.24021  -0.32756   0.43639  -0.11057   0.50472\n",
            "  0.43853   0.19738  -0.1498   -0.046979 -0.83286   0.39878   0.062174\n",
            "  0.28803   0.79134   0.31798  -0.21933  -1.1015   -0.080309  0.39122\n",
            "  0.19503  -0.5936    1.7921    0.3826   -0.30509  -0.58686  -0.76935\n",
            " -0.61914  -0.61771  -0.68484  -0.67919  -0.74626  -0.036646  0.78251\n",
            " -1.0072   -0.59057  -0.7849   -0.39113  -0.49727  -0.4283   -0.15204\n",
            "  1.5064  ]\n",
            "jumps:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0.]\n",
            "over:  [ 0.12972    0.088073   0.24375    0.078102  -0.12783    0.27831\n",
            " -0.48693    0.19649   -0.39558   -0.28362   -0.47425   -0.59317\n",
            " -0.58804   -0.31702    0.49593    0.0087594  0.039613  -0.42495\n",
            " -0.97641   -0.46534    0.020675   0.086042   0.39317   -0.51255\n",
            " -0.17913   -1.8333     0.5622     0.41626    0.075127   0.02189\n",
            "  3.784      0.71067   -0.073943   0.15373   -0.3853    -0.070163\n",
            " -0.35374    0.074501  -0.084228  -0.45548   -0.081068   0.39157\n",
            "  0.173      0.2254    -0.12836    0.40951   -0.26079    0.090912\n",
            " -0.60515   -0.9827   ]\n",
            "the:  [ 4.1800e-01  2.4968e-01 -4.1242e-01  1.2170e-01  3.4527e-01 -4.4457e-02\n",
            " -4.9688e-01 -1.7862e-01 -6.6023e-04 -6.5660e-01  2.7843e-01 -1.4767e-01\n",
            " -5.5677e-01  1.4658e-01 -9.5095e-03  1.1658e-02  1.0204e-01 -1.2792e-01\n",
            " -8.4430e-01 -1.2181e-01 -1.6801e-02 -3.3279e-01 -1.5520e-01 -2.3131e-01\n",
            " -1.9181e-01 -1.8823e+00 -7.6746e-01  9.9051e-02 -4.2125e-01 -1.9526e-01\n",
            "  4.0071e+00 -1.8594e-01 -5.2287e-01 -3.1681e-01  5.9213e-04  7.4449e-03\n",
            "  1.7778e-01 -1.5897e-01  1.2041e-02 -5.4223e-02 -2.9871e-01 -1.5749e-01\n",
            " -3.4758e-01 -4.5637e-02 -4.4251e-01  1.8785e-01  2.7849e-03 -1.8411e-01\n",
            " -1.1514e-01 -7.8581e-01]\n",
            "lazy:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0.]\n",
            "dog:  [ 0.11008   -0.38781   -0.57615   -0.27714    0.70521    0.53994\n",
            " -1.0786    -0.40146    1.1504    -0.5678     0.0038977  0.52878\n",
            "  0.64561    0.47262    0.48549   -0.18407    0.1801     0.91397\n",
            " -1.1979    -0.5778    -0.37985    0.33606    0.772      0.75555\n",
            "  0.45506   -1.7671    -1.0503     0.42566    0.41893   -0.68327\n",
            "  1.5673     0.27685   -0.61708    0.64638   -0.076996   0.37118\n",
            "  0.1308    -0.45137    0.25398   -0.74392   -0.086199   0.24068\n",
            " -0.64819    0.83549    1.2502    -0.51379    0.04224   -0.88118\n",
            "  0.7158     0.38519  ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gh9BSyxWROWK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------------\n",
        "# code for working with movie reviews data \n",
        "# Source: Miller, T. W. (2016). Web and Network Data Science.\n",
        "#    Upper Saddle River, N.J.: Pearson Education.\n",
        "#    ISBN-13: 978-0-13-388644-3\n",
        "# This original study used a simple bag-of-words approach\n",
        "# to sentiment analysis, along with pre-defined lists of\n",
        "# negative and positive words.        \n",
        "# Code available at:  https://github.com/mtpa/wnds       \n",
        "# ------------------------------------------------------------"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9Hm6U1GKROWM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Utility function to get file names within a directory\n",
        "def listdir_no_hidden(path):\n",
        "    start_list = os.listdir(path)\n",
        "    end_list = []\n",
        "    for file in start_list:\n",
        "        if (not file.startswith('.')):\n",
        "            end_list.append(file)\n",
        "    return(end_list)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "m73icBoLROWO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# define list of codes to be dropped from document\n",
        "# carriage-returns, line-feeds, tabs\n",
        "codelist = ['\\r', '\\n', '\\t']   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xY55KAUXR3Qv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#import nltk\n",
        "#nltk.download()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Co2l_IRzROWQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "13e28407-e8ac-4ed2-c26b-4dcdd0a0d826"
      },
      "cell_type": "code",
      "source": [
        "# We will not remove stopwords in this exercise because they are\n",
        "# important to keeping sentences intact\n",
        "if REMOVE_STOPWORDS:\n",
        "    print(nltk.corpus.stopwords.words('english'))\n",
        "\n",
        "# previous analysis of a list of top terms showed a number of words, along \n",
        "# with contractions and other word strings to drop from further analysis, add\n",
        "# these to the usual English stopwords to be dropped from a document collection\n",
        "    more_stop_words = ['cant','didnt','doesnt','dont','goes','isnt','hes',\\\n",
        "        'shes','thats','theres','theyre','wont','youll','youre','youve', 'br'\\\n",
        "        've', 're', 'vs'] \n",
        "\n",
        "    some_proper_nouns_to_remove = ['dick','ginger','hollywood','jack',\\\n",
        "        'jill','john','karloff','kudrow','orson','peter','tcm','tom',\\\n",
        "        'toni','welles','william','wolheim','nikita']\n",
        "\n",
        "    # start with the initial list and add to it for movie text work \n",
        "    stoplist = nltk.corpus.stopwords.words('english') + more_stop_words +\\\n",
        "        some_proper_nouns_to_remove"
      ],
      "execution_count": 330,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[u'i', u'me', u'my', u'myself', u'we', u'our', u'ours', u'ourselves', u'you', u\"you're\", u\"you've\", u\"you'll\", u\"you'd\", u'your', u'yours', u'yourself', u'yourselves', u'he', u'him', u'his', u'himself', u'she', u\"she's\", u'her', u'hers', u'herself', u'it', u\"it's\", u'its', u'itself', u'they', u'them', u'their', u'theirs', u'themselves', u'what', u'which', u'who', u'whom', u'this', u'that', u\"that'll\", u'these', u'those', u'am', u'is', u'are', u'was', u'were', u'be', u'been', u'being', u'have', u'has', u'had', u'having', u'do', u'does', u'did', u'doing', u'a', u'an', u'the', u'and', u'but', u'if', u'or', u'because', u'as', u'until', u'while', u'of', u'at', u'by', u'for', u'with', u'about', u'against', u'between', u'into', u'through', u'during', u'before', u'after', u'above', u'below', u'to', u'from', u'up', u'down', u'in', u'out', u'on', u'off', u'over', u'under', u'again', u'further', u'then', u'once', u'here', u'there', u'when', u'where', u'why', u'how', u'all', u'any', u'both', u'each', u'few', u'more', u'most', u'other', u'some', u'such', u'no', u'nor', u'not', u'only', u'own', u'same', u'so', u'than', u'too', u'very', u's', u't', u'can', u'will', u'just', u'don', u\"don't\", u'should', u\"should've\", u'now', u'd', u'll', u'm', u'o', u're', u've', u'y', u'ain', u'aren', u\"aren't\", u'couldn', u\"couldn't\", u'didn', u\"didn't\", u'doesn', u\"doesn't\", u'hadn', u\"hadn't\", u'hasn', u\"hasn't\", u'haven', u\"haven't\", u'isn', u\"isn't\", u'ma', u'mightn', u\"mightn't\", u'mustn', u\"mustn't\", u'needn', u\"needn't\", u'shan', u\"shan't\", u'shouldn', u\"shouldn't\", u'wasn', u\"wasn't\", u'weren', u\"weren't\", u'won', u\"won't\", u'wouldn', u\"wouldn't\"]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "I8smBAR_ROWU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# text parsing function for creating text documents \n",
        "# there is more we could do for data preparation \n",
        "# stemming... looking for contractions... possessives... \n",
        "# but we will work with what we have in this parsing function\n",
        "# if we want to do stemming at a later time, we can use\n",
        "#     porter = nltk.PorterStemmer()  \n",
        "# in a construction like this\n",
        "#     words_stemmed =  [porter.stem(word) for word in initial_words]  \n",
        "def text_parse(string):\n",
        "    # replace non-alphanumeric with space \n",
        "    temp_string = re.sub('[^a-zA-Z]', '  ', string)    \n",
        "    # replace codes with space\n",
        "    for i in range(len(codelist)):\n",
        "        stopstring = ' ' + codelist[i] + '  '\n",
        "        temp_string = re.sub(stopstring, '  ', temp_string)      \n",
        "    # replace single-character words with space\n",
        "    temp_string = re.sub('\\s.\\s', ' ', temp_string)   \n",
        "    # convert uppercase to lowercase\n",
        "    temp_string = temp_string.lower()    \n",
        "    if REMOVE_STOPWORDS:\n",
        "        # replace selected character strings/stop-words with space\n",
        "        for i in range(len(stoplist)):\n",
        "            stopstring = ' ' + str(stoplist[i]) + ' '\n",
        "            temp_string = re.sub(stopstring, ' ', temp_string)        \n",
        "    # replace multiple blank characters with one blank character\n",
        "    temp_string = re.sub('\\s+', ' ', temp_string)    \n",
        "    return(temp_string)    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ePpbxldNROWV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Manually import these"
      ]
    },
    {
      "metadata": {
        "id": "4eKN3v_XROWn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "b9bc7318-9206-43b9-d0a5-2a9c37843402"
      },
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------\n",
        "# gather data for 500 negative movie reviews\n",
        "# -----------------------------------------------\n",
        "dir_name = 'movie-reviews-negative'\n",
        "    \n",
        "filenames = listdir_no_hidden(path=dir_name)\n",
        "num_files = len(filenames)\n",
        "\n",
        "for i in range(len(filenames)):\n",
        "    file_exists = os.path.isfile(os.path.join(dir_name, filenames[i]))\n",
        "    assert file_exists\n",
        "print('\\nDirectory:',dir_name)    \n",
        "print('%d files found' % len(filenames))"
      ],
      "execution_count": 332,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Directory: movie-reviews-negative\n",
            "500 files found\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "K8OypOfaROWu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "f73abb52-96f3-48e1-f31a-f98ee3fd5ac6"
      },
      "cell_type": "code",
      "source": [
        "# Read data for negative movie reviews\n",
        "# Data will be stored in a list of lists where the each list represents \n",
        "# a document and document is a list of words.\n",
        "# We then break the text into words.\n",
        "\n",
        "def read_data(filename):\n",
        "\n",
        "  #with open(filename, encoding='utf-8') as f:\n",
        "  with open(filename) as f:\n",
        "    data = tf.compat.as_str(f.read())\n",
        "    data = data.lower()\n",
        "    data = text_parse(data)\n",
        "    data = TreebankWordTokenizer().tokenize(data)  # The Penn Treebank\n",
        "\n",
        "  return data\n",
        "\n",
        "negative_documents = []\n",
        "\n",
        "print('\\nProcessing document files under', dir_name)\n",
        "for i in range(num_files):\n",
        "    ## print(' ', filenames[i])\n",
        "\n",
        "    words = read_data(os.path.join(dir_name, filenames[i]))\n",
        "\n",
        "    negative_documents.append(words)\n",
        "    # print('Data size (Characters) (Document %d) %d' %(i,len(words)))\n",
        "    # print('Sample string (Document %d) %s'%(i,words[:50]))"
      ],
      "execution_count": 333,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Processing document files under movie-reviews-negative\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "__qN6QNRROW5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "78d2729f-b16d-47c7-ec06-a75235fb6a35"
      },
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------\n",
        "# gather data for 500 positive movie reviews\n",
        "# -----------------------------------------------\n",
        "dir_name = 'movie-reviews-positive'  \n",
        "filenames = listdir_no_hidden(path=dir_name)\n",
        "num_files = len(filenames)\n",
        "\n",
        "for i in range(len(filenames)):\n",
        "    file_exists = os.path.isfile(os.path.join(dir_name, filenames[i]))\n",
        "    assert file_exists\n",
        "print('\\nDirectory:',dir_name)    \n",
        "print('%d files found' % len(filenames))"
      ],
      "execution_count": 334,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Directory: movie-reviews-positive\n",
            "500 files found\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "aQgCCoMoROW7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "f785777d-2e51-4474-ca81-c60912e7a49a"
      },
      "cell_type": "code",
      "source": [
        "# Read data for positive movie reviews\n",
        "# Data will be stored in a list of lists where the each list \n",
        "# represents a document and document is a list of words.\n",
        "# We then break the text into words.\n",
        "\n",
        "def read_data(filename):\n",
        "\n",
        "  with open(filename) as f:\n",
        "    data = tf.compat.as_str(f.read())\n",
        "    data = data.lower()\n",
        "    data = text_parse(data)\n",
        "    data = TreebankWordTokenizer().tokenize(data)  # The Penn Treebank\n",
        "\n",
        "  return data\n",
        "\n",
        "positive_documents = []\n",
        "\n",
        "print('\\nProcessing document files under', dir_name)\n",
        "for i in range(num_files):\n",
        "    ## print(' ', filenames[i])\n",
        "\n",
        "    words = read_data(os.path.join(dir_name, filenames[i]))\n",
        "\n",
        "    positive_documents.append(words)\n",
        "    # print('Data size (Characters) (Document %d) %d' %(i,len(words)))\n",
        "    # print('Sample string (Document %d) %s'%(i,words[:50]))\n",
        "\n"
      ],
      "execution_count": 335,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Processing document files under movie-reviews-positive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OkwQu0GOROW8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "33f145a8-418a-4966-a95e-e8d04f0af4a4"
      },
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------------\n",
        "# convert positive/negative documents into numpy array\n",
        "# note that reviews vary from 10 to 629 words   \n",
        "# so we use the first 20 and last 20 words of each review \n",
        "# as our word sequences for analysis\n",
        "# -----------------------------------------------------\n",
        "max_review_length = 0  # initialize\n",
        "for doc in negative_documents:\n",
        "    max_review_length = max(max_review_length, len(doc))    \n",
        "for doc in positive_documents:\n",
        "    max_review_length = max(max_review_length, len(doc)) \n",
        "print('max_review_length:', max_review_length) \n",
        "\n",
        "min_review_length = max_review_length  # initialize\n",
        "for doc in negative_documents:\n",
        "    min_review_length = min(min_review_length, len(doc))    \n",
        "for doc in positive_documents:\n",
        "    min_review_length = min(min_review_length, len(doc)) \n",
        "print('min_review_length:', min_review_length) \n",
        "\n"
      ],
      "execution_count": 359,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "max_review_length: 629\n",
            "min_review_length: 10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "eZ6p0altROW_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# construct list of 1000 lists with 40 words in each list\n",
        "from itertools import chain\n",
        "documents = []\n",
        "for doc in negative_documents:\n",
        "    doc_begin = doc[0:10]\n",
        "    doc_end = doc[len(doc) - 30: len(doc)]\n",
        "    documents.append(list(chain(*[doc_begin, doc_end])))    \n",
        "for doc in positive_documents:\n",
        "    doc_begin = doc[0:10]\n",
        "    doc_end = doc[len(doc) - 30: len(doc)]\n",
        "    documents.append(list(chain(*[doc_begin, doc_end])))    \n",
        "\n",
        "# create list of lists of lists for embeddings\n",
        "embeddings = []    \n",
        "for doc in documents:\n",
        "    embedding = []\n",
        "    for word in doc:\n",
        "       embedding.append(limited_index_to_embedding[limited_word_to_index[word]]) \n",
        "    embeddings.append(embedding)\n",
        "    \n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AcYYaG6zcIDu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#embeddings=np.array(embeddings)\n",
        "#embedding=np.array(embedding)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UMqjPqpRcCPX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#embed=np.vstack([embeddings, embedding])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "opKQwfhzN2Vl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c83b0796-ce9e-4ef1-e9b6-b5b7741a2466"
      },
      "cell_type": "code",
      "source": [
        "embeddings=np.array(embeddings)\n",
        "embeddings.shape"
      ],
      "execution_count": 366,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1000,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 366
        }
      ]
    },
    {
      "metadata": {
        "id": "iKS7kYozQy8w",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#embed=np.column_stack((embeddings, embedding))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "D7TJb7GNLGnQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a43b58bb-f3c6-470e-dc3f-3e82e57270b6"
      },
      "cell_type": "code",
      "source": [
        "len(embeddings)"
      ],
      "execution_count": 338,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 338
        }
      ]
    },
    {
      "metadata": {
        "id": "7ckHYn64Lbms",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7a7f6286-42de-49b6-f1fa-21759277b77a"
      },
      "cell_type": "code",
      "source": [
        "embedding=np.array(embedding)\n",
        "embedding.shape"
      ],
      "execution_count": 365,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(40, 50)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 365
        }
      ]
    },
    {
      "metadata": {
        "id": "MEmkmc8rROXA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "outputId": "4599709d-c6d9-4e8d-ec36-33ebe703ede9"
      },
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------------    \n",
        "# Check on the embeddings list of list of lists \n",
        "# -----------------------------------------------------\n",
        "# Show the first word in the first document\n",
        "test_word = documents[0][0]    \n",
        "print('First word in first document:', test_word)    \n",
        "print('Embedding for this word:\\n', \n",
        "      limited_index_to_embedding[limited_word_to_index[test_word]])\n",
        "print('Corresponding embedding from embeddings list of list of lists\\n',\n",
        "      embeddings[0][0][:])\n",
        "\n"
      ],
      "execution_count": 183,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First word in first document: all\n",
            "Embedding for this word:\n",
            " [ 0.19253   0.10006   0.063798 -0.087664  0.52217   0.39105  -0.41975\n",
            " -0.45671  -0.34053  -0.11175   0.014754  0.31734  -0.50853  -0.1156\n",
            "  0.74303   0.097618  0.34407  -0.1213   -0.16938  -0.84088  -0.11231\n",
            "  0.40602   0.76801   0.091138  0.10782  -1.2673   -0.57709  -0.36208\n",
            "  0.34824  -0.75458   4.0426    0.94967  -0.22668  -0.35777   0.3413\n",
            "  0.13072   0.23045  -0.036997 -0.25889   0.12977  -0.39031  -0.049607\n",
            "  0.45766   0.56782  -0.46165   0.41933  -0.5492    0.081191 -0.30485\n",
            " -0.30513 ]\n",
            "Corresponding embedding from embeddings list of list of lists\n",
            " [ 0.19253   0.10006   0.063798 -0.087664  0.52217   0.39105  -0.41975\n",
            " -0.45671  -0.34053  -0.11175   0.014754  0.31734  -0.50853  -0.1156\n",
            "  0.74303   0.097618  0.34407  -0.1213   -0.16938  -0.84088  -0.11231\n",
            "  0.40602   0.76801   0.091138  0.10782  -1.2673   -0.57709  -0.36208\n",
            "  0.34824  -0.75458   4.0426    0.94967  -0.22668  -0.35777   0.3413\n",
            "  0.13072   0.23045  -0.036997 -0.25889   0.12977  -0.39031  -0.049607\n",
            "  0.45766   0.56782  -0.46165   0.41933  -0.5492    0.081191 -0.30485\n",
            " -0.30513 ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "57PEDcwVNyw3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ux-qOIa5ROXC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "outputId": "06ae94a3-fa98-4311-b88d-7701dffbe6cf"
      },
      "cell_type": "code",
      "source": [
        "# Show the seventh word in the tenth document\n",
        "test_word = documents[6][9]    \n",
        "print('First word in first document:', test_word)    \n",
        "print('Embedding for this word:\\n', \n",
        "      limited_index_to_embedding[limited_word_to_index[test_word]])\n",
        "print('Corresponding embedding from embeddings list of list of lists\\n',\n",
        "      embeddings[6][9][:])\n",
        "\n"
      ],
      "execution_count": 184,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First word in first document: heartthrob\n",
            "Embedding for this word:\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0.]\n",
            "Corresponding embedding from embeddings list of list of lists\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uq7Ltl8bROXD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "outputId": "bccefccb-9042-4033-d1d3-e83da70b7ba9"
      },
      "cell_type": "code",
      "source": [
        "# Show the last word in the last document\n",
        "test_word = documents[999][39]    \n",
        "print('First word in first document:', test_word)    \n",
        "print('Embedding for this word:\\n', \n",
        "      limited_index_to_embedding[limited_word_to_index[test_word]])\n",
        "print('Corresponding embedding from embeddings list of list of lists\\n',\n",
        "      embeddings[999][39][:])        \n",
        "\n"
      ],
      "execution_count": 185,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First word in first document: based\n",
            "Embedding for this word:\n",
            " [ 0.44515   0.14297  -0.31849   0.93227   0.15758   0.10426  -0.86868\n",
            " -1.0764    0.55314   0.30039   0.50631   0.014296 -0.31297  -0.28746\n",
            " -0.57734   0.033763 -0.22912   0.45513  -0.57208   0.12396   0.76135\n",
            "  0.20192  -0.31061   0.27998  -1.039    -1.6102   -0.96987  -0.50075\n",
            " -0.68246  -0.13174   3.2179   -0.80151  -0.12665  -0.67749   0.35736\n",
            " -0.91313  -0.55365  -0.070563  0.20915   0.11336   0.66136  -0.087329\n",
            "  0.41196  -0.080373 -0.061894  0.38201  -0.21459   0.86345  -0.11521\n",
            "  0.54568 ]\n",
            "Corresponding embedding from embeddings list of list of lists\n",
            " [ 0.44515   0.14297  -0.31849   0.93227   0.15758   0.10426  -0.86868\n",
            " -1.0764    0.55314   0.30039   0.50631   0.014296 -0.31297  -0.28746\n",
            " -0.57734   0.033763 -0.22912   0.45513  -0.57208   0.12396   0.76135\n",
            "  0.20192  -0.31061   0.27998  -1.039    -1.6102   -0.96987  -0.50075\n",
            " -0.68246  -0.13174   3.2179   -0.80151  -0.12665  -0.67749   0.35736\n",
            " -0.91313  -0.55365  -0.070563  0.20915   0.11336   0.66136  -0.087329\n",
            "  0.41196  -0.080373 -0.061894  0.38201  -0.21459   0.86345  -0.11521\n",
            "  0.54568 ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "w_E9PYHbROXG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c0148c2a-5875-4e40-c554-d677c7ce5794"
      },
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------------    \n",
        "# Make embeddings a numpy array for use in an RNN \n",
        "# Create training and test sets with Scikit Learn\n",
        "# -----------------------------------------------------\n",
        "embeddings_array = np.array(embeddings)\n",
        "\n",
        "\n",
        "embeddings_array.shape\n",
        "\n"
      ],
      "execution_count": 186,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1000,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 186
        }
      ]
    },
    {
      "metadata": {
        "id": "yfiSJm5A1XC6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Define the labels to be used 500 negative (0) and 500 positive (1)\n",
        "thumbs_down_up = np.concatenate((np.zeros((500), dtype = np.int32), \n",
        "                      np.ones((500), dtype = np.int32)), axis = 0)\n",
        "\n",
        "# Scikit Learn for random splitting of the data  \n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Random splitting of the data in to training (80%) and test (20%)  \n",
        "X_train, X_test, y_train, y_test = \\\n",
        "    train_test_split(embeddings_array, thumbs_down_up, test_size=0.20, \n",
        "                     random_state = RANDOM_SEED)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_cJnR7mZTA2u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a1c8c3b2-d099-43c0-fdd8-ba2458ed2d75"
      },
      "cell_type": "code",
      "source": [
        "embeddings_array.shape"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1000,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "metadata": {
        "id": "JsagGkw3ROXM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# --------------------------------------------------------------------------      \n",
        "# We use a very simple Recurrent Neural Network for this assignment\n",
        "# Gron, A. 2017. Hands-On Machine Learning with Scikit-Learn & TensorFlow: \n",
        "#    Concepts, Tools, and Techniques to Build Intelligent Systems. \n",
        "#    Sebastopol, Calif.: O'Reilly. [ISBN-13 978-1-491-96229-9] \n",
        "#    Chapter 14 Recurrent Neural Networks, pages 390-391\n",
        "#    Source code available at https://github.com/ageron/handson-ml\n",
        "#    Jupyter notebook file 14_recurrent_neural_networks.ipynb\n",
        "#    See section on Training an sequence Classifier, # In [34]:\n",
        "#    which uses the MNIST case data...  we revise to accommodate\n",
        "#    the movie review data in this assignment    \n",
        "# --------------------------------------------------------------------------  \n",
        "reset_graph()\n",
        "\n",
        "n_steps = embeddings_array.shape[1]  # number of words per document \n",
        "n_inputs = embeddings_array.shape[2]  # dimension of  pre-trained embeddings\n",
        "n_neurons = 20  # analyst specified number of neurons\n",
        "n_outputs = 2  # thumbs-down or thumbs-up\n",
        "\n",
        "learning_rate = 0.001"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "I79jV-RzROXR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
        "y = tf.placeholder(tf.int32, [None])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gHQRgXvVROXS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "basic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)\n",
        "outputs, states = tf.nn.dynamic_rnn(basic_cell, X, dtype=tf.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CynEPID3ROXU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "logits = tf.layers.dense(states, n_outputs)\n",
        "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,\n",
        "                                                          logits=logits)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tyyHeH3WROXZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "loss = tf.reduce_mean(xentropy)\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "training_op = optimizer.minimize(loss)\n",
        "correct = tf.nn.in_top_k(logits, y, 1)\n",
        "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5VEdGkBkROXc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "init = tf.global_variables_initializer()\n",
        "\n",
        "n_epochs = 50\n",
        "batch_size = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wcwshBXSROXd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with tf.Session() as sess:\n",
        "    init.run()\n",
        "    for epoch in range(n_epochs):\n",
        "        print('\\n  ---- Epoch ', epoch, ' ----\\n')\n",
        "        for iteration in range(y_train.shape[0] // batch_size):          \n",
        "            X_batch = X_train[iteration*batch_size:(iteration + 1)*batch_size,:]\n",
        "            y_batch = y_train[iteration*batch_size:(iteration + 1)*batch_size]\n",
        "            print('  Batch ', iteration, ' training observations from ',  \n",
        "                  iteration*batch_size, ' to ', (iteration + 1)*batch_size-1,)\n",
        "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
        "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
        "        acc_test = accuracy.eval(feed_dict={X: X_test, y: y_test})\n",
        "        print('\\n  Train accuracy:', acc_train, 'Test accuracy:', acc_test)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bcRBPLOdSlFC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}